---

title: Platform for continuous graph update and computation
abstract: A continuous stream data (e.g., messages, tweets) is received by ingest nodes of a platform. The ingest nodes may analyze the data to create a transaction of graph updates, assign a sequence number to the transaction, and distribute the graph updates with the sequence number to graph nodes of the platform. The graph nodes may store graph updates from ingest nodes, and then the ingest nodes may report graph update progresses in a progress table. A snapshot may be taken based on the progress table, and then graph-mining computation may be implemented. Tolerate failures and decaying may be supported and incremental expansion may be allowed to cope with increasing update rates and computation needs.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09589069&OS=09589069&RS=09589069
owner: Microsoft Technology Licensing, LLC
number: 09589069
owner_city: Redmond
owner_country: US
publication_date: 20160121
---
This application claims priority to and is a continuation of U.S. patent application Ser. No. 13 520 628 filed on Jul. 5 2012 which is a national stage application of an international patent application PCT CN2012 073529 filed Apr. 5 2012 which is hereby incorporated in its entirety by reference.

Increasingly popular online services e.g. Twitter Facebook and Foursquare provide updated information from various users in a relatively short amount of time. Information available on these services is continuously generated and is far more time sensitive than mostly static web pages. For example breaking news appears and is propagated quickly by some of these online services with new popular activities and hot topics arising constantly from real time events in the physical world. Although each message or update may be small and contain limited textual content a data stream may contain rich connections between users topics and messages and these connections may be used to generate important social phenomenon.

Distributed designs may take a data stream to construct a continuously changing graph structure to capture the relationship existing in the stream. The designs may decouple graph mining from graph updates of the graph structure. A distributed system may separate graph structure metadata from the application data of the graph structure. An epoch commit protocol may be implemented to generate global consistent snapshots on the graph structure. Based on these consistent snapshots graph mining algorithms may be performed to extract timely insights from the stream.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

A data stream e.g. messages tweets is received through a set of ingest nodes of a platform. An ingest node of the set of ingest nodes may analyze each incoming feed of the data stream e.g. a tweet and its associated context to create a transaction of graph updates assign a sequence number to the transaction and distribute the graph updates with the sequence number to a plurality of graph nodes of the platform. The graph nodes may provide a distributed in memory key value store with enhanced graph support. Each of the graph nodes may store graph structure metadata of the data stream separately from associated application data.

In addition after graph nodes store the graph updates the ingest node may report a graph update progress in a progress table. Periodically a snapshot may be taken based on the progress table. This progress table may be used to as a logical clock to define an end of an epoch. Within this epoch all stored local graph updates may be executed in the graph nodes following a pre determined order. The execution of graph updates may trigger incremental graph computation on the new snapshot to update associated application data and to extract timely insights from the data stream.

In some instances the techniques discussed herein may support tolerate failures and decaying and allow incremental expansion to cope with increasing update rates and computation needs.

The processes and systems described herein may be implemented in a number of ways. Example implementations are provided below with reference to the following figures.

The graph node may include two layers a storage layer and a computation layer . The storage layer may maintain graph data and the computation layer may execute incremental graph mining computations. Specifically the storage layer may maintain each vertex with an adjacency list as metadata of graph structure and separately store associated data for graph mining computations. The computation layer may execute the computations based on graph mining algorithms that operate on the associated data .

The graph nodes may store graph updates indicated by graph update operations sent by the ingest nodes . The ingest node may report a graph update progress in the global progress table that may be maintained by a central service. For example the ingest nodes may mark the global progress table with sequence numbers including multiple sequence numbers. Periodically the snapshooter may instruct the graph nodes to take a snapshot based on a current vector indicated by sequence numbers in the global progress table . The current vector may be used as a global logical clock to define an end of an epoch. After the epoch is defined the graph nodes may execute and commit all stored local graph updates in this epoch to produce a graph structure snapshot. In various embodiments these local graph updates may be executed following a pre determined order.

After updates in the graph structure due to the epoch the computation layer may execute incremental graph computations on the new snapshot to update associated values of interest.

In accordance with various embodiments an ingest node of the ingest nodes e.g. an ingest node or an ingest node may turn each incoming record of the data stream into a transaction including a set of graph update operations that may span logical partitions of the graph nodes . For example these operations may include creating vertex V adding an outgoing edge to vertex Vand adding an incoming edge to vertex V. Each of those operations can be executed entirely on the graph structure associated with a vertex. In addition the ingest node may create a continuous sequence of transactions each with a continuously increasing sequence number. Those sequence numbers may be used to construct a global logical clock to decide which transactions should be included in a snapshot and also used as the identifier for that snapshot.

In various embodiments a graph may be split into a fixed number e.g. 512 of the logical partitions which may be further assigned to physical machines of the graph nodes . For example a graph partition may be performed based on the hashing of vertex IDs and locality considerations may not be necessary. In some embodiments each of the logical partitions may include a set of vertices each with a set of directed weighted edges stored in a sorted list. Meanwhile edges may be considered as part of the graph structure and added and or modified in the storage layer . Each vertex of the set of vertices may also have a set of vertex fields that store the associated data for algorithms of the graph mining computation in the computation layer . The type of values stored in vertex filed may be arbitrary as long as it can be serialized.

For example as illustrated in the ingest node may send graph update operations with associated sequence numbers to partition v and partition u respectively. Specifically in partition u the corresponding operations may be grouped to generate first grouped operations which may be sorted based on an order of the associated sequence numbers and represented as 0 3 5 . Similarly in partition v the corresponding operations may be grouped and sorted to generate second group operations i.e. 1 2 4 .

Suppose that the ingest node has received acknowledgements from all relevant partitions in the graph nodes e.g. partition u and partition v that graph update operations for all transactions with sequence numbers up to 3 has been received and stored. As a result the ingest node may update its entry to the 3 first sequence number . To initiate a snapshot the snapshooter may take from the global progress table a global vector i.e. 3 . . . 7 comprising one or more sequence number . The global vector may be used as a global logical clock to define the end of an epoch . This newly defined epoch may be broadcasted to the graph nodes such that graph updates belonging to the epoch are processed in the same deterministic but artificial order in the logical partitions . Accordingly a graph update from the ingest node with a sequence number s is included in the epoch if and only if s is not greater than the 3 first sequence number .

Similarly a graph update from the ingest node with a sequence number s is included in the epoch if and only if s is not greater than the 7 second sequence number . In some embodiments operations on a logical partition are processed in serial and there may be enough the logical partitions on each graph node leading to sufficient concurrency at the server level.

In some embodiments the process of creating a snapshot may continue incoming updates. The ingest nodes may continuously send new graph updates into the graph nodes with higher sequence numbers. The process of the ingest nodes dispatching and the graph nodes storing graph update operations may overlap with the process of creating snapshots by applying those updates. Therefore the deferred execution may not affect throughput over a sufficiently long period of time. The consistent snapshot mechanism of this disclosure may effectively batches operations in a small epoch window to strike a balance between reasonable timeliness and be able to handle high incoming rate of updates the higher the rate the more effective this batching may be.

The epoch commit protocol may guarantee atomicity in that either all operations in a transaction are included in a snapshot or none of them are included in the snapshot. This may exclude a snapshot that includes one vertex with an outgoing edge but with no matching incoming edge to the destination vertex. The protocol may further ensures that all transactions from the same ingest node are processed in the sequence number order. Thanks to the separation of graph updates and graph mining only simple graph updates may be dealt with when creating consistent snapshots and therefore leveraging the fact that each transaction consists of a set of graph structure updates that can each be applied on a single vertex structure. For those updates depended on states of other vertices they may be executed in the graph mining phase.

In some embodiments the snapshot mechanism described in this disclosure may ensure consensus on the set of transactions to be included in a snapshot and impose an artificial order within that set so that all the transactions may be processed in the same order. In some embodiments the order may be artificial. For example the graph nodes may be instructed to process updates from the ingest node in a certain sequence number before processing those in order. This externally imposed order may not need to take into account any causal relationship partially because the mechanism separates graph updates from graph mining and graph updates are usually simple and straightforward. Therefore the externally imposed order may reflect neither the physical time order nor any causal order. In various embodiments different externally imposed orders may be applied and the resulting graphs may be similar. In some embodiments vertex creation is made deterministic. For example if there is a vertex created for each twitter user ID that vertex may have an internal ID that depends on that twitter user ID deterministically. Accordingly an edge from or to that vertex may be created before that vertex is created thereby eliminating cross operation dependencies.

At the platform may receive the data stream e.g. messages and tweets . In some embodiments the data stream may include dynamics streaming data feeds which may be continuously generated. The new information of the dynamic streaming data feeds may be more time sensitive than mostly static web pages. For example breaking news may appear and propagate quickly within the dynamic streaming data feeds and new popular activities and treading topics may arise constantly from real time events in the physical world. Meanwhile rich connections between entities such as users topics and data feeds may be used to reveal important social phenomena. In some embodiments the dynamic streaming data feeds may use multiple metadata e.g. hashtags to identify controversial information associated with messages.

At the platform may produce a snapshot to define the graph structure data associated with the data stream . In some embodiments the platform may produce consistent distribute snapshots by using the epoch commit protocol which is described in greater details with reference to below.

At the platform may perform graph computations to conduct operations e.g. compiling application data associated with the graph data . In some embodiments the platform may execute incremental graph mining such that the computation results may be updated based on recent changes in the data stream . These recent changes are reflected in new snapshots. In some embodiments graph mining algorithms e.g. search algorithms and TunkRank algorithm may operate on a set of vertex fields that store the associated data .

At the platform may present computation results to users based on the application data . For example the platform may present search results user influences shortest paths between two vertices e.g. two users in a graph and controversial topics that are associated with the data stream .

At the ingest node may assign the sequence number to the transaction. At the ingest node may distribute operations with the sequence number among the graph nodes. In some embodiments a set of graph update operations from the ingest nodes may be sorted and grouped in the logical partitions to generate operations grouped by original ingest nodes.

At the graph node may store the graph updates from the ingest node . In some embodiments the graph node may maintain each vertex with an adjacency list as metadata of the graph structure . Accordingly the graph updates may modify the metadata that defines the graph structure . In some embodiments the graph node may separately store the associated data . In some embodiments the ingest node may be configured to map a vertex ID to the logical partitions and to assign the logical partitions and their replicates to servers.

At after the graph nodes stores the operations of the transaction the ingest node may mark a graph update progress in the global progress table . The global progress table may record the sequence number for the ingest node to monitor the graph update progress.

At the snapshooter may define an end of the epoch based on the global vector comprising current sequence numbers of each ingest node in the global progress table e.g. the ingest node and the ingest node . The global vector may be used as a global logical clock to define the end of the epoch .

At the graph nodes may execute stored local graph updates in the epoch to produce a graph structure snapshot after the epoch is defined. The snapshooter may broadcast the definition of the epoch to each graph node such that all graph updates in the epoch are processed in the same deterministic order in logical partitions .

For example suppose that an ingest node i updates its entry to sequence number sif this ingest node has received acknowledgments from corresponding graph nodes that graph update operations for transactions up to shave been received and stored. Periodically e.g. 10 seconds the snapshooter may take from the current global progress table the vector of sequence numbers e.g. s s . . . s where sis the sequence number associated with ingest node i. The snapshooter may then use the vector as a global logical vector clock to define the end of the current epoch. The decision is broadcasted to all graph nodes where all graph updates belonging to this epoch are processed in the same deterministic but artificial order in all logical partitions. A graph update from ingest node i with sequence number s is included in the current epoch i.e. s s . . . s if and only if s is not greater than sholds.

In some embodiments updates in the graph structure in response to the defining of the epoch may trigger an incremental graph computation on snapshots to update the associated data . Various algorithms may be used to implement the incremental graph computation.

As discussed above the computation layer of the graph nodes may execute incremental graph mining. Computation results may be updated based on recent changes in the graph. Graph structure changes may be reflected in new snapshots graph mining algorithms may operate on a set of the vertex fields that store the associated data for the algorithms.

In some embodiments a vertex based computation model may be used for graph mining computation. In this model the data of interest may be stored along with vertices and computation proceeds by processing across every vertex. In addition graph scale reductions may be used to compute global values which may be arbitrary complex values e.g. top X influential users or number of vertices of certain type .

In some embodiments the platform may implement a hybrid of computation models based on a pull model and a push model with changes to support incremental computation and efficient distributed execution. Under this hybrid model typically changes in the associated data may propagate in a sub graph sparked by changes in the graph structure e.g. adding an edge .

At a graph scale aggregation of the vertices may be implemented to compute global values using graph scale reductions. These global values may be arbitrary complex values e.g. top X influential users or a number of vertices of a certain type . The operations to may be performed by a loop process via dashed line form the operation that leads back to the decision that may include propagate changes if necessary. In some embodiments propagation driven by other vertices may change the status of the vertex. In some embodiments changes in user defined vertex fields may propagate in a sub graph in response to certain changes in the structure of the graph e.g. adding an edge . The propagation may be terminated when status changes are not detected across all vertices in the graph structure .

In the push model each vertex can send a partial update to another vertex s vertex field. For example the pagerank of a vertex is a weighted sum of the pageranks of its neighboring vertices and each vertex sends its pagerank to its out neighbors and a system adds them together to form the total pagerank. In incremental algorithms each vertex may send its incremental change to the value of vertex field. For example in the pagerank each vertex may send the difference of its current and previous pagerank. For the model to work the updates may be associative and commutative. A feature of the model is the ability to perform sender side aggregation. For each vertex field programmer can define a local aggregation function that combines updates sent by several vertices to a one single update.

Modifications over the push model may enable incremental computation by keeping track of dirty fields for a new snapshot and during computation. When a field is declared dirty its update function may be invoked. The role of the update function is to push its difference of its new value to previous values to neighboring vertices. The platform may keep track of the value that was sent to each of the neighboring vertices to perform incremental calculation.

In some embodiments processes may be used to not only support the push model but also provide a way to handle each individual message separately in the vertex update function. In this disclosure the messages may be handled by the platform and combined by the user defined aggregation function. Update function may see the final value stored in the vertex field.

A pull model may be modified for distributed computation. A vertex update function in a pull model may read the values of its neighbor vertices and produce a new value for itself. If the vertex update function determines the change was significant it will ask the platform to update its neighbors and the computation propagates in the graph dynamically. In the platform update function may not be restricted to reading its neighbors and may want to read neighbors of certain type or an individual vertex e.g. a newly created vertex . Therefore for optimal performance programmers may be suggested to reduce the amount of required vertex information for the update function to perform. In addition different update functions may need different types of data. In some embodiments some functions may require a value of a particular vertex field of a neighboring vertex but other functions may require more data e.g. a list of edges of the neighbor .

In some embodiments the platform may schedule updates to vertices in a way that minimizes network communication. In particular the platform may combine requests to same vertices if several update functions request for the same vertex and execute the updates when all requested data is available. A synchronous model may be executed where the program issues synchronous calls to vertices. Requests may be aggressively batched so there are more chances of merging requests and to reduce the amount of RPC calls between servers.

In some embodiments users can define functions that are invoked when there are new vertices or new in out edges in a snapshot. These new vertices or new in out edges may be used as initialization of incremental graph mining. In the push model the corresponding vertex field to dirty may be set to subsequently lead to invoking the update function on the vertex. Similarly in the pull model an initialization phase may involve asking the system to prepare the data needed to execute an update function.

In addition to vertex based computation the platform may provide a mechanism to compute global values using aggregator functions that execute a distributed reduction over all vertices.

In some embodiments the platform may also be designed for frequent incremental computation steps. It may adopt a scheduling mechanism. Computation may proceed by executing consecutive super steps on which every vertex that is scheduled to run is executed by each partition. Computational consistency may not be enforced such that neighboring vertices can be updated in parallel.

In some embodiments the platform may execute a defined maximum number of super steps at each snapshot unless the task queues are empty and there are no vertices to update which may be an indication of converged computation. The execution model of the platform may also be related to Bulk Synchronous Parallel BSP and dynamic scheduling and global aggregators may be updated after each BSP step.

The algorithm may include lines 1 4 which may process a graph of user vertices with edges connecting users who have mentioned each other. For example a stronger connection between users based on who mentions who in Twitter may be used. If a tweet contains username it may mean that the submitter of the micro blog mentions user username i.e. paying attention to username . In line 5 each EmitOperations may emit two createEdge operations one for the source to add an outgoing edge and the other for the destination to add an incoming edge. As shown in line 8 code may be added to mark new out edges and vertex to initiate pushes.

In lines 9 17 updateFunction vertex may send the difference of new and previous weighted TunkRank to its neighbors. In line 19 code may be added to perform sum operation. In line 20 code may be added to detect whether the field has changed enough dirty to trigger computation i.e. updateFunction . In some embodiments by adjusting a parameter i.e. in the trigger the algorithm may adjust the accuracy computation time trade off. In addition the algorithm may use a global aggregator object to maintain a list of K most influential users. In some embodiments we set the to 0.001 a value sufficient to find top influential users.

At the platform may assign an incarnation number to the ingest node and the incarnation number may be paired with a sequence number associated with a transaction in the ingest node . Accordingly sequence numbers may be replaced with pairs e.g. c s wherein c is an incarnation number and s is a sequence number. At the pairs may be used in graph structure updates sent to the graph nodes and may be recorded in the global progress table .

At the platform may determine whether an ingest node fails and recovers or whether a new machine takes the role of the failed ingest node. At the recovered ingest node or the replaced ingest node may seal incarnation number if the ingest node fails and recovers or the new machine takes the role of the failed ingest node. The recovered ingest node may consult the global progress table for the pair including the incarnation number associated with the ingest node and the sequence number associated with the transaction.

At the platform may generate a new incarnation number by adding one to the original incarnation number and may generate a new sequence number by resetting the sequence number to zero 0 or adding one to the original sequence number.

At the platform may discard operations associated with the ingest node that have sequence numbers being greater than the sequence number. To avoid any loss of transactions all incoming data feeds may be stored reliably and can only be garbage collected after they have been reflected in the global progress table .

For example when an ingest node fails and recovers or when a new machine takes the role of a failed ingest node that resurrected ingest node i may consult the global progress table for the pair c s associated with ingest node i. The resurrected ingest node may seal cat sand use s 1 as the new incarnation number. This ingest node can reset the sequence number to zero 0 or continue at s 1. By sealing cat S all requests with c S where s s are considered invalid and discarded.

In some embodiments the platform may separately handle fault tolerance at the storage layer and at the computation layer by using different mechanisms. At the storage layer graph update operations need to be stored reliably on graph nodes. The platform may leverage ingest nodes and use a simple quorum based replication mechanism. Specifically each logical partition may be replicated on k e.g. 3 different machines and can tolerate f e.g. 1 failure where k 2f 1 holds. Graph update operations may then be sent to all replicas and an ingest node may consider the operation reliably stored as long as f 1 replicas have responded. The ingest node may keep a counter for the number of operations for each logical partition and attach the counter with each operation. A replica can use the counter to identify holes and ask the missing information from other replicas. Replicas may create the same snapshots as they apply the same set of operations in the same order.

In some embodiments at the computation layer as discussed above the platform may trigger incremental graph mining computation on consistent snapshots. Each invocation of computation may take a relatively small amount of time e.g. order of minutes . Because snapshots are reliably stored with replication at the storage layer the platform may roll back and re execute if it encounters any failures in a computation phase. The result of computation can be replicated to tolerate failures. The platform may implement a primary backup replication scheme where the primary does the computation and copies the results to the secondaries.

The scale of the platform may depend on some factors including the rate of incoming data feeds the size of the resulting graphs and the complexity of graph mining computation. In some embodiments the platform may recruit more machines into the system in order to handle higher load larger amount of data and or heavier computation. For example the platform may create a large number of logical partitions up front and incremental expansion can then be achieved by moving certain logical partitions to new machines. For example suppose that the platform may want to migrate a logical partition from S to T. The platform may communicate with each ingest node s about the migration and about a promise to send all future operations on that logical partition to both S and T starting from sequence number t. Once a snapshot with a logical clock s s . . . s satisfying s tfor each 1 i n is created the platform instructs a copy of that snapshot from S to T. Once T receives the snapshot it has all the information needed to take over the logical partition from S. Because computation overlaps with incoming updates T can usually catch up with S quickly without causing any performance degradation.

In some embodiments the value of information decays over time and outdated information may gradually have less and less impact on results. The platform may support decaying by leveraging global logical clocks based on sequence numbers. For example suppose that information of interest in the last n days and that the information within those n days has a different weight depending on which day it is. The platform may essentially create n 1 parallel graphs to track the last n days and plus the current day. The window may slide when a day passes by. The platform may align those decaying time boundaries with the epochs defined by logical clocks of sequence numbers. When a day passes in the real time the platform may look at the current epoch number and use this as the boundary. Accordingly the real graph used for computation may be constructed by taking a weighted average of those parallel graphs.

In a very basic configuration the computing device typically includes at least one processing unit and system memory . Depending on the exact configuration and type of computing device the system memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination of the two. The system memory typically includes an operating system one or more program modules and may include program data . The operating system includes a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API . The computing device is of a very basic configuration demarcated by a dashed line . Again a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.

The computing device may have additional features or functionality. For example the computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Computer readable media may include at least two types of computer readable media namely computer storage media and communication media. Computer storage media may include volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. The system memory the removable storage and the non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other non transmission medium that can be used to store the desired information and which can be accessed by the computing device . Any such computer storage media may be part of the computing device . Moreover the computer readable media may include computer executable instructions that when executed by the processor s perform various functions and or operations described herein.

In contrast communication media may embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transmission mechanism. As defined herein computer storage media does not include communication media.

The computing device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. These devices are well known in the art and are not discussed at length here.

The computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network. These networks may include wired networks as well as wireless networks. The communication connections are one example of communication media.

It is appreciated that the illustrated computing device is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well known computing devices systems environments and or configurations that may be suitable for use with the embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor base systems set top boxes game consoles programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and or the like. For example some or all of the components of the computing device may be implemented in a cloud computing environment such that resources and or services are made available via a computer network for selective use by mobile devices.

Although the techniques have been described in language specific to structural features and or methodological acts it is to be understood that the appended claims are not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing such techniques.

