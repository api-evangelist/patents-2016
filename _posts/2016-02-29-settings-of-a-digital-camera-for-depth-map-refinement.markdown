---

title: Settings of a digital camera for depth map refinement
abstract: Systems and methods are disclosed for identifying depth refinement image capture instructions for capturing images that may be used to refine existing depth maps. The depth refinement image capture instructions are determined by evaluating, at each image patch in an existing image corresponding to the existing depth map, a range of possible depth values over a set of configuration settings. Each range of possible depth values corresponds to an existing depth estimate of the existing depth map. This evaluation enables selection of one or more configuration settings in a manner such that there will be additional depth information derivable from one or more additional images captured with the selected configuration settings. When a refined depth map is generated using the one or more additional images, this additional depth information is used to increase the depth precision for at least one depth estimate from the existing depth map.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09521391&OS=09521391&RS=09521391
owner: Adobe Systems Incorporated
number: 09521391
owner_city: San Jose
owner_country: US
publication_date: 20160229
---
This application is a continuation application of U.S. patent application Ser. No. 14 576 936 filed on Dec. 19 2014 now allowed which is related to U.S. application Ser. No. 14 552 332 filed Nov. 24 2014 allowed and U.S. application Ser. No. 14 577 792 filed Dec. 19 2014 allowed the contents of which are incorporated herein in their entirety.

This disclosure relates generally to methods and systems for determining configuration settings for a digital camera and a quantity of images to be captured by the digital camera using those settings for improving depth information.

A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

Digital cameras including digital single lens reflex DSLR cameras and digital cameras integrated into mobile devices often have sophisticated hardware and software that enables a user to capture digital images using a combination of different user defined and camera defined configuration settings. A digital image provides a digital representation of a particular scene. A digital image may subsequently be processed by itself or in combination with other images of the scene to derive additional information from the image. For example one or more images of a scene may be processed to estimate the depths of the objects depicted within the scene i.e. the distance of each object from a location from which the images were taken. The depth estimates for each object in a scene or possibly each pixel within an image are included in a file referred to as a depth map. Among other things depth maps may be used to improve existing image editing techniques e.g. cutting hole filling copy to layers of an image etc. .

Conventionally depth maps are generated using one of a variety of techniques. Such techniques include depth from defocus techniques which use out of focus blur to estimate depth of the imaged scene. Depth estimation using such techniques is possible because imaged scene locations will have different amounts of out of focus blur i.e. depth information depending on the camera configuration settings e.g. aperture setting and focus setting used to take the image s . Estimating depth therefore involves estimating the amount of depth information at the different scene locations whether the depth information is derived from one image or from multiple images of the scene. Conventionally the accuracy of such depth estimates depends on the number of images used. Generally speaking the greater the number of images that are input the greater the amount of depth information that can be compared for any one position e.g. pixel in the scene.

Thus many conventional depth from defocus techniques may require a dense set of input images in order to estimate scene depth with a higher degree of certainty. However conventional techniques cannot predictively determine the optimal number of images and the corresponding camera configuration settings needed for estimating scene depth map with any particular degree of certainty. Nor can conventional techniques be used to analyze an existing depth map to predictively determine a number of additional images that could be captured of the scene with particular camera configuration settings so that sufficiently more depth information would be available to refine the existing depth map i.e. improve the accuracy of its depth estimates .

Accordingly it is desirable to provide improved solutions for analyzing an existing depth map or other scene depth information to predictively determine a number of additional images to be captured of the scene and the camera configuration settings used for capturing them such that sufficient depth information is available for refining the depth estimates provided by the existing depth map or other scene depth information.

In some examples systems and methods are provided for predictively determining depth refinement image capture instructions for use in refining a depth map. In particular the depth refinement image capture instructions once determined indicate a quantity of additional images to take and with which aperture and focus settings to take them such that an existing depth map may be refined with additional depth information derived from the additional image s . Refining an existing depth map includes improving the accuracy of depth estimates in the existing depth map. Depth refinement image capture instructions are determined in a manner that is scene dependent because data associated with the existing depth map is analyzed. For example the techniques for generating depth refinement image capture instructions described herein take as input a combination of an all in focus image of a scene an existing depth map of the scene and or a measure of uncertainty corresponding to the existing depth map of the scene. The all in focus image is an image of the underlying scene that is generated without any blur. The measure of uncertainty is a representation of the accuracy of each depth estimate in the existing depth map which provides a depth estimate for each pixel of an image. For example the measure of uncertainty may indicate that for a particular pixel a depth estimate of five meters has a particular variance. In some examples the existing depth map is also provided as an input. However because the depth estimates found in the depth map can be derived from the measure of uncertainty and the all in focus image the existing depth map is optional to implementing the techniques described herein. In some examples a depth likelihood corresponding to the existing depth map is also provided as an input. The depth likelihood is a mapping of probability to depth and may be provided in the form of a depth likelihood map.

The degree to which the depth estimates in an existing depth map can be improved is measured by an improvement criterion. The improvement criterion is a sum of the increase in the depth precision e.g. in bits that would be possible at a given pixel in the existing depth map if a particular configuration setting is used to capture an additional image and additional depth information is derived therefrom. The improvement criterion is evaluated over a set of possible configuration settings for each pixel to determine the total degree of improvement for each pixel as a function of configuration settings. This function may be referred to herein as a voting map. The voting map indicates for each pixel in the existing depth map the degree to which additional depth information derived from an image captured using a particular configuration setting will improve the depth estimate at that pixel. The voting map is then used to select which configuration settings should be used to take one or more additional images of the scene.

Depth refinement image capture instructions for one or more images are then determined by analyzing the voting map. For example the voting map may indicate that an image of the scene taken with a particular focus setting and a particular aperture setting will meaningfully improve the accuracy of the depth estimates for a sufficient percentage of the pixels of the existing depth map and that another image taken with a different focus setting and or a different aperture setting will further meaningfully improve the accuracy of the depth estimates another sufficient percentage of the pixels. Outlying data in the voting map may be ignored so that additional images are not captured and processed for the purpose of achieving only insignificant refinements to the existing depth map.

These illustrative examples are mentioned not to limit or define the disclosure but to provide examples to aid understanding thereof. Additional examples are discussed in the Detailed Description and further description is provided there.

Computer implemented systems and methods are disclosed for determining depth refinement image capture instructions for use in connection with refining an existing depth map. As introduced above a depth map may be generated by for example comparing differences in defocus blur associated with two or more input images.

The techniques described herein are used to predictively determine depth refinement image capture instructions in a manner that considers the amount of possible increase in depth precision at each pixel in an existing depth map. In one example a depth map is generated using conventional techniques or in accordance with particular techniques discussed in related disclosures incorporated by reference herein as indicated below. A depth map includes depth information for the scene which is typically indicated as an estimate of depth for each pixel of the image. The existing depth map may have been previously generated from one or more images of the scene captured using a digital camera. The existing depth map has a corresponding measure of uncertainty that indicates a variance for each depth estimate in the existing depth map. The existing depth map also has a corresponding depth likelihood for each depth estimate in the existing depth map. The depth likelihood is a mapping of probability to depth and may be provided in the form of a depth likelihood map.

In some examples analysis of the existing depth map may indicate that it provides suitable depth estimates for the scene. However in other cases the analysis may indicate that improved e.g. improved as to accuracy depth estimates can be achieved by accounting for additional depth information from one or more additional images captured using certain camera configuration settings. In some examples this includes predictively determining how many images should be captured with the digital camera and which configuration settings e.g. aperture settings and focus settings should be used for capturing them.

In some examples refining depth estimates includes determining for each configuration setting e.g. aperture and focus that is of interest whether additional depth information can be derived from an image patch of an image captured with that configuration setting can be used to discriminate between a range of possible depth values corresponding to an existing depth estimate. In some examples the evaluated configuration settings are those configuration settings available for a particular digital camera with a particular lens. For example a particular lens and camera combination may only have a discrete number of possible apertures settings and focus settings.

The techniques described herein take as input a combination of an all in focus image an existing depth and or a measure of uncertainty corresponding to the existing depth and. Each of these inputs are generated using any suitable depth estimation technique and are used as described herein to determine the depth refinement image capture instructions. Using the all in focus image and a range of possible depth values e.g. between nine and twelve meters determined from the measure of uncertainty for the existing depth map image patches are simulated to determine how blurry each image patch would be at each of the possible depth values e.g. a nine meters ten meters eleven meters and twelve meters . The blurriness of an image patch corresponds to the amount of depth information that can be derived from the image patch. When considered in the frequency domain the blurriness of the image may correspond to the amount of frequency content in the image patch. In some examples the distances among the possible depths have a greater or smaller level of granularity. If for a particular configuration setting of interest a simulated image patch at a first possible depth and a simulated image patch at a second possible depth are sufficiently different in terms of blurriness then an image captured with that configuration setting may be a good choice to refine the depth estimate of the image patch from the existing depth map. This is because the image captured with that configuration setting would enable adequate discrimination between the first possible depth and the second possible depth. This concept is referred to as depth discriminability. In some examples a camera configuration setting is selected in a manner that maximizes depth discriminability between a range of possible depth values corresponding to the image patch.

In some examples an improvement criterion is generated as part of the process of determining depth refinement image capture instructions. The improvement criterion indicates the sum of the increase in depth precision e.g. in bits possible at each pixel if a particular configuration setting e.g. aperture setting and focus setting of a digital camera is used. In some examples based on the improvement criterion the particular configuration setting that maximizes the increase in total depth precision over a scene may be selected. In any event the improvement criterion may be evaluated over a set of possible configuration settings in order to determine a total amount of improvement in the depth precision as a function of the configuration settings. This function may be referred to herein as a voting map. The voting map combines for each pixel in the existing depth map votes as to the configuration settings that will most improve the depth estimate at that pixel. Each image patch that is analyzed provides a vote for the particular configuration setting that will improve its corresponding depth estimate s . In some examples a particular image patch votes for more than one configuration setting. The votes of all image patches may be averaged or otherwise aggregated and a configuration setting corresponding to the average or aggregate vote may be used. In some examples determining the configuration settings of a next image to capture includes finding the location of a maximum in the voting map. In some examples a voting map has more than one local maximum corresponding to different depth ranges that can be improved by depth information derived from images captured using different configuration settings. In this example configuration settings corresponding to each maximum may be selected and thus the depth refinement image capture instructions may indicate that more than one additional images should be taken.

In some examples after a quantity of images having certain configuration settings is recommended and captured a refined measure of uncertainty refined all in focus image and refined depth map are generated. In some examples a refined depth likelihood map is also generated. The techniques discussed above and in more detail below are then repeated to further refine the depth estimates in the refined depth map. In some examples an input is received that indicates a minimum level of depth precision a total quantity of additional images to capture an amount of time for capturing additional images a desire to capture additional images in a batch or any other suitable input for constraining the determination of the depth refinement image capture instructions. Based on any one of these inputs the selection of configuration settings for inclusion in the depth refinement image capture instructions may be adjusted. For example if the input indicates that a user is willing to take only three additional images the configuration settings receiving the three highest votes from the voting map e.g. having the three greatest values may be selected and included in the depth refinement image capture instructions.

The techniques described herein relating to generating depth refinement image capture instructions for use in depth map refinement may be used to improve existing image editing techniques. For example the existing depth map may be an image file with depth information a flat file with depth information or any other suitable type of file. In any case the depth information may be used as part of an image editing process. For example such information may be relevant in distinguishing between foreground objects and background objects. Such a distinction may be relevant to selecting objects whether in the foreground or background within the image. Imagine an example scene where an image including depth estimation information generated as described herein depicts a child e.g. a foreground element standing in front of a tree e.g. a background element . A user desiring to clip the child from the scene may indicate as such by selecting a portion of the child using an image editing application. The image editing application may then access the depth information in addition to selection cues such as color and texture to generate an outline of the child to be clipped. Thus in some examples the depth information may be used to supplement conventional selection cues such as colors and texture.

Turning now to the figures illustrates block diagram for implementing techniques relating to determining depth refinement image capture instructions for use in depth map refinement as described herein. The block diagram includes a depth refinement engine . The depth refinement engine is configured to receive or access an existing depth map a measure of uncertainty and an all in focus image . Each of the existing depth map the measure of uncertainty and the all in focus image may be received from a user accessed from memory or retrieved in any other suitable manner. The existing depth map comprises depth estimates for each object in a scene or possibly for each pixel within an image that corresponds to the scene. In some examples the existing depth map was previously outputted using a depth estimation technique. Example depth estimation techniques include depth from defocus techniques that analyze blur in one or more images to estimate depth and computer vision techniques that estimate depth using sets of all in focus images. Regardless of how the existing depth map is generated it is provided to the depth refinement engine . The depth refinement engine is configured to analyze the existing depth map and refine the included depth estimates by suggesting depth refinement image capture instructions .

As part of generating the existing depth map a depth estimation technique may also generate the measure of uncertainty and the all in focus image . In some examples the measure of uncertainty and the all in focus image are used by a depth estimation technique to generate the existing depth map . The measure of uncertainty comprises the variance of each depth estimate for each pixel or patch represented in the existing depth map . The all in focus image represents the imaged scene as it would appear if none of the pixels in the image experienced any blur. In this manner the all in focus image may be considered a sharp image. In some examples the all in focus image is generated from one or more images of the scene that were used to generate the existing depth map . In some examples the all in focus image is captured by the camera.

The measure of uncertainty the all in focus image and optionally the existing depth map collectively depth refinement input are pre computed prior to being provided to the depth refinement engine . In some examples a depth likelihood map corresponding to the existing depth map is also included as an input to the depth refinement engine . In some examples the depth likelihood map is derived from the existing depth map and the measure of uncertainty . In some examples the depth likelihood map includes depth likelihoods for each pixel of the existing depth map for pluralities of pixels of the existing depth map for image patches of the existing depth map which may include one or more pixels or for the entirety of the existing depth map . As used herein an image patch refers to a small area of an image e.g. the images N that includes a plurality of pixels. The depth refinement engine takes the depth refinement input as input and outputs the depth refinement image capture instructions . Ultimately a purpose of generating the depth refinement image capture instructions or at least a portion of the depth refinement image capture instructions is to refine the existing depth map using additional depth information derived from one or more images N captured by image capture device according to the instructions.

The depth refinement engine therefore determines the depth refinement image capture instructions in a manner such that the precision of the depth estimates in the existing depth map can be increased when a refined depth map is generated using the images N . The increase in precision is measured according to an improvement criterion. The improvement criterion as described in more detail herein indicates for each pixel an amount of possible increase in depth precision for a depth estimate of the pixel. To this end the depth refinement engine determines a quantity of images and configuration settings . The quantity of images indicates the number of images i.e. the number of the images N that should be taken using the image capture device and the configuration settings indicate the settings of the image capture device for capturing the images N such that the existing depth map may be refined using depth information derived from the images N . In particular the configuration settings indicate the camera settings in terms of aperture settings and focus settings that should be used when capturing the recommended number of images identified by the quantity of images . The aperture settings relate to the amount of light let into the camera when an image is captured. The aperture of a camera is adjusted using a mechanism of blades that adjusts the amount of light. The focus settings relate to a distance of a focal plane from the image capture device e.g. a digital camera and is adjusted accordingly.

The depth refinement engine not only generates each portion of the depth refinement image capture instructions but also associates the portions. For example assume that the depth refinement engine determines that the quantity of images that should be captured is four. The depth refinement engine also associates with each of the four images a particular aperture setting from the aperture settings and a particular focus setting from the focus settings . In some examples the aperture settings and the focus settings for each of the additional images are different. In some examples however at least some of the aperture settings and or at least some of the focus settings for each of the additional images are the same.

The depth refinement image capture instructions are then provided to the image capture device . The image capture device is configured according to the depth refinement image capture instructions in order to capture the images N . Because the images N are captured with the image capture device according to the depth refinement image capture instructions the images N include configuration information N . The configuration information is associated with the image the configuration information is associated with the image and so forth. The configuration information N for each of the images N includes at least the aperture setting and the focus setting used by the image capture device to capture the images N . Using depth information derived from the images N the existing depth map can be improved.

The communication service is configured to manage communications between the other modules of the depth refinement engine and other devices or components e.g. hardware and or software components that communicate with the depth refinement engine . For example the depth refinement input is received by the communication service . In some examples depth refinement input is provided to another module of the depth refinement engine . For example as discussed below after receiving the depth refinement input the communication service provides the depth refinement input or a portion thereof to the depth refinement instructions service . The communication service also provides the depth refinement image capture instructions to other modules of the depth refinement engine to an operator of a digital camera to a digital camera to an output device e.g. a printer to a storage structure associated with the depth refinement engine and or to other similar locations. When the depth refinement engine is implemented on the same user device configured to capture images and used to generate the depth map e.g. a mobile device with a digital camera a tablet computer with a digital camera a handheld digital camera a laptop computer with a digital camera and other similar users devices the communication service processes requests to perform operations received from other components of that user device. When the depth refinement engine is implemented as part of an image editing service e.g. hosted on a network server the communication service processes one or more requests from user devices to access portions of the depth refinement engine . In some examples other input is received by the communication service . In some examples these other inputs bound and or constraint the determination of depth refinement image capture instructions .

The depth refinement instructions service is configured to implement the techniques relating to determining the depth refinement image capture instructions for use in depth map refinement as described herein. In some examples the depth refinement instructions service determines the depth refinement image capture instructions based on a portion of the depth refinement input . In some examples the depth refinement image capture instructions indicate the quantity of images to be captured an aperture setting for each of the images and a focus setting for each of the images . In some examples the depth refinement instructions service identifies depth refinement image capture instructions such that depth ambiguity for textured patches of constant depth within a simulated scene is eliminated. In some examples determining the depth refinement image capture instructions includes identifying a depth range in the existing depth map that can be refined. In some examples refining the existing depth map includes determining the depth refinement image capture instructions such that when the images are taken and depth information derived therefrom is combined with the existing depth map the depth likelihoods for the depth estimates of the existing depth map will be improved.

The depth map refinement service is configured to refine the existing depth map using depth information derived from image s captured according to the depth refinement image capture instructions . In some examples the depth map refinement service also generates a refined measure of uncertainty a refined all in focus image and a refined depth likelihood as part of generating a refined depth map. In some examples the depth map refinement service generates an improved or refined depth map and the depth refinement instruction module analyzes the improved depth map to see if the improved depth map can be improved yet again. If so the depth refinement instructions service generates second depth refinement image capture instructions for capturing additional images. The additional depth information derived from these additional images are provided to the depth map refinement service to generate a second improved depth map. In this manner the improvement or refinement of an existing depth map may be iterative. As used herein iteratively evaluating refers to sequentially evaluating and or evaluating in parallel.

The range of possible depths in the illustrated example is 9 12 meters. Thus a first simulated image patch A is generated at nine meters a second simulated image patch B is generated at ten meters a third simulated image patch C is generated at eleven meters and a fourth simulated image patch D is generated at twelve meters. In some examples the simulated image patches A D are plotted along an image patch manifold which extends below the line used to illustrate curve . In some examples the image patch manifold is considered one dimensional because depth is the only thing that changes along the curve of the image patch manifold . Thus regardless of whether the simulated image patches A D are 5 5 20 20 or any other suitable size of pixel grids the image patch manifold remains one dimensional. In some examples the length of the curve from the minimum depth i.e. nine meters to the maximum depth i.e. twelve meters corresponds to the number of bits of precision that a depth estimate in the existing depth map can be improved. In some examples the bits of precision are also referred to the number of bins that the depth range can be divided into. Thus in some examples it is desirable for the curve to be long thereby allowing for a greater possible increase in depth precision. In other words the greater the length of the curve the easier it is to discriminate between the possible depth values to determine the most likely depth value for an image patch. In some examples the increase in depth precision results in a reduction in the variance of the depth estimates of the measure of uncertainty .

In some examples in order to generate the image patch manifold the depth refinement engine selects configuration settings for a next image to be captured such that a depth estimate for an image patch with a flat unimodal depth likelihood is refined to become a peaked unimodal depth likelihood. The depth likelihood may be represented in a graph with depth along the X axis and probability along the Y axis. The flat unimodal depth likelihood may correspond to a range of possible depths for the depth estimate and the peaked unimodal depth likelihood may represent the depth estimate after the next image has been taken and an improved depth map has been generated. In some examples the range of possible depth values for the depth estimate i.e. the range of possible depth values over which simulated image patches A D are evaluated is determined by finding a maximum likelihood for the depth estimate and selecting the range of possible depth values by selecting possible depth values that are less than the maximum by some percentage. In some examples the range of possible depth values for the depth estimate includes a range of possible depth values that corresponds to a multimodal depth likelihood. In some examples the measure of uncertainty is used to determine the range of possible depth values. For example a depth estimate from the existing depth map is provided and variance from the measure of uncertainty corresponding to the depth estimate is used to determine a minimum and a maximum possible depth value. The minimum possible depth value and the maximum possible depth value may be determined by taking a number of standard deviations away from the depth estimate in decreasing depth and increasing depth respectively.

For each possible depth value of the range of possible depth values to be evaluated each simulated image patch A D is evaluated over a set of configuration settings e.g. aperture settings and or focus settings in order to determine an improvement criterion for each configuration setting at each possible depth value. For each possible depth value a configuration setting and a corresponding blur kernel are selected. The blur kernel is used to blur the applicable all in focus image patch A D to obtain a corresponding simulated image patch A D . The simulated image patch A D represents the amount of blur an image patch will experience at the possible depth value when the image is captured using the selected configuration setting. The process is repeated for each configuration setting of interest e.g. each configuration setting available for the particular imaging device used to capture the images .

Once the evaluation of each configuration setting is completed over the range of possible depth values or at any other time after at least two simulated image depth patches e.g. A and B have been generated for the corresponding all in focus image patches e.g. A and B the image patch manifold is computed. The image patch manifold indicates the bits of depth precision by which the depth estimate can be improved. The image patch manifold is considered a 1 D manifold because it is one dimensional as to depth. In other words along the image patch manifold are plotted each of the simulated image patches A D for each possible depth value for the first set of configuration settings. A greater angle e.g. A D between the different simulated image patches A D indicates greater depth discriminability for their depth estimates.

The overall length of the image patch manifold for the first configuration setting is computed. In some examples computing the length of the image patch manifold is performed by taking a measure of distance between patch appearances for each pair of simulated images A D corresponding to adjacent possible depth values. Next the measures of distance between the patch appearances are summed to determine a measure of the length of the image patch manifold . In the illustrated example the length of the image patch manifold therefore corresponds to the summation of the measure of distance between simulated image patches A and B the measure of distance between simulated image patches B and C and the measure of distance between simulated image patches C and D . In some examples patch appearance comprises blurriness of the patches which may be represented in the frequency domain or the patch domain.

The length of the image patch manifold corresponds to the improvement in terms of bits of depth precision that can be achieved in the depth estimate for the image patch using the first configuration setting. When the length of the manifold is long that indicates there is good depth discriminability between the possible depth values using the selected configuration setting. If the selected configuration settings provides low depth discriminability then the length of the image patch manifold will be shorter. A poor choice of configuration setting is one that results in a very short image patch manifold . This means that it will be difficult to discriminate between the possible depths within the range of possible depths values.

In some examples the length of the curve and or the image patch manifold corresponds to the improvement criterion. In some examples a voting map is generated to aggregate improvement criteria for each image patch. In effect each image patch provides a vote for the configuration setting that will produce a next image of the scene from which sufficient depth information can be derived and used to achieve an increase the depth precision for the image patch s depth estimate. The voting map may indicate votes for each pixel represented in the existing depth map or at least those pixels represented in the portion of the existing depth map that has been evaluated. Due to overlapping image patches including common pixels multiple votes may have been provided for at least some pixel represented in the voting map. The configuration setting s receiving the most votes may be used to generate the depth refinement image capture instructions . In some examples the votes are tallied in other examples they may be averaged or otherwise aggregated. In some examples the votes may be combined into a matrix that includes combinations of aperture settings and focus settings and the amount of possible increase in depth precision for each pixel. In some examples outlying votes are ignored so that additional images are not captured and processed for the purpose of achieving only insignificant refinements to the existing depth map .

The voting map may be unimodal or multimodal. In some examples a highest peak representing the greatest number of votes in the voting map is selected and used to generate a depth refinement image capture instruction . In some examples the highest peak is the only peak i.e. the voting map is unimodal. In some examples the voting map will have multiple peaks i.e. will be multimodal. In such cases a number of peaks may be selected and used to generate depth refinement image capture instructions indicating that multiple additional images should be captured.

In some examples the quantity of images to be captured may correspond to or be constrained by user input. For example the user input may indicate a mode to guide details about what a user is willing to do to improve the depth estimates in the existing depth map . A mode may comprise a single best shot mode a best number of shots mode a best batch mode a time mode or any other suitable mode. The single best shot mode indicates that the user is willing to take a single image in order to improve the depth estimates. In this example the most significant peak in the voting map is selected. The best number of shots mode indicates that the user is willing to take a predetermined number of shots in order to improve the depth estimates. In this example the user designates a whole number i.e. and the corresponding number of most significant peaks in the voting map will be selected. The best batch mode indicates that the user is willing to capture the images in a batch. The time mode indicates a time that the user is willing to spend to capture additional images and or wait to refine the existing depth map . For example the user may indicate whether by using the time mode or otherwise that the user is only willing to spend three seconds to capture the images . Based on this input and in accordance with techniques described herein the quantity images to be captured and or the iterations of the process for determining depth refinement image capture instructions may be adjusted.

In some examples the digital camera other user device or service in accordance with techniques provided herein generate the depth refinement image capture instructions e.g. the quantity of images and the configuration settings including the aperture settings and the focus settings which are provided to the operator or the digital camera . The depth refinement image capture instructions instruct the operator and or the camera how to particularly capture the images N . In some examples the depth refinement image capture instructions are stored in a look up table which is accessible by the operator and or the digital camera . For example a hardcopy of a portion of the look up table may be provided to the operator and the operator may manually adjust the configuration settings of the digital camera to capture the images in accordance with the depth refinement image capture instructions . In some examples the depth refinement image capture instructions are generated and stored in memory of the digital camera or other user device for use at a later time. In some examples the depth refinement image capture instructions particular to the digital camera because they rely on calibration data particular to the digital camera .

In some examples prior to a first image being captured the depth refinement image capture instructions may have indicated a first aperture setting and a first focus setting for use by the camera while capturing the image . Accordingly the operator adjusts the digital camera according to the first aperture setting selected out of for example f 1.2 f 2 f 4 f 5.6 f 8 f 16 f 22 or other suitable aperture settings corresponding to the first image as identified by the depth refinement image capture instructions . The operator then focuses the digital camera according to the first focus setting corresponding to the first image as identified by the depth refinement image capture instructions and captures the image representing the scene using the digital camera .

Similarly in some examples prior to a second image being captured the depth refinement image capture instructions may have indicated a second aperture setting and a second focus setting for use by the camera while capturing the image . The operator then adjusts the digital camera according to the second aperture setting selected out of for example f 1.2 f 2 f 4 f 5.6 f 8 f 16 f 22 or other suitable aperture settings corresponding to the second image as identified by the depth refinement image capture instructions . The operator then focuses the digital camera according to the second focus setting corresponding to the second image as identified by the depth refinement image capture instructions and captures the image representing the scene using the digital camera . In some examples the operator captures more images with the same or different aperture settings and focus settings . Because the captured images N have particular blur characteristics from which depth information can be derived and used to improve the existing depth map they can be used by the depth refinement engine and or the depth generation engine A or B to output one or more improved depth maps.

Techniques relating to determining depth refinement image capture instructions for use in depth map refinement have been introduced and described above. In some examples certain features of the techniques described above may be implemented in accordance with the equations discussed and provided below.

In some examples high depth precision may be needed for ground truth depths and low precision may be needed for depths where there are no objects. Thus in some examples the techniques described herein may be applied to determine additional images in order to refine depths in areas of low contrast.

In some examples a good estimation of hidden image frequencies may have been acquired previously in accordance with techniques described herein and or other techniques. Blur kernels may be assumed to be n 1 . . . N and the captured images may i the posterior distribution of the all in focus image may a Gaussian with mean

By inspecting the likelihood function computed from the last N images a depth range of ambiguity d d for each image patch may be determined. In some examples this may be achieved by examining if the unimodal condition has been served but more conveniently it may be determined by comparing to an empirical threshold a range of depth values where E d circumflex over d . In some examples it may be desirable to divide the bin of ambiguity into a refined set of bins and ensure the likelihood of individual bins is contrasted enough.

Therefore depth refinement may be seen as a classification problem. In some examples the distribution of the defocused patch for each depth bin from the posterior hidden image distribution from the previous batches as well as the lens settings for an additional image may be computed. Next the image patch in the additional image may provide a noisy observation and its depth may be estimated by assigning it to the closest cluster.

In some examples an image patch manifold M i that may correspond to defocus patches of the same hidden image may be computed. As only the defocus level varies the image patch manifold may be a 1 D manifold numerically simulated.

In some examples the risk of erroneous depth assignment may be bounded by ensuring that the clusters for each depth bin are suitably separated. Since the distance between any pair of cluster may be typically larger than their distance on the manifold M the number of separable classes may be bounded by the length of the 1 D manifold.

In some examples the following theorem may be relevant Let be the minimal allowed image difference under any pair of depth hypotheses in the range d d the maximal number of separable depth bins

In some examples a strategy may be developed for refining depth estimation of general scenes that collects votes from all patches in the image. In some examples in accordance with techniques described herein for a discrete set of l 0 may be preecomputed. Next a certain configuration setting may be evaluated by 

In some examples the threshold to vote may be held to be between 1 M in order to disregard textureless areas and areas of very high certainty.

In some examples the final voting map may be a function and and may be multimodal. Therefore all the local maximals may be selected in the voting map as the next images to capture in the next batch.

In some examples a corollary may be used to resolve ambiguity in narrow range. If d d is narrow enough to linearize M the focus and aperture should be chosen so that the defocus level is approximately 1.2 pixel.

For the corollary the following strategy for very far scenes where the defocus kernel radius does not vary a lot may be determined 

In some examples where the scene is extremely far away the optimal strategy for acquiring depth may be to use the focus settings producing defocus of 1.2 pixel and using the largest aperture available.

The process begins at by receiving an all in focus image and a measure of uncertainty . In some examples the measure of uncertainty corresponds to an existing depth map . In some examples the measure of uncertainty indicates the relative accuracy of depth estimates for image patches from an existing image corresponding to the existing depth map . In some examples the process receives the existing depth map and or a depth likelihood map. In some examples the depth likelihood map indicates likelihoods for each depth estimate included in the existing depth map and may be generated by a depth likelihood function.

At the process identifies an image patch of an existing image corresponding to the existing depth map . In some examples identifying the image patch includes selecting an image patch of the existing image located at a first position in the existing image. An image patch is small grouping of pixels in an image in which depth is assumed to be continuous. While the techniques described herein are implemented on an image patch by image patch basis the depth estimates of the existing depth map are refined on a pixel by pixel basis. In some examples an image patch includes a plurality of pixels centered around a particular pixel.

At the process identifies a configuration setting from a set of configuration settings. In some examples the set of configuration settings includes aperture settings and focus settings available on a particular digital camera and lens combination. For example if five focus settings are available and five aperture settings are available the set of configuration settings includes twenty five configuration settings.

At the process selects a possible depth value falling with a depth range corresponding to the image patch. In some examples the depth range includes a plurality of possible depth values and selecting the possible depth value includes determining the depth range corresponding to the image patch. In some examples determining the depth range is based on the measure of uncertainty . For example a maximum possible depth value and a minimum possible depth value may be located at distances away from the depth estimate of the image patch equivalent to one or more standard deviations of the variance for the depth estimate. In some examples the depth range is determined based on the depth likelihood s for the patch. For example for a unimodal depth likelihood the depth range may be determined by selecting values that intersect with the depth likelihood and that are less probable than the peak of the depth likelihood. For example if the depth likelihood for the depth estimate had a peak at 0.8 meters the depths located at the intersection of a horizontal line drawn at 10 less than 0.8 meters may be used to define the depth range. In some examples a multimodal depth likelihood corresponding to the depth estimate is used to determine the depth range. In this example the depth range may extend from a first peak of the multimodal depth likelihood to a second peak of the multimodal depth likelihood or may extend from some other depth less than the first peak and greater than the second peak.

At the process identifies a blur kernel based on the selected possible depth value and the selected configuration setting. In some examples identifying the blur kernel includes accessing camera calibration information that indicates a mapping of configuration settings e.g. aperture settings and focus settings and depths to blur kernels. A blur kernel therefore is a function of depth aperture setting and focus setting. In some examples the identified blur kernel is use to blur an all in focus image patch corresponding to the selected image patch to provide a simulated image patch. The simulated image patch thus corresponds to the same position in the scene as the selected image patch. The simulated image patch is a simulation of the appearance e.g. in terms of blurriness of the image patch at the possible depth value if the image were captured using the selected configuration setting.

At the process determines an amount of possible increase in depth precision for the selected image patch that can be achieved by accounting for additional depth information in an additional image captured using the selected configuration setting. In some examples the amount of possible increase in depth precision is a function of configuration settings and corresponds to an image patch manifold as described with reference to . In some examples the length of the image patch manifold is determined after the configuration setting has been evaluated over each possible depth value of the depth range e.g. blocks until the decision at is no . In some examples the amount of possible increase in depth precision for the selected image patch comprises an improvement criterion for the depth range. Thus the length of the image patch manifold and or the curve running between the maximum possible depth value and the minimum possible depth value corresponds to the improvement criterion. The improvement criterion therefore indicates the amount of depth precision by which the depth estimate for the image patch can be improved. At it is determined whether there are other possible depth values of the range of possible depth values identified as part of . If the answer at is yes the process continues to where a next possible depth value falling within the depth range is selected. The process then returns to and is repeated from where a blur kernel based on the next possible depth value and the configuration setting is selected.

When it is determined at that no further possible depth values of the depth range need to be evaluated the process continues to where it determines whether there are other configuration settings of interest. If so the process continues to where a next configuration setting from the set of configuration settings is identified. The process then returns to and is repeated from . When it is determined at that no additional configurations settings are of interest the process continues to .

At the process determines whether there are any other images patches to be evaluated in the existing image corresponding to the existing depth map . If so a next image patch is identified at and at the process is repeated from for the next image patch. In some examples certain image patches where there is no texture in the existing image or where there is no depth estimate in the existing depth map are ignored by the process . When it is determined at that no additional image patches require evaluation the process continues to where a voting map is generated based on the amount of increase in depth precision for each image patch. In some examples a voting map is generated for each pixel and is as a function of configuration settings i.e. indicates which configuration settings will improve the depth estimates for each pixel represented in the existing depth map .

At the process determines at least one depth refinement image capture instruction based on the voting map. This may be performed in a variety of different ways. For example the depth refinement image capture instructions may be determined by selecting a configuration setting corresponding to the location of the maximum i.e. greatest number of votes from the voting map. The depth refinement image capture instructions may also be determined by selecting a set of the top local maxima from the voting map. In other examples the depth refinement image capture instructions may also be determined by selecting from the depth map a first configuration setting corresponding to a first local maximum associated with a first depth range and a second configuration setting corresponding to a second local maximum associated with a second depth range. The process may be repeated one or more times to generate additional depth refinement image capture instructions for further refining a refined depth map.

A user device may be any suitable device capable of capturing an image and or performing one or more operations on images. In some examples the user device is any suitable computing device such as but not limited to digital camera a mobile phone a smart phone a personal digital assistant PDA a laptop computer a personal computer a desktop computer a set top box a thin client device or other computing device. The user device is utilized by one or more users not shown for interacting with the image editing service .

The user device therefore includes a processor that is communicatively coupled to a memory and that executes computer executable program code and or accesses information stored in the memory . In some examples the memory stores a web service application and one or more engines e.g. an image characteristic engine A a depth generation engine A the depth refinement engine A an image editing engine A a web services application . The processor may be a microprocessor an application specific integrated circuit ASIC a state machine or other processing device. The processor may also include any of a number of processing devices including one. Such a processor can include or may be in communication with a computer readable medium storing instructions that when executed by the processor cause the processor to perform the operations described herein. The web service application may enable the user to interact with the image editing service over the network . The user device also comprises an image capture device A . The image capture device A is configured to capture one or more images. In some examples the image capture device A comprises a conventional digital camera including a lens aperture setting focus setting an infrared projector and or a structured light device. Any uses of digital camera throughout this specification are for illustrative purposes only and a person of ordinary skill in the art would understand that such term may generally be used to refer to any image capture device executed by or integrated with any one of the user devices N or any similar device. Therefore the terms digital camera and user device may sometimes be used generically and interchangeably herein. In some examples the user device is a digital camera and may be configured with the image capture device A in order to capture images but may not include any or some of the engines. In this example the user device or an operator of the user device is provided with the depth refinement image capture instructions to use while capturing the images using the image capture device A .

The image editing service includes a processor that is communicatively coupled to a memory and that executes computer executable program code and or accesses information stored in the memory . In some examples the memory stores an operating system and one or more engines e.g. the image characteristic engine A the depth generation engine B the depth refinement engine B and the image editing engine B . The operating system comprises any suitable operating system configured for interacting with the image editing service . The processor comprises a microprocessor an ASIC a state machine or other processing device. The processor also comprises any of a number of processing devices including one. Such a processor can include or may be in communication with a computer readable medium storing instructions that when executed by the processor cause the processor to perform the operations described herein.

The memory comprises any suitable computer readable medium. The computer readable medium may include any electronic optical magnetic or other storage device capable of providing a processor with computer readable instructions or other program code. A computer readable medium may include for example a magnetic disk memory chip ROM RANI an ASIC a configured processor optical storage magnetic tape or other magnetic storage or any other medium from which a computer processor can read instructions. The instructions may include processor specific instructions determined by a compiler and or an interpreter from code written in any suitable computer programming language including for example C C C Visual Basic Java Python Perl JavaScript and ActionScript.

The image editing service also includes a number of external or internal devices such as input or output devices. For example the image editing service includes input output I O device s and or ports such as for enabling connection with a keyboard a mouse a pen a voice input device a touch input device a display speakers a printer or other I O device. The image editing service also includes additional storage which may include removable storage and or non removable storage. The additional storage may include but is not limited to magnetic storage optical disks and or tape storage. The disk drives and their associated computer readable media may provide non volatile storage of computer readable instructions data structures program modules and other data. The image editing service also includes a user interface . The user interface is utilized by an operator or other authorized user to access portions of the image editing service . In some examples the user interface includes a graphical user interface web based applications programmatic interfaces such as application programming interfaces APIs or other user interface configurations. The image editing service also includes data store . The data store comprises data structures for storing information related to the implementation of the techniques described herein. Such information is stored in image database . Within the image database is stored input images depth maps depth likelihood maps voting maps measures of uncertainty together with their associated information and any other suitable data related to implementing the techniques described herein.

The depth generation engines A B are configured to generate depth maps based on a plurality of the images captured by the image capture device or captured by another image capture device and provided to one of the user devices N or the image editing service . In some examples the depth generation engines A B perform one or more operations to generate depth maps in accordance with techniques described in U.S. application Ser. No. 14 552 332 filed on Nov. 24 2014 the entirety of which is hereby incorporated by reference. For example as discussed in more detail in U.S. application Ser. No. 14 552 332 the depth generation engines A B may generate depth maps depth likelihood maps all in focus images and measures of uncertainty. In some examples after at least one depth refinement image capture instruction has been generated and at least one additional image has been captured the depth generation engines A B operating as discussed in U.S. application Ser. No. 14 552 332 may generate a refined depth map a refined depth likelihood map a refined all in focus image and or a refined measure of uncertainty. At least a portion of the refined depth map the refined depth likelihood map the refined all in focus images and the refined measure of uncertainty may then be inputted into the depth refinement engine to further refined the refined depth map. In this manner depth refinement may be iterative.

The image characteristic engines A B are configured to determine image capture instructions for capturing images that can be used to generate a depth map such as the existing depth map . In some examples the image characteristic engines A B perform one or more operations to generate image capture instructions in accordance with techniques described in co pending U.S. application Ser. No. 14 577 792. For example as discussed in more detail in U.S. application Ser. No. 14 577 792 the image capture instructions determined by the image characteristic engines A B identify a quantity of images and the aperture settings and or focus settings to be used to capture them such that a quality depth map can be generated from the images. In some examples one of the image characteristic engines A B is used to determine image capture instructions. The images captured according to those instructions are provided to one of the depth generation engines A B and a depth map is generated. In some examples the depth map along with other data is provided to one of the depth refinement engines A B where depth refinement image capture instructions are determined for refining the depth map.

The image editing engines A B are configured to perform one or more operations relating to image editing. For example after the images have been captured in accordance with the image capture instructions and a depth map has been generated and optionally refined one of the image editing engines A B are utilized to edit an image corresponding to the depth map. As noted previously the depth map may be stored as a separate file associated with the image or may be included as data or metadata within the image file.

Numerous specific details are set forth herein to provide a thorough understanding of the claimed subject matter. However those skilled in the art will understand that the claimed subject matter may be practiced without these specific details. In other instances methods apparatuses or systems that would be known by one of ordinary skill have not been described in detail so as not to obscure the claimed subject matter.

Unless specifically stated otherwise it is appreciated that throughout this specification discussions utilizing terms such as processing computing calculating determining and identifying or the like refer to actions or processes of a computing device such as one or more computers or a similar electronic computing device or devices that manipulate or transform data represented as physical electronic or magnetic quantities within memories registers or other information storage devices transmission devices or display devices of the computing platform.

The system or systems discussed herein are not limited to any particular hardware architecture or configuration. A computing device can include any suitable arrangement of components that provide a result conditioned on one or more inputs. Suitable computing devices include multipurpose microprocessor based computer systems accessing stored software that programs or configures the computing system from a general purpose computing apparatus to a specialized computing apparatus implementing one or more embodiments of the present subject matter. Any suitable programming scripting or other type of language or combinations of languages may be used to implement the teachings contained herein in software to be used in programming or configuring a computing device.

Embodiments of the methods disclosed herein may be performed in the operation of such computing devices. The order of the blocks presented in the examples above can be varied for example blocks can be re ordered combined and or broken into sub blocks. Certain blocks or processes can be performed in parallel.

The use of adapted to or configured to herein is meant as open and inclusive language that does not foreclose devices adapted to or configured to perform additional tasks or steps. Additionally the use of based on is meant to be open and inclusive in that a process step calculation or other action based on one or more recited conditions or values may in practice be based on additional conditions or values beyond those recited. Headings lists and numbering included herein are for ease of explanation only and are not meant to be limiting.

While the present subject matter has been described in detail with respect to specific embodiments thereof it will be appreciated that those skilled in the art upon attaining an understanding of the foregoing may readily produce alterations to variations of and equivalents to such embodiments. Accordingly it should be understood that the present disclosure has been presented for purposes of example rather than limitation and does not preclude inclusion of such modifications variations and or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art.

