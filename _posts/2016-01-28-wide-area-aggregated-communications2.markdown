---

title: Wide area aggregated communications
abstract: Methods, systems, and techniques for federating operations, in an optimized way using wide area networks are provided. Example systems provide an API for generating and handling federated requests as an aggregation. In one example Wide Area Network Aggregation System, WANAS provides an API, a connection manager, and connection iterators to manage inter-site connections and requests, and inter-pod requests and messaging. These components cooperate to distribute a task across multiple physically disparate sites using a representative connection to the site and fan out of requests to other pods within the site.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09553903&OS=09553903&RS=09553903
owner: VMware, Inc.
number: 09553903
owner_city: Palo Alto
owner_country: US
publication_date: 20160128
---
This application is a continuation of and claims priority to U.S. patent application Ser. No. 14 184 593 filed on Feb. 19 2014. The disclosure of the foregoing application is incorporated here by reference.

This application is related by subject matter to U.S. patent application Ser. No. 14 184 518 filed on Feb. 19 2014 which is incorporated herein by reference.

The present disclosure relates to methods techniques and systems for communication across wide area networks and in particular to methods techniques and systems for communicating between a plurality of sites with potentially multiple endpoints using multi endpoint targets to link the sites or perform federated operations.

Connections across wide area networks are typically slow and may incur high latency or even in some cases be unreliable. Often times it becomes important to communicate with one or more systems across such networks while minimizing latency and maximizing bandwidth. For example in large datacenter installations each of which may support thousands of virtual remote desktops it may be important to locate resources across many perhaps even physically disparate sites which are connected by wide area networks.

For example a request from a first datacenter at one site to find a resource in another datacenter at a physically disparate second site connected over a wide area network a WAN may cause performance issues simply because the target server is connected via a WAN connection. In order to successfully find a server at the second site that can access the required resource multiple WAN operations may be incurred which are inherently slower compared to local area network operations typically used within the datacenter. This latency ultimately may not be acceptable in certain situations such as to connect a user to his her virtual desktop especially when the user might be used to apparent immediate access or close to immediate access when attempting to connect to a desktop that resides within the user s home datacenter at the first site .

Examples described herein provide enhanced computer and network based methods systems and techniques for aggregated communication across wide area networks to minimize failure and latency. Examples provide a Wide Area Network Aggregation System WANAS and corresponding technology WANA technology which provides an Application Programming Interface API to enable a client client code client logic and the like executing at a source computing site e.g. a datacenter to make a single request referred to herein as an aggregated request or a request for a federated operation for a resource which is then transformed into individual site requests by the API and then automatically propagated to physically disparate computing sites e.g. other datacenters using a single connection to a representative target of each site. Accordingly the site requests can be processed in parallel. The target receiving or recipient computing site is then responsible for fanning out e.g. spreading transmitting the requests for example in parallel forwarding to others and the like the site request using local requests to individual pods e.g. groups of servers virtualization infrastructure installations and the like that comprise the computing site. The pods within a computing site are typically connected to each other via a local area network LAN and thus communication between the pods within a site using local requests is typically faster than over a wide area network WAN . The fan out of the site request using local requests allows for parallelism of execution of the site request amongst all the pods in a computing site. Since the site requests are also sent in parallel from the client using a single connection to each remote computing site then the entire federated operation can be executed with a large degree of parallelism thus reducing latency.

In addition each request supported by the WANA technology specifies a request target that is a list of equivalent server endpoints that can respond to the request. The technology supports mechanisms on the receiving end which allow for failover to other endpoints that can respond. A connection iterator is defined for each request that knows how to cycle through the target list of equivalent endpoints and fail over to a next server in the list upon encountering a configurable timeout. For example the fan out of a site request to a local request to each pod specifies as a target a list of the equivalent servers within the pod that can respond. Failover of the request to the next server in the list is automatically managed by the connection iterator. This avoids multiple roundtrip messages across the WAN when errors such as failed or unavailable servers are encountered.

In addition requests are preferably executed across pre established connections formed by a connection manager which executes on each server. The connection manager provides and manages the life cycle of pre established connections between servers within a computing site. When one connection fails another can be established thus providing an additional type of failover load balancing and any other reason for choosing one connection over another configurable or not. Also the parallelism and automatic fan out provided by sharing pre established connections can reduce the number of multiple concurrent connections across the WAN required when a broker in one pod needs to communicate with brokers in multiple other pods at one or more remote sites for responding to a single request.

Although described with reference to operations federated across physical computing sites the techniques of a WANAS may be useful to support a variety of other messaging communications as well. For example the local requests within a computing site and within a pod may be different from the site requests and or from the initial aggregation request allowing for flexibility. In addition in some example WANASes a single WAN connection to a target computing site may support multiple requests concurrently or in sequence that are potentially from different clients within the same broker thus effectively providing a shared connection. Sharing a connection allows elimination of the setup and tear down time associated with establishing WAN connections thus effectively reducing latency by using an already existing connection.

In one example of the WANAS the computing sites constitute a virtualization environment comprising a multitude of datacenters that desire to communicate to locate or provision virtualization resources such as virtual desktops and to manage user connections across the remote sites. This allows for a cooperative virtualization environment that can span geographic locations important in the mobile workforce of today in order to ensure fast user response time and locate a user s applications data and work environment no matter where the user is located. Each datacenter typically supports one or more pods which are typically virtualization infrastructure installations an example of which is described below with respect to . In one example the pod unit of infrastructure is a VMWARE VIEW now VMWARE HORIZON VIEW installation. Each such installation for example may support a large number of users such as up to 10 000 users. Thus an entire data center may support hundreds of thousands of users enabling the WANAS to support millions of users. Using WANAS the pods cooperate across the datacenters to form a distributed brokering system a federation for support of virtualization activities and resources.

Each site request and is forwarded sent transmitted propagated etc. in parallel as opposed to sequentially to a different remote datacenter e.g. datacenters and . In particular each site request designates a list of all of the connections brokers in all of the pods in a target datacenter. Thus for example site request specifies that information is desired regarding the user s virtual desktops in datacenter and is sent to a target list of equivalent broker endpoints including all of the connection brokers shown in pods and 9 connection brokers . One of the pre established connections to broker is used to communicate the site request to one of the brokers that can represent the site here broker . Broker recognizes determines realizes calculates computes etc. that the request is a site request and so is responsible for not only obtaining a result of the inquiry for the installation it represents but aggregating its own result with results from all of the other installations in the datacenter to which it belongs. If for some reason broker is unavailable or does not respond within a determined time period the site request automatically will be sent by a connection iterator executing as part of the site request to another representative broker within the target datacenter based upon the target list of 9 connection brokers and information regarding pre established connections.

Each broker in a datacenter when receiving a site request is responsible for fan out of the site request into local requests to each of the other installations in the site. For example broker is responsible for turning the site request into a local request to send to each of its virtualization infrastructure installations pods and . Each local request designates a list of all of the connection brokers in the target pod so that any one of the connection brokers can respond and the first to respond with the requested state provides the answer for that installation. Thus for example when the site request is sent to site via a connection to broker in pod the receiving broker determines realizes calculates computes etc. that it is a site request to fan out and generates and forwards corresponding local requests to installations and . The local request sent to installation is sent to a request target list with broker first on the request target list as shown. Similarly the local request sent to installation is sent to a request target list with broker first on the request target list as shown. A connection iterator executing on broker controls failover to a next broker on the list when the targeted broker is unresponsive or unavailable. Site request is handled similarly. The connection broker receiving the site request determines that it is a site request to fan out and generates and forwards a corresponding local request to installation . The local request sent to installation is sent to a request target list with broker first on the request target list as shown. A connection iterator executing on broker controls failover to a next broker on the list when the targeted broker is unresponsive or unavailable. As noted earlier the requests sent via each local request and to each site request could vary. In any case the broker receiving the responses from the local requests accumulates aggregates composes combines etc. the responses into a single response to the site request which is ultimately returned to the requesting installation .

As noted above a pod is a unit of infrastructure and in the virtualization context may be a single installation such as a VMWARE VIEW or VMWARE HORIZON VIEW installation. is a block diagram illustrating use of an example general virtualization infrastructure installation for executing components of an example Wide Area Network Aggregation System in the computing environment illustrated in .

As shown in remote users for example users and may access centrally managed user desktops such as those implemented by virtual machines running in a datacenter using network e.g. a local area network or other private or publically accessible wide area network such as the Internet through any number of different types of devices. These virtual machines VMs are complete computation environments containing virtual equivalents of the hardware and system software components of a physical system and are typically implemented by an extensive virtualization infrastructure which includes a variety of software and hardware components.

The term desktop refers to a human interface environment through which users can launch interact with and manage applications settings and or data etc. Virtualized desktops may export user interfaces e.g. keyboard and mouse input audio and visual output to the user from virtual machines running remotely in the datacenter or locally on the client or in some cases using a hybrid. In the case of virtualization environments the client side of the desktop typically includes a bitmap representation of the desktop running wherever it is being run. Input to and output from the virtualized desktop are reflected in the bitmap representation on the client as it occurs on the virtualized desktop.

Remote access to virtualized desktops is generally provided to client devices through a Virtual Desktop Management VDM Server . The VDM Server provides virtual desktops to the remote user devices and manages the corresponding virtual machines through communications with a software interface of a Virtual Machine Management Server VMMS . The Virtual Machine Management Server VMMS is responsible for provisioning and maintaining the multitude of Virtual Machines VMs implemented across potentially a multitude of physical computers such as computer and . When a user wishes to access an existing virtual machine the user establishes a connection through the VDM Server and a virtual desktop is presented as a user interface on the user s client device through which communications are made with the underlying virtual machine. Component the connection broker represents the broker referred to in . Thus a single virtualization infrastructure installation referred to herein may comprise multiple instances of the architecture demonstrated in .

In the example shown as implemented for example by VMware Inc. s virtualization infrastructure architecture each physical computer for example computer contains the underlying hardware virtualization software here a hypervisor and one or more virtual machines for example VM and VM which each contain Agent Software guest system software labeled here as A in each VM box. The Agent Software is typically responsible for connecting each VM to the VDM Server and manages each desktop connection. It typically notifies the VDM Server upon each login logoff and disconnect. The Agent Software also provides support for remote devices such as USB devices etc.

The VMMS typically manages pools of compute resources used to run virtual machines on a set of clusters typically containing multiple servers with CPUs memory and communications hardware network . A virtual computer a virtual machine or VM when active consumes physical compute resources and is managed by a hypervisor layer such as hyperviser running on physical computer . The hypervisor manages physical resources as well as maintains virtual to physical hardware mappings. While some VMMS specialize in virtual machine management such as VMware s VirtualCenter Microsoft s Virtual Machine Manager Citrix s XenCenter others can manage both physical and virtual computers such as IBM s Director HP s OpenView and Microsoft s System Center Suite.

The Software Interface running on the VMMS communicates with these hypervisors e.g. hypervisor to provision and manage each VM. For example according to traditional virtualization techniques when a remote user e.g. user requests access to a particular existing desktop the VDM Server through its software communicates with the VMMS through its software interface to start the corresponding VM executing on an appropriate physical computer and to relay the user interface exported by the VM to the remote user so that the user can interact with the desktop. In some instances e.g. according to administrator policies when the desktop is exited or otherwise shutdown the VDM Server communicates with the VMMS to save the VM image to the datastore as appropriate and to de allocate physical and VM system resources as needed.

In general the VMMS Server provides interfaces to enable other programs such as the Pool Manager to control the lifecycle of the various virtual machines that run on a hypervisor. For example it supports operations for

In one example existing virtualization infrastructure providing by VMware Inc. a Virtual Desktop Management VDM Server includes an Administrative Console an Inventory Manager a Connection Broker and a Pool Manager . The Connection Broker allows a remote user such as remote user through a client device to select a type of virtualized desktop and initiate a desktop session or to access an existing connection to a virtualized desktop.

The Inventory Manager maintains a mapping of different user belongings in the system. For example user may be entitled to certain applications may have access to more than one desktop etc. The Inventory Manager also keeps track of the running virtual desktops in the system. The mappings may be stored using any number of mechanisms including using one or more directory servers accessible through network .

The Pool Manager component manages the complete lifecycle of virtual desktops. Desktops in a pool are grouped together based on similar software requirements. Desktop Administrators create logical desktops groups desktop pools that are provisioned typically from the same base image including the Agent Software. For example a desktop pool may include virtual machines that run the same set of software applications and run the same operating system. As yet another example a desktop pool may contain a set of cloned virtual machines that are identical in every aspect but are customized to include unique identity that includes for example a unique computer name IP MAC Address Domain membership Software license serial numbers OS specific security identifiers among other things. The base image can be a virtual machine or a template virtual machine that is created and or managed by the VMMS .

The software state of all the virtual machines in a desktop pool may be persistent or non persistent. Persistent desktops maintain the state of the files or applications stored inside the virtual machines. Non Persistent desktops are stateless desktops the desktop state is restored to the original state after every user session. In some cases the Desktop Administrator can define how frequently the revert to golden state operation should be performed. The restore to pristine image or revert to golden state operation can also be scheduled to occur based on certain conditions.

The Administrative Console typically provides a user interface for a Desktop Administrator to manage the configuration of desktop pools define user access policies manage ongoing maintenance software installed in the desktops etc. For example the Administrative Console may permit the Desktop Administrator to create desktop pools associate desktop pools with a VMMS provide other details such as the compute resources hosts clusters needed to run the VMs logical resource pools e.g. VI DRS Resource Pools for load balancing type of memory sharing configuration e.g. reserve memory for virtual machines or use memory overcommit storage used to provision the VMs identifying one or more datastores guest customization details like a custom naming scheme for VMs in the pool e.g. a marketingxxxx custom name such that the VMs in the pool are called marketing0001 to marketing9999 domain membership info e.g. to add VM to domain vmware vdi.com etc. associate a desktop pool with a master image or template define VM state policies keep VMs powered on suspend VM when user logs off etc. set up custom messages to be sent to end users when the desktops are put into maintenance mode e.g. for weekly patches hotfixes etc. and or manage the application registration and load process.

The Directory Server stores the persistent state required for managing the virtual desktops. For example the VMs in a desktop pool maybe associated with one or more users. The user identifiers for a pool may be stored in the directory server . The users may also be referenced through an external directory server such as Microsoft Active Directory Novell eDirectory IBM Tivoli Directory Server etc. In one example implementation the directory server may contain state for 

Examples and details of variants of virtualization architectures such as that described with reference to can be found in U.S. patent application Ser. No. 11 395 012 entitled Virtualized Desktop Allocation System Using Virtual Infrastructure filed Mar. 31 2006 U.S. patent application Ser. No. 12 058 269 entitled Synchronized Clones filed Mar. 28 2008 and in U.S. Patent Publication No. 2009 0216975 entitled Extending Server Based Desktop Virtual Machine Architecture to Client Machines filed Feb. 23 2009.

As described above each aggregate request generated by a client using the WANA API may be transformed into one or more site requests that are transmitted in parallel to all of the other remote sites e.g. datacenters that are configured to be connected to the source or host site via a WAN. In one example WANAS a connection between each of the sites is established ahead of time e.g. pre established before the transmissions occur and may even be practically speaking permanent between sites. That is from the source site s perspective a connection to each remote site is always available although the exact connection configuration which server is the endpoint of the connection may vary based upon configurations controlled by a the local connection manager at the server. Supporting pre established connections removes connection setup cost at request time. Other example WANASes may support just in time connections or other configurations.

In an example WANAS a connection manager is responsible for managing the life cycle of all of its connections over the WAN and will load balance and rotate as needed between servers over time. The connection manager may even pool connections allowing for their re use for multiple potentially concurrent requests. In addition pod health may be used to determine which endpoints may be better candidates for connection attempts. It can use health information to determine which endpoints are available and which are not available for connection helping to reduce the failure rate for connection attempts. The connection manager may even proactively load balance requests across a set of endpoints by creating new connections to endpoints in advance of them being needed and retiring old connections when an equivalent connection is available. In this way a particular requesting server may cycle around the possible endpoints in a target of a site request. Since most endpoints are also requestors of other site and or local requests endpoint load can accordingly be spread across all endpoints and newly available endpoints can handle their share of the request load. As mentioned in one embodiment a connection manager is associated with the server. In other embodiments connection managers may be associated with sites groups of servers etc.

A cross WAN connection is initially set up between a source server VDM server on site and server however these other connections between source server and between server or server may be used or set up as needed in some configurations to effectuate the site request as indicated by the dotted lines. Recall that a connection iterator associated with the request at the source end is responsible for such failover to other servers targeted in the request as needed. Similarly WAN connection is initially set up between a source server on site and server however other connections between source server and between server and may be used or set up as needed in some configurations to effectuate the site request as indicated by the dotted lines. Connections and are controlled by a connection manager component associated with the source server on site . The connection manager is responsible for the life cycles of connections to other servers and may perform functions described above such as load balancing rotating connections between the pods to reduce chance of failure and the like.

As mentioned with respect to the aggregation may also result in one or more local requests within datacenter . For example if there are additional pods installations within datacenter the aggregation will generate a local request to each of these pods. Local request exemplifies one such local request. It has its own iterator which is used to iterate as needed through a target list of local VDM servers not shown within site . There is one such local request for each pod.

The site request once received for example request and is then further transformed into inter pod messages referred to here as local requests to produce the parallelism or fan out referred to earlier. That is the server that receives the site request will determine that it is a high level request that needs to be further distributed to the other pods in the site in order to determine an answer to the site request. This can be done recursively or equivalently by iteration as each server knows that it has either received a site request and needs to produce fanned out requests or it has received a local request and needs to perform the requested operation which may be for example to determine status of a requested resource open a connection to a virtual machine etc. Some typical requests in a virtualization environment include 

The fan out yields advantages with respect to the latency of federated operations. Because each local request can be generated and sent out at approximately the same time near concurrently the processing of performing the federated operation is done in parallel. This ultimately reduces the time needed to respond to the user who was for example merely trying to find out what desktops the user was already running.

It is notable for failover purposes that any server can be used as a representative of the site in order for communication to occur. Similarly local requests are generated to a pod with a list of targets that are equivalently able to respond to the request. Thus unlike a traditional message which has an explicit endpoint that must be available in order to respond the local requests generated in response to a site request to produce pod message fan out have a list of endpoints that can handle the local request and thus provide additional failover capabilities. Each site and each local request has a connection iterator for example connection iterators and which help deliver an actual message e.g. a LAN message to a specific endpoint that is ready and able to respond. These actual messages constitute sub requests. More than one sub request may be needed and sent if the connection iterator detects that a sub request to a specific endpoint has failed.

Although the techniques of wide area network aggregation are generally applicable to any type of distributed task across wide area networks the phrase federated operation is used generally to imply any type of operation that can be distributed and not just tasks within virtualization environments. Essentially the concepts and techniques described are applicable to any type of distributed operation that promulgates requests across a wide area network. Also although certain terms are used primarily herein other terms could be used interchangeably to yield equivalent systems and examples. In addition terms may have alternate spellings which may or may not be explicitly mentioned and all such variations of terms are intended to be included.

Examples described herein provide applications tools data structures and other support to implement a Wide Area Network Aggregation System to be used for distributing virtualization operations. Other examples of the described techniques may be used for other purposes including for federated tasks more generally. In the following description numerous specific details are set forth such as data formats and code sequences etc. in order to provide a thorough understanding of the described techniques. The examples described also can be practiced without some of the specific details described herein or with other specific details such as changes with respect to the ordering of the logic different logic etc. Thus the scope of the techniques and or functions described are not limited by the particular order selection or decomposition of aspects described with reference to any particular routine module component and the like

In block the logic receives a designated operation from the aggregation request such as a federated operation relating to virtualization resources for example a status request or a provisioning request. Identification of the user and other parameters may also be designated in the API call.

In blocks the logic executes a loop to transform the aggregation request to one or more inter site requests. In particular in block the logic starts the loop by choosing a remote site as the current site to send a request to. As described earlier the remote site is a physically disparate site and may for example be a datacenter remotely located from the client site. Each source broker can consult information e.g. network topology data to determine a list of brokers configured for each remote site. Further the source broker can query its connection manager to determine to which broker endpoints in the remote site the source broker already has an established connection and to determine those connections to use in preference to others. In the case where no connections yet exist from the source broker to a broker in the remote site the logic can request a new connection to an endpoint in the remote site.

In block the logic determines whether there are still more remote sites to process and if so continues in block otherwise continues in block to generate and process local requests within the source broker s site. In particular in block the logic generates a site request to send to a server e.g. connection broker executing on the server in the remote site. As described this request is preferably sent via a pre established connection to a broker endpoint in a server of the remote site being processed however the request is sent to a target list of all the broker endpoints present in the remote site since any one of the brokers can act as a representative e.g. can substitute for the other are equivalent act as a proxy etc. for processing the site request. Again to which broker endpoint to send the request is chosen with the aid of a connection manager associated with the source broker. If this connection fails for whatever reason and the broker endpoint that is connected doesn t process the site request in a timely manner e.g. by a configurable time out period then the source broker can reissue the request using a different broker endpoint from this target list.

In block the logic forwards e.g. sends transmits etc. the generated site request to the initially targeted broker endpoint and then returns to the beginning of the loop in block to process the next remote site.

In blocks the logic executes a loop to transform the aggregation request to one or more local pod requests as appropriate. In particular as described with reference to site requests are sent to remote sites and local pod requests are sent to pods with the source site that are other than the pod that contains the source broker. See for example requests and in . In block the logic determines whether there are other pods other than the pod containing the source broker within the client site that also need to be queried and starts the local request loop by choosing a local pod to send a request to. In block the source broker logic determines whether there are still other pods remaining to be processed and if so then the logic continues in block otherwise it awaits responses to the requests already sent and continues to block .

In block the logic generates a local request to each such determined pod and specifies as a target list of endpoints all of the broker endpoints in that pod. Like the site request target list the target list of the local request contains a set of equivalent e.g. representative substitute proxy broker endpoints that can respond to the local request. The local request may potentially be in a format different from the site requests formulated above in block . In block the logic then sends the generated local request to the initially targeted broker endpoint in the current pod. The logic then continues back to the

In block the logic awaits a separate response from each site request and from each local request. Typically the logic executes a timer based event handler where either a response from one of the requests triggers processing or a timeout for that request is triggered. Blocks through describe the logic executed to either process a response or iterate over the set of potential target broker endpoints for a particular request local or site . This iteration as described with respect to is typically executed by a connection iterator associated with each request. Different implementations may yield slightly modified logic.

In block after the logic detects that a response to a request has arrived or a corresponding timeout has been triggered the logic determines in block whether a timeout has triggered an event and if so continues in block otherwise continues in block .

In block the logic determines whether it has received a response to a local request or a site request. If so then in block the logic determines that it has received a result for that request and returns it as the result of that request. If not then no result has been received a timeout has not occurred but a response was received so it was likely a failed result and the logic continues in block to return a failure code for that request. Note that this block would not be reached unless some sort of response was returned or a timeout occurred otherwise the request would still be pending a response. 

In block after it is determined that a timeout for a particular request has been triggered the logic determines whether there are additional equivalent broker endpoints in the target list of the request that can be tried and if so continues in block to send the request to a next endpoint in that target list and reset the timeout for the request. Otherwise the logic determines that there are no further equivalent endpoints in the target list and so in block the logic returns a failure code for that request.

Of note the logic in blocks through is processed for each request. In some embodiments the timeouts are configurable and may be on a request level per request type etc. To aid in correlating a response with a particular request a correlation identifier ID may be indicated in each request and in each response. In addition each request instance a message to a particular endpoint may contain other values such as a message header and or a message body either of which may comprise key value pairs giving information about the request.

Once all of the responses including results and or failure codes have been determined for each of the requests the logic proceeds to block . In block the logic receives the responses to local requests and in block the logic receives the response to the issued site requests. In block the logic determines whether an overall configurable timeout has occurred for the aggregation and if not continues to await responses otherwise continues in block .

In block the logic performs the designated operation see block on the source broker which represents a result for the client source pod. Of note this logic can be performed at different times and much earlier in the overall flow if desired. In block the logic aggregates e.g. accumulates adds sums collects etc. all of the responses to the site and local requests with the local result for the source pod. In block the logic sends returns forwards etc. the aggregated response as the response to the aggregation federation request which is a response to the API call.

More specifically in block the logic receives a site request with the requested operation and any other parameters sent in the request. The requested operation and parameters need not be identical to those specified in the initial aggregation request.

In block the logic processes the requested operation e.g. get the status of all of user U s connections at the receiving broker on behalf of the receiving pod. Recall that if this request fails or timeout the source broker will reissue the request using the logic of to a different broker endpoint of the receiving pod or even a different pod in the receiving remote site. This operation may be performed at many different times in the logic of processing the site request.

In blocks the logic executes a loop to transform the site request to one or more local pod requests to the other pods of the receiving site. This constitutes the fan out of the site request to all of the pods present and available in the receiving site. In particular in block the logic starts the loop by choosing a first pod as the current pod to send a request to. If a request needs to be sent to every other pod in the site the order the pod requests are processed may not matter. In block the logic determines whether there are still more pods to process and if so continues to block otherwise continues to await responses at marker to block of .

In block the logic generates a local request to forward to the pod being processed for example a current designated virtualization installation. The local request is generated with a target list of a set of equivalent broker endpoints present in that pod for example all of the connection brokers in a single virtualization infrastructure installation .

In block the logic forwards the generated local request to the pod being processed and then returns to the beginning of the loop in block to process the next pod in the receiving cite. As explained above although the loop shown here initiates these local requests one after another they are preferably initiated as much as possible in parallel and executed by the individual pods virtually simultaneously. The intent is to provide parallel processing of local requests. In other words they are not intended to be executed sequentially when one desires to obtain the best performance possible out of the WANAS.

As described with reference to the pod that has received the site request then awaits responses to the fanned out local requests. The logic reissues local requests to a different broker endpoint in the other pod when a timeout occurs or the request fails. This logic continues until responses or failed requests have been received.

Specifically in block the logic detects that a response to one or more local requests has been received and continues in block . In block the logic aggregates e.g. accumulates adds sums collects etc. the responses to all of the local requests with the local result into an overall response to send in response to the received site request and in block returns this overall response. Marker X shows the response being returned into block of .

In particular in block a designated representative server of a pod receives a local request with a requested operation.

In block the receiving broker handles the received requested operation on behalf of the receiving pod. In block the receiving broker sends the result as a response to the local request and proceed as marker D block of which describes a triggered event resulting from a result being generated . In

The computing system may comprise one or more server and or client computing systems and may span distributed locations. In addition each block shown may represent one or more such blocks as appropriate to a specific example WANAS or may be combined with other blocks. Moreover the various blocks of the WANAS may physically reside on one or more machines which use standard e.g. TCP IP or proprietary interprocess communication mechanisms to communicate with each other.

In the example shown computer system comprises a computer memory memory a display one or more Central Processing Units CPU Input Output devices e.g. keyboard mouse CRT or LCD display etc. other computer readable media and one or more network connections . The WANAS is shown residing in memory . In other example WANASes some portion of the contents some of or all of the components of the WANAS may be stored on and or transmitted over the other computer readable media . The components of the WANAS preferably execute on one or more CPUs and manage the generation propagation and operation of federated operations as described herein. Other code or programs and potentially other data repositories such as data repository also reside in the memory and preferably execute on one or more CPUs . Of note one or more of the components in may not be present in any specific implementation. For example some example WANASes embedded in other software may not provide means for user input or display.

In a typical example the WANAS includes one or more Application Programming Interfaces API one or more connection managers and one or more aggregation federated request instances . In at least some examples some components are provided external to the WANAS and available potentially over one or more networks . Other and or different modules may be implemented. In addition the WANAS may interact via a network with application or client code that uses results of the performed federated operations one or more client computing systems and or one or more third party information provide systems such as purveyors of some of the virtual information used in virtual desktop state connection tables etc. data repository . Also of note the state data repository may be provided external to the WANAS as well for example in a knowledge base accessible over one or more networks .

In an example WANAS components modules of the WANAS are implemented using standard programming techniques. For example the WANAS may be implemented as a native executable running on the CPU along with one or more static or dynamic libraries. In other example WANASes the WANAS may be implemented as instructions processed by a virtual machine. A range of programming languages known in the art may be employed for implementing such example example WANASes including representative implementations of various programming language paradigms including but not limited to object oriented functional procedural scripting and declarative.

The example WANASes described above may also use well known or proprietary synchronous or asynchronous client server computing techniques. Also the various components may be implemented using more monolithic programming techniques for example as an executable running on a single CPU computer system or alternatively decomposed using a variety of structuring techniques known in the art including but not limited to multiprogramming multithreading client server or peer to peer running on one or more computer systems each having one or more CPUs. Some example WANASes may execute concurrently and asynchronously and communicate using message passing techniques. Equivalent synchronous examples are also supported.

In addition additional programming interfaces to the data stored as part of the WANAS e.g. in the data repository can be available by standard mechanisms such as through C C C and Java APIs libraries for accessing files databases or other data repositories through scripting languages such as XML or through Web servers FTP servers or other types of servers providing access to stored data. The data repository may be implemented as one or more database systems file systems or any other technique for storing such information or any combination of the above including implementations using distributed computing techniques.

Also portions of an example WANAS may be implemented in a distributed environment comprising multiple even heterogeneous computer systems and networks. Different configurations and locations of programs and data are contemplated for use with techniques of described herein. In addition the server may be physical or virtual computing system and may reside on the same physical system. Also one or more of the modules may themselves be distributed pooled or otherwise grouped such as for load balancing reliability or security reasons. A variety of distributed computing techniques are appropriate for implementing the components of the illustrated WANAS in a distributed manner including but not limited to TCP IP sockets RPC RMI HTTP Web Services XML RPC JAX RPC SOAP etc. and the like. Other variations are possible. Also other functionality could be provided by each component module or existing functionality could be distributed amongst the components modules in different ways yet still achieve the functions of an WANAS.

Furthermore in some examples some or all of the components of the WANAS may be implemented or provided in other manners such as at least partially in firmware and or hardware including but not limited to one or more application specific integrated circuits ASICs standard integrated circuits controllers executing appropriate instructions and including microcontrollers and or embedded controllers field programmable gate arrays FPGAs complex programmable logic devices CPLDs and the like. Some or all of the system components and or data structures may also be stored as contents e.g. as executable or other machine readable software instructions or structured data on a computer readable medium e.g. a hard disk memory network other computer readable medium or other portable media article to be read by an appropriate drive or via an appropriate connection such as a DVD or flash memory device to enable the computer readable medium to execute or otherwise use or provide the contents to perform at least some of the described techniques. Some or all of the components and or data structures may be stored on tangible non transitory storage mediums. Some or all of the system components and data structures may also be stored as data signals e.g. by being encoded as part of a carrier wave or included as part of an analog or digital propagated signal on a variety of computer readable transmission mediums which are then transmitted including across wireless based and wired cable based mediums and may take a variety of forms e.g. as part of a single or multiplexed analog signal or as multiple discrete digital packets or frames . Such computer program products may also take other forms in other examples. Accordingly examples described in this disclosure may be practiced with other computer system configurations.

All of the above U.S. patents U.S. patent application publications U.S. patent applications foreign patents foreign patent applications and non patent publications referred to in this specification and or listed in the Application Data Sheet are incorporated herein by reference in their entirety.

From the foregoing it will be appreciated that although a specific example Wide Area Network Aggregation System has been described herein for purposes of illustration various modifications may be made without deviating from the spirit and scope of the present disclosure. For example the methods and systems for performing federated operations discussed herein are applicable to other architectures other than a virtualization specific architecture. Also the methods and systems discussed herein are applicable to differing protocols communication media optical wireless cable etc. and devices such as wireless handsets electronic organizers personal digital assistants tablets portable email machines game machines pagers navigation devices such as GPS receivers etc. 

