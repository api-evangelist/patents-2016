---

title: Restoring redundancy in a storage group when a storage device in the storage group fails
abstract: Provided are a system, computer program, and method for restoring redundancy in a storage group when a storage device in the storage group fails. In response to detecting a failure of a first storage device in a storage group, wherein the storage group stores each of a plurality of extents in the first storage device and a second storage device to provide redundancy, a determination is made whether a spare storage device that has a storage capacity less than that of the storage group. One of the extents in a storage location in the second storage device that is beyond an upper limit of positions in the spare storage device is moved to a new storage location. The spare drive is incorporated into the storage group to provide redundant storage for the storage group, wherein the extents in the storage group are copied to the spare drive.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09588856&OS=09588856&RS=09588856
owner: International Business Machines Corporation
number: 09588856
owner_city: Armonk
owner_country: US
publication_date: 20160330
---
This application is a continuation of U.S. patent application Ser. No. 13 912 153 filed Jun. 6 2013 which is a non provisional application that claims priority benefits under Title 35 United States Code Section 119 a d from United Kingdom Patent Application entitled AN APPARATUS FOR RESTORING REDUNDANCY by Matthew J. FAIRHURST and Eric J. BARTLETT having United Kingdom Patent Application Serial No. GB 1211041.7 filed on Jun. 22 2012 which United States and United Kingdom Patent Applications are incorporated herein by reference in their entirety.

Provided are a system computer program and method for restoring redundancy in a storage group when a storage device in the storage group fails.

RAID systems store and access multiple heterogeneous individual hard drives as if the array were a single larger disk. Distributing data over these multiple disks reduces the risk of losing the data if one drive fails and it also improves access time. RAID was developed for use in transaction or applications servers and large file servers. Currently RAID is also utilized in desktop or workstation systems where high transfer rates are needed.

Systems today are often sold with a small number of drive types typically with different performance characteristics e.g. HDD SSD . Over time drive technology changes and different capacities of drives become available. For these or other reasons clients may end up with a variety of drive capacities for example a mixture of 300 GB and 600 GB drives.

Most RAID algorithms require that drives in the same array share the same capacity. This means that if in the example a 600 GB drive fails then a 300 GB drive cannot be used as a hot spare drive wherein a hot spare drive can be used to restore redundancy in the event of a drive failure because its capacity is too small.

A first solution to this problem is to maintain separate pools of 300 GB and 600 GB spare drives. The drawback to this solution is that more spare drives may be required due to different capacities than would be required from a coverage plan based purely on reliability. For example 300 GB spares cannot be used once all 600 GB spare drives are used up if a 600 GB drive fails.

A second solution to this problem is to provision purely 600 GB spare drives that can cover both array sizes. The drawback to this solution is that space is wasted when a 600 GB spare drive replaces a 300GB drive. Further a 600 GB spare drive is typically the most expensive fastest and largest type of drive in the system. US 20080109601 discloses systems and methods for RAID Restriping. One method includes selecting an initial RAID device for migration based on at least one score creating an alternate RAID device moving data from the initial RAID device to the alternate RAID device and removing the initial RAID device.

Provided are a system computer program and method for restoring redundancy in a storage group when a storage device in the storage group fails. In response to detecting a failure of a first storage device in a storage group wherein the storage group stores each of a plurality of extents in the first storage device and a second storage device to provide redundancy a determination is made whether a spare storage device that has a storage capacity less than that of the storage group. One of the extents in a storage location in the second storage device that is beyond an upper limit of positions in the spare storage device is moved to a new storage location. The spare drive is incorporated into the storage group to provide redundant storage for the storage group wherein the extents in the storage group are copied to the spare drive.

According to a first aspect there is provided an apparatus for restoring redundancy in a system having data comprising one or more extents storable in a plurality of RAID drives associated with a group wherein the data has redundancy across the group the apparatus comprising a failure monitor responsive to detection of a failure of a RAID drive and responsive to a determination that a spare drive does not have a capacity that is greater than or equal to the group s capacity operable to determine whether there are available extents associated with at least one of a functional RAID drive associated with the group a RAID drive associated with another group and a non RAID drive and a virtual volume migrator responsive to a determination that there are available extents operable to migrate an extent occupying space associated with the functional RAID drive associated with the group having a position that is higher than an upper limit of the spare drive to an available extent such that a spare drive having a capacity that is not greater than or equal to the group s capacity can be used to replace the failed drive wherein an available extent associated with the functional RAID drive associated with the group has a position that is lower than the upper limit of the spare drive.

According to a second aspect there is provided a method for restoring redundancy in a system having data comprising one or more extents storeable in a plurality of RAID drives associated with a group wherein the data has redundancy across the group the method comprising the steps of determining in response to detection of a failure of a RAID drive and in response to a determination that a spare drive does not have a capacity that is greater than or equal to the group s capacity whether there are available extents associated with at least one of a functional RAID drive associated with the group a RAID drive associated with another group and a non RAID drive and migrating in response to a determination that there are available extents an extent occupying space associated with the functional RAID drive associated with the group having a position that is higher than an upper limit of the spare drive to an available extent such that a spare drive having a capacity that is not greater than or equal to the group s capacity can be used to replace the failed drive wherein an available extent associated with the functional RAID drive associated with the group has a position that is lower than the upper limit of the spare drive.

According to a third aspect there is provided a computer program comprising program code means adapted to perform all the steps of the method above when said program is run on a computer.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fibre a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fibre cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Java and all Java based trademarks and logos are trademarks or registered trademarks of Oracle and or its affiliates Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

Referring to there is shown a prior art storage virtualisation system comprising a storage controller which provides virtualisation services and presents one or more virtual volumes to a user by combining extents wherein contiguous blocks are grouped into one or more sets when the sets are concatenated to create a single logical virtual volume such sets are termed extents . Typically a virtual volume uses capacity extents from multiple sources and a user is unaware of the multiple sources.

Such sources are shown in comprise a first set of directly attached e.g. storage systems managed by a RAID group manager which is operable to control access to RAID groups and recover from drive failures drives that are arranged into one or more RAID groups wherein each drive in the same RAID group has the same capacity and each RAID group s usable capacity is split into extents of equal size. As drives come in various capacities if a RAID group comprises members having different capacities the lowest capacity of the group is used for each of the drives in the group termed herein a RAID group s member capacity . This can result in space associated with the larger drives of the RAID group being wasted.

A further source is depicted in and comprises a second set of directly attached drives that are assigned as spare drives it should be understood that RAID groups store data redundantly on a plurality of drives so that the data can be read back even if some of the drives fail. In an example where the same data is stored mirrored to two drives RAID 1 mirroring if either drive fails the data can be read from the other drive. A spare drive is a drive that is not part of a RAID group. When a drive in a RAID group fails in a hot spare takeover process the RAID group manager automatically uses a spare drive to take the place of the failed drive.

Typically the functional drive and the failed drive have identical contents. The RAID group manager makes a decision that a particular spare drive should replace the failed drive this is a logical replacement meaning that reads and writes are sent to each of the functional drive and the spare drive instead of each of the functional drive and the failed drive. Subsequently the RAID group has to be resynchronized rebuilt by copying the data from the functional drive to the spare drive in order to re establish the mirror during this time the RAID group manager is responsible for making sure that reads to the RAID group do not use un copied areas of the spare drive.

The use of a spare drive allows for a time period in which a RAID group does not have redundancy to be minimized.

It should be understood that a spare drive can only be used by a RAID group if the spare drive s capacity is as least as large as the RAID group s member capacity.

A further source is depicted in and comprises externally attached storage systems e.g. storage systems that are not managed by the RAID group manager but that present storage to the system using e.g. a fibre channel SAN and provide extents to the system .

A virtual volume has no particular affinity to particular RAID groups spare drives and or externally attached storage systems.

A virtual volume manager is operable to define which extents from which particular RAID groups and or externally attached storage systems are associated with extents on particular virtual volumes . The virtual volume manager executes this process by for each virtual volume defined by a user of the system holding a table recording for each extent of a virtual volume a current location of the extent. The location of the extent comprises either an identity of a RAID group and a location of the extent on the RAID group or an identity of an externally attached storage system and a location of the extent on the externally attached storage system.

The system typically comprises unused extents that the virtual volume manager is e.g. operable to assign to further virtual volumes and use for other purposes such as for storing system configuration data.

Referring to there is shown a modified storage virtualisation system of the system shown in according to a preferred embodiment. The system comprises a storage controller that presents one or more virtual volumes . The storage controller further comprises a virtual volume manager having a virtual volume migrator and a RAID group manager having a RAID group member failure monitor .

The system is operable to access one or more RAID groups one or more spare drives and one or more externally attached storage systems .

With reference to at step the RAID group member failure monitor makes a determination as to whether a member a drive of a RAID group has failed. For example the RAID group member failure monitor detects a failure by issuing a read or write command to a drive and the drive subsequently responds by indicating that it has failed or by detecting that a drive is no longer responding to commands.

If the RAID group member failure monitor makes a determination that a member of a RAID group has not failed nothing further is done and the process passes back to step .

If the RAID group member failure monitor makes a determination that a member of a RAID group has failed the process passes to step where the RAID group member failure monitor makes a determination as to whether any of the spare drives has a capacity that is greater than or equal to the associated RAID group s member capacity. The RAID group member failure monitor makes the determination by keeping in memory the capacity of each spare drive and comparing the numeric capacities with the RAID group s member capacity.

In response to a determination that a spare drive has a capacity that is greater than or equal to the RAID group s member capacity the RAID group manager executes at step a hot spare takeover process and the process passes back to step .

In response to a determination that a spare drive does not have a capacity that is greater than or equal to the RAID group s member capacity the process passes to step where the RAID group member failure monitor makes a determination using arithmetic operations as to whether the RAID group s member capacity is less than the capacity of the spare drive and if so whether there are enough free extents in the system to hold each of the extents currently allocated to the one or more virtual volumes . Further before migrating extents the RAID group member failure monitor may need to ensure that there are enough free extents associated with the system s RAID groups and externally attached storage systems to store the extents currently allocated to the one or more virtual volumes . Step will be described in more detail with reference to the examples below.

If the RAID group member failure monitor makes a determination that at least one of a spare drive does not have a capacity greater than the RAID group s member capacity and that if the RAID group s member capacity were to greater than the spare drive and there are not enough free extents in the system nothing further is done and the process passes back to step . In this case the system is vulnerable until an adequate replacement drive is found.

If the RAID group member failure monitor makes a determination that if the RAID group s member capacity is greater than the capacity of the spare drive and there are enough free extents in the system then the process passes to step where the virtual volume migrator determines whether there are one or more extents occupying space associated with the remaining functional drives in the same RAID group as that of the failed drive that are at a position that is higher than the upper limit of the spare drive.

If the virtual volume migrator makes a determination that there are one or more extents having a position higher than the upper limit of the spare drive at step the virtual volume migrator migrates the one or more extents to otherwise unused extents in the system . For example the virtual volume migrator reads an extent from its old location and writes the extent to its new location when this copy is complete the virtual volume migrator updates its records to indicate the new location of the virtual volume s extent.

If the virtual volume migrator migrates the one or more extents to a functional drive associated with the same RAID group as that of the failed drive the virtual volume migrator migrates the one or more extents such that the extents are located at a position that is lower than the upper limit of the spare drive. If the virtual volume migrator migrates the one or more extents to a functional drive that is not associated with the same RAID group as that of the failed drive and or migrates the one or more extents to an externally attached storage system the virtual volume migrator migrates the one or more extents to any available location.

At step if the virtual volume migrator makes a determination that there are no one or more extents having a position higher than the upper limit of the spare drive the process passes to step in

With respect to at step the RAID group member failure monitor modifies the RAID group s member capacity to match the capacity of the spare drive as redundancy cannot be provided for the remaining capacity by assigning a value associated with the capacity of the spare drive to a memory location associated with the RAID group s member capacity.

A worked example of the process of the preferred embodiment will now be described with reference to the figures.

With reference to the system comprises a virtual volume a RAID group and a spare drive . The virtual volume comprises three 20 GB extents and having an overall capacity of 60 GB. The RAID group comprises a first drive and a second drive each having a 400 GB capacity. The RAID group uses RAID 1 mirroring and thus its overall capacity is 400 GB. The extents of the virtual volume occupy space on the RAID group . The spare drive has a capacity of 200 GB. In the example herein there are no externally attached storage systems .

The map shows that the three extents and making up the virtual volume are each stored on the same RAID Group at different positions within the RAID Group .

It should be noted that in the present example the extents associated with the virtual volume are not arranged in the same order on the RAID group as the order associated with the virtual volume . Further contrary to how the extents are represented on the virtual volume the extents are not located adjacent to each other on the RAID group . From the example it should be noted that the order and layout of extents on the RAID group are arbitrary.

With reference to at step the RAID group member failure monitor makes a determination as to whether a member of a RAID group has failed.

In the example herein the second drive fails a representation is shown in whereby only one physical copy of each of the extents and reside in the RAID group namely on the first drive . In this state the virtual volume is still intact but its extents have no more redundancy. If the first drive fails the virtual volume will be lost. Without the described embodiments if there are no spare drives currently available to act as a replacement drive for the second drive this risky situation may persist indefinitely.

At step the RAID group member failure monitor makes a determination as to whether any of the spare drives has a capacity that is greater than or equal to the RAID group s member capacity. In the example herein as the spare drive has a capacity of 200 GB and the RAID group s member capacity is 400 GB the process passes to step where the RAID group member failure monitor makes a determination as to whether a spare drive has a capacity that is less than the RAID group s member capacity and if the RAID group s member capacity were to equal the lesser capacity of the spare drive whether there are enough free extents in the system to hold each of the extents currently allocated to the one or more virtual volumes .

As there are enough free extents on the first drive namely a functional drive associated with the same RAID group as that of the failed drive to migrate extents and the process passes to step where the virtual volume migrator determines whether there are one or more extents occupying space associated with the remaining functional drives in the same RAID group as that of the failed drive that are at a position that is higher than the upper limit of the spare drive which has a capacity of 200 GB . At step it is determined by the virtual volume migrator that the positions of the extents and associated with the first drive are higher than the upper limit of the spare drive and at step the virtual volume migrator migrates the one or more extents on the first drive . In the example herein the virtual volume migrator migrates extents and so that they are located within the first 200 GB of the first drive . A representation of the resulting first drive is shown in where the ordering of the extents following migration is not important.

Following the migration the virtual volume manager updates the virtualization map for the virtual volume as follows 

The updated map shows extents and have been migrated to a position such that they are located at a position that is lower than the upper limit of the corresponding spare drive .

Advantageously described embodiment exploit virtualization knowledge to migrate used extents away from the part of a drive that cannot be protected by a small spare drive when migrating such extents to another drive associated with the same group as a failed drive. Advantageously described embodiments can also migrate such extents to drives associated with different groups and or to externally attached storage systems.

At step the RAID group member failure monitor modifies the RAID group s member capacity to match the capacity of the spare drive that is the RAID group s member capacity of 400 GB is modified to equal 200 GB .

At step a hot spare takeover process is completed by using the spare drive in replacement of the second drive and copying the data from the first drive to the spare drive . This results in the system being redundant again as there two physical copies of each virtual volume extent now exist. The resulting system is shown in whereby represents space on the first drive that is temporarily not available for use the effective capacity of the first drive is diminished until a larger drive can be installed at which point such larger drive is exchanged with the spare drive so that redundancy can be provided for the further 200 GB of the first drive s capacity.

With reference to the above example and step it should be noted that there are enough free extents on the first drive to migrate extents and away from the dead zone . In another example if the entire capacity of the first drive was being used and if the RAID group member failure monitor determines that there are not enough free extents on the system s other RAID groups and or externally attached storage systems to store the extents that need to be migrated the system requires an adequately sized spare drive to store the migrated extents.

Advantageously described embodiments provide a technique for allowing smaller hot spare drives to be used to restore redundancy to customer data in an array of larger drives in conjunction with a virtualization function. The virtualization function is used to move customer data away from the portion of a drive that cannot be protected by the smaller drive when migrating such extents to another drive associated with the same group as a failed drive. Advantageously the described embodiments can also migrate such extents to drives associated with different groups and or to externally attached storage systems. One advantage offered by the preferred embodiment is that wider spare coverage can be offered and the system can maintain redundancy under more failure scenarios. Further fewer larger drives are required as spare drives saving significant costs.

