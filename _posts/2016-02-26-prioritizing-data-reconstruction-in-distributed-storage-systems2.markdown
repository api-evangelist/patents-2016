---

title: Prioritizing data reconstruction in distributed storage systems
abstract: A method of prioritizing data for recovery in a distributed storage system includes, for each stripe of a file having chunks, determining whether the stripe comprises high-availability chunks or low-availability chunks and determining an effective redundancy value for each stripe. The effective redundancy value is based on the chunks and any system domains associated with the corresponding stripe. The distributed storage system has a system hierarchy including system domains. Chunks of a stripe associated with a system domain in an active state are accessible, whereas chunks of a stripe associated with a system domain in an inactive state are inaccessible. The method also includes reconstructing substantially immediately inaccessible, high-availability chunks having an effective redundancy value less than a threshold effective redundancy value and reconstructing the inaccessible low-availability and other inaccessible high-availability chunks, after a threshold period of time.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09535790&OS=09535790&RS=09535790
owner: Google Inc.
number: 09535790
owner_city: Mountain View
owner_country: US
publication_date: 20160226
---
This U.S. patent application is a continuation of and claims priority under 35 U.S.C. 120 from U.S. patent application Ser. No. 14 169 274 filed on Jan. 31 2014 which is hereby incorporated by reference in its entirety.

A distributed system generally includes many loosely coupled computers each of which typically includes a computing resource e.g. one or more processors and or storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s storage resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes. Alternatively Remote Direct Memory Access RDMA primitives may be used to transfer data from server hardware to client processes.

One aspect of the disclosure provides a method of prioritizing data for recovery in a distributed storage system. The method includes for each stripe of a file that includes a plurality of stripes having chunks determining using a data processing device whether the stripe includes high availability chunks or low availability chunks and determining using the data processing device an effective redundancy value for each stripe of the file. The effective redundancy value is based on the chunks and any system domains associated with the corresponding stripe. The distributed storage system has a system hierarchy including system domains. Moreover each system domain has an active state or an inactive state. Chunks of a stripe associated with a system domain in the active state are accessible whereas chunks of a stripe associated with a system domain in the inactive state are inaccessible. The system domain may enter the inactive state during scheduled maintenance unscheduled failures or other reasons. When chunks of a stripe become inaccessible the method includes reconstructing substantially immediately using the data processing device inaccessible high availability chunks having an effective redundancy value less than a threshold effective redundancy value. Moreover when chunks of a stripe become inaccessible the method also includes reconstructing after a threshold period of time 1 inaccessible low availability chunks and 2 inaccessible high availability chunks having an effective redundancy value greater than or equal to the threshold effective redundancy. Therefore certain high availability chunks at high risk of becoming lost based on the effective redundancy value receive relatively quicker reconstruction than low availability chunks or other high availability chunks that are not at high risk of becoming lost.

Implementations of the disclosure may include one or more of the following features. In some implementations the method further includes updating the effective redundancy value for each stripe of the file associated with a system domain when the system domain is in the inactive state. The threshold period of time may be between about 15 minutes and about 30 minutes. Other threshold periods are possible as well.

The system hierarchy may include system levels such as first second third and fourth system levels. The first system level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers. Each host machine has a system domain. The second system level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines. Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain. The third system level corresponds to power deliverers communication deliverers or cooling deliverers of cells having associated racks. Each power deliverer communication deliverer or cooling deliverer of the cell has a system domain. The fourth system level corresponds to a distribution center module of the cells each distribution center module has a system domain.

In some examples for each stripe the method includes determining the effective redundancy value for each system level. Determining the effective redundancy value for a replicated stripe having replica chunks includes identifying a number of unique system domains having at least one available replica chunk at a corresponding system level. Determining the effective redundancy value for an encoded stripe having data chunks and code chunks at the second system level includes identifying a number of unique system domains within the second system level capable of being inactive while maintaining data accessibility. In addition determining the effective redundancy value for an encoded stripe having data chunks and code chunks at the third or fourth system level includes identifying a system domain within the third or fourth level capable of being inactive while maintaining data accessibility and having the largest number of chunks compared to the remaining system domains. Additionally when more than one system domains has the largest number of chunks the method includes randomly selecting one of the system domains.

In some implementations determining the effective redundancy value for a nested code stripe having data chunks code check chunks and word check chunks further includes determining one of a column effective redundancy or a stripe effective redundancy. Determining a column effective redundancy includes identifying a number of unique chunks within a column capable of being reconstructed from other chunks within the column and determining a stripe effective redundancy including identifying a number of unique chunks within a stripe capable of being reconstructed from other chunks within the stripe.

Another aspect of the disclosure provides a method of prioritizing data for recovery in a distributed storage system. The method includes determining using a computing processor an effective redundancy value for each stripe of a file the file including stripes having chunks the effective redundancy value based on the chunks and any system domains associated with the corresponding stripe the distributed storage system having a system hierarchy including system domains each system domain having an active state or an inactive state. When a system domain is in the inactive state the method includes updating the effective redundancy value for each stripe of the file associated with that system domain. In addition the method includes causing reconstruction of a stripe when its effective redundancy value is less than a threshold effective redundancy value.

In some implementations the system hierarchy includes a first a second a third and a fourth level. The first system level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers. Each host machine has a system domain. The second system level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines. Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain. The third system level corresponds to power deliverers communication deliverers or cooling deliverers of cells having associated racks. Each power deliverer communication deliverer or cooling deliverer of the cell having a system domain. The fourth system level corresponds to a distribution center module of the cells each distribution center module having a system domain. In some examples the method further includes for each stripe determining the effective redundancy value for each system level.

In some examples determining the effective redundancy value for a replicated stripe having replica chunks includes identifying a number of unique system domains having at least one available replica chunk at a corresponding system level. Determining the effective redundancy value for an encoded stripe having data chunks and code chunks at the second system level includes identifying a number of unique system domains within the second system level capable of being inactive while maintaining data accessibility. In some examples determining the effective redundancy value for an encoded stripe having data chunks and code chunks at the third or fourth system level includes identifying a system domain within the third or fourth level capable of being inactive while maintaining data accessibility and having the largest number of chunks compared to the remaining system domains. When more than one system domains has the largest number of chunks the method includes randomly selecting one of the system domains.

In some implementations determining the effective redundancy value for a nested code stripe having data chunks code check chunks and word check chunks includes determining one of a column effective redundancy or a stripe effective redundancy. Determining a column effective redundancy includes identifying a number of unique chunks within a column capable of being reconstructed from other chunks within the column and determining a stripe effective redundancy includes identifying a number of unique chunks within a stripe capable of being reconstructed from other chunks within the stripe.

In some implementations the method includes determining whether the stripe includes high availability chunks or low availability chunks. When chunks of the stripe become inaccessible the method includes reconstructing substantially immediately using the data processing device inaccessible high availability chunks having an effective redundancy value less than a threshold effective redundancy value. Moreover when chunks of a stripe become inaccessible the method also includes reconstructing a threshold period of time 1 inaccessible low availability chunks and 2 inaccessible high availability chunks having an effective redundancy value greater than or equal to the threshold effective redundancy. The threshold period of time may be between about 15 minutes and about 30 minutes. Other threshold periods are possible as well.

Yet another aspect of the disclosure provides a system for prioritizing data for recovery in a distributed storage system. The system includes memory hosts each memory host having non transitory memory and a computer processor in communication with the memory hosts. The computer processor executes instructions to manage striping of files across the memory hosts each file includes stripes having chunks. For each stripe of a file having a plurality of stripes having chunks the computer processor determines using a data processing device and whether the stripe includes high availability chunks or low availability chunks. In addition the computer processor determines an effective redundancy value for each stripe of the file. The effective redundancy value is based on the chunks and any system domains associated with the corresponding stripe. The distributed storage system has a system hierarchy that includes system domains where each system domain has an active state or an inactive state. Chunks of a stripe associated with a system domain in the active state are accessible and chunks of a stripe associated with a system domain in the inactive state are inaccessible.

When chunks of a stripe become inaccessible the computer processor causes reconstruction substantially immediately of inaccessible high availability chunks having an effective redundancy value less than a threshold effective redundancy value. Moreover when chunks of a stripe become inaccessible the computer processor causes reconstruction after a threshold period of time of 1 inaccessible low availability chunks and 2 inaccessible high availability chunks having an effective redundancy value greater than or equal to the threshold effective redundancy.

The computer processor causes reconstruction of inaccessible high availability chunks that have an effective redundancy value less than a threshold effective redundancy value and causes reconstruction of the remaining inaccessible chunks using the data processing device after a threshold period of time following an inaccessibility time point of the inaccessible chunks. The remaining inaccessible chunks may be high availability and or low availability chunks.

In some implementations the computer processor updates the effective redundancy value for each stripe of the file associated with a system domain when the system domain is in the inactive state. The threshold period of time may be between about 15 minutes and about 30 minutes.

In some examples the system hierarchy includes first through fourth system levels. The first system level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers. Each host machine has a system domain. The second system level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines. Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain. The third system level corresponds to power deliverers communication deliverers or cooling deliverers of cells having associated racks. Each power deliverer communication deliverer or cooling deliverer of the cell has a system domain. The fourth system level corresponds to a distribution center module of the cells each distribution center module has a system domain. The computer processor determines for each stripe the effective redundancy value for each system level.

The computer processor may determine the effective redundancy value for a replicated stripe having replica chunks by identifying a number of unique system domains having at least one available replica chunk at a corresponding system level. In some examples the computer processor determines the effective redundancy value for an encoded stripe having data chunks and code chunks at the second system level by identifying a number of unique system domains within the second system level capable of being inactive while maintaining data accessibility. The computer processor may determine the effective redundancy value for an encoded stripe having data chunks and code chunks at the third or fourth system level by identifying a system domain within the third or fourth level capable of being inactive while maintaining data accessibility and having the largest number of chunks compared to the remaining system domains. When more than one system domains has the largest number of chunks the computer processor randomly selects one of the system domains.

In some implementations the computer processor determines the effective redundancy value for a nested code stripe having data chunks code check chunks and word check chunks by determining one of a column effective redundancy or a stripe effective redundancy. The computer processor determines a column effective redundancy by identifying a number of unique chunks within a column capable of being reconstructed from other chunks within the column and determines a stripe effective redundancy by identifying a number of unique chunks within a stripe capable of being reconstructed from other chunks within the stripe.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

Referring to in some implementations a distributed storage system includes loosely coupled resource hosts e.g. computers or servers each having a computing resource e.g. one or more processors or central processing units CPUs in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks that may be used for caching data. A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the resource hosts through a network e.g. via RPC .

The distributed storage systems may include multiple layers of redundancy where data is replicated and or encoded and stored in multiple data centers. Data centers house computer systems and their associated components such as telecommunications and storage systems . Data centers usually include backup power supplies redundant communications connections environmental controls to maintain a constant temperature and security devices. Data centers may be large industrial scale operations that use a great amount of electricity e.g. as much as a small town . Data centers may be located in different geographical locations e.g. different cities different countries and different continents . In some examples the data centers or portions thereof requires maintenance e.g. due to a power outage or disconnecting a portion of the storage system for replacing parts or a system failure or a combination thereof . The data stored in these data centers and in particular the distributed storage system may be unavailable to users clients during the maintenance period resulting in the impairment or halt of a user s operations. During maintenance or unplanned failures of the distributed storage system some data has a higher risk of becoming fully unavailable than other data . It is desirable to segment the data population into high availability data and low availability data and determine which data is more at risk of loss due to the maintenance and then recover replicate that data without recovering all or most of the remaining data which would lead to inefficiencies.

In some implementations the distributed storage system is single sided eliminating the need for any server jobs for responding to remote procedure calls RPC from clients to store or retrieve data on their corresponding resource hosts and may rely on specialized hardware to process remote requests instead. Single sided refers to the method by which most of the request processing on the resource hosts may be done in hardware rather than by software executed on CPUs of the resource hosts . Rather than having a processor of a resource host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the resource host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This single sided distributed storage architecture offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the resource hosts . This has the effect of decoupling the requirements for storage and CPU cycles that typical two sided distributed storage systems carry. The single sided distributed storage system can utilize remote storage resources regardless of whether there are spare CPU cycles on that resource host furthermore since single sided operations do not contend for server CPU resources a single sided system can serve cache requests with very predictable low latency even when resource hosts are running at high CPU utilization. Thus the single sided distributed storage system allows higher utilization of both cluster storage and CPU resources than traditional two sided systems while delivering predictable low latency.

In some implementations the distributed storage system includes a storage logic portion a data control portion and a data storage portion . The storage logic portion may include a transaction application programming interface API e.g. a single sided transactional system client library that is responsible for accessing the underlying data for example via RPC or single sided operations. The data control portion may manage allocation and access to storage resources with tasks such as allocating storage resources registering storage resources with the corresponding network interface controller setting up connections between the client s and the resource hosts handling errors in case of machine failures etc. The data storage portion may include the loosely coupled resource hosts 

The distributed storage system may store data in dynamic random access memory DRAM and serve the data from the remote hosts via remote direct memory access RDMA capable network interface controllers . A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . Both the resource hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the resource host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the resource host creates a client key for each registered memory region 

The single sided operations performed by the network interface controllers may be limited to simple reads writes and compare and swap operations none of which may be sophisticated enough to act as a drop in replacement for the software logic implemented by a traditional cache server job to carry out cache requests and manage cache policies. The transaction API translates commands such as look up or insert data commands into sequences of primitive network interface controller operations. The transaction API interfaces with the data control and data storage portions of the distributed storage system .

The distributed storage system may include a co located software process to register memory for remote access with the network interface controllers and set up connections with client processes . Once the connections are set up client processes can access the registered memory via engines in the hardware of the network interface controllers without any involvement from software on the local CPUs of the corresponding resource hosts .

Referring to in some implementations the distributed storage system includes multiple cells each cell including resource hosts and a curator in communication with the resource hosts . The curator e.g. process may execute on a computing processor e.g. server having a non transitory memory connected to the network and manage the data storage e.g. manage a file system stored on the resource hosts control data placements and or initiate data recovery. Moreover the curator may track an existence and storage location of data on the resource hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple resource hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file in a way that accesses of sequential segments are made to different physical storage devices e.g. cells and or resource hosts . Striping is useful when a processing device requests access to data more quickly than a storage device can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses.

In some implementations the transaction API interfaces between a client e.g. with the client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the transaction API may find the storage location of certain data on resource host s and obtain a key that allows access to the data . The transaction API communicates directly with the appropriate resource hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a resource host is non operational or the data was moved to a different resource host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata may include a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions. Referring to in some implementations file descriptors stored by the curator contain metadata such as the file map which maps the stripes to data chunks D and code chunks C stored on the memory hosts . To open a file a client sends a request to the curator which returns a file descriptor . The client uses the file descriptor to translate file chunk offsets to remote memory locations . The file descriptor may include a client key e.g. a 32 bit key that is unique to a chunk on a memory host and is used to RDMA read that chunk . After the client loads the file descriptor the client may access the data of a file via RDMA or another data retrieval method.

The curator may maintain status information for all memory hosts that are part of the cell . The status information may include capacity free space load on the memory host latency of the memory host from a client s point of view and a current state. The curator may obtain this information by querying the memory hosts in the cell directly and or by querying a client to gather latency statistics from a client s point of view. In some examples the curator uses the memory host status information to make rebalancing draining recovery decisions and allocation decisions.

The curator s may allocate chunks in order to handle client requests for more storage space in a file and for rebalancing and recovery. In some examples the processor replicates chunks among the storage devices differently than distributing the data chunks D and the code chunks C among the storage devices . The curator may maintain a load map of memory host load and liveliness. In some implementations the curator allocates a chunk by generating a list of candidate memory hosts and sends an allocate chunk request to each of the candidate memory hosts . If the memory host is overloaded or has no available space the memory host can deny the request . In this case the curator selects a different memory host . Each curator may continuously scan its designated portion of the file namespace examining all the metadata every minute or so. The curator may use the file scan to check the integrity of the metadata determine work that needs to be performed and or to generate statistics. The file scan may operate concurrently with other operations of the curator . The scan itself may not modify the metadata but schedules work to be done by other components of the system and computes statistics.

Referring to data may be one or more files where each file has a specified replication level and or error correcting code . The curator may divide each file into a collection of stripes with each stripe being encoded independently from the remaining stripes . For a replicated file each stripe is a single logical chunk that the curator replicates as stripe replicas and stores on multiple storage resources . In that scenario a stripe replica is also referred to as a chunk . For an erasure encoded file each stripe consists of multiple data chunks D and non data chunks C e.g. code chunks that the curator places on multiple storage resources where the collection of data chunks D and non data chunks C forms a single code word. In general the curator may place each stripe on storage resources independently of how the other stripes in the file are placed on the storage resources . The error correcting code adds redundant data or parity data to a file so that the file can later be recovered by a receiver even when a number of errors up to the capability of the code being used were introduced. The error correcting code is used to maintain data integrity in storage devices to reconstruct data for performance latency or to more quickly drain machines.

Referring to each stripe is divided into data chunks D and non data chunks C based on an encoding level e.g. Reed Solomon Codes nested codes or other erasure coding. The non data chunks C may be code chunks C e.g. for Reed Solomon codes . In other examples the non data chunks C may be code check chunks CC word check chunks WC and code check word check chunks CCWC for nested coding .

A data chunk D is a specified amount of data . In some implementations a data chunk D is a contiguous portion of data from a file . In other implementations a data chunk D is one or more non contiguous portions of data from a file . For example a data chunk D can be 256 bytes or other units of data .

A damaged chunk e.g. data chunk D or non data chunk C is a chunk containing one or more errors. Typically a damaged chunk is identified using an error detecting code . For example a damaged chunk can be completely erased e.g. if the chunk was stored in a hard drive destroyed in a hurricane or a damaged chunk can have a single bit flipped. A healthy chunk is a chunk that is not damaged. A damaged chunk can be damaged intentionally for example where a particular resource host is shut down for maintenance. A damaged chunk may be a missing or unavailable chunk. In that case damaged chunks can be identified by identifying chunks that are stored at resource hosts that are being shut down. In some implementations damaged chunks may be recovered using healthy chunks . damaged chunks e.g. data chunks D or non data chunks C may be damaged due to various reasons. Damaged chunks within a stripe may be recovered from the healthy chunks . The non data chunks C of a file include an error correcting code chunk . The error correcting code chunks include a chunk of data based on one or more data chunks D. In some implementations each code chunk C is the same specified size e.g. 256 bytes as the data chunks D. The code chunks C are generated using an error correcting code e.g. a Maximal Distance Separable MDS code. Examples of MDS codes include Reed Solomon codes. Various techniques can be used to generate the code chunks C. In general any error correcting code can be used that can reconstruct d data chunks D from any set of d unique healthy chunks either data chunks D or code chunks C .

A codeword is a set of data chunks D and code chunks C based on those data chunks D. If an MDS code is used to generate a codeword containing d data chunks D and c code chunks C then all of the chunks data or code can be reconstructed as long as any healthy chunks data or code are available from the codeword.

Referring to in nested coding techniques an encoded data block includes a data block having data chunks D and error correcting code chunks i.e. non data chunks C that is being stored is viewed as forming a two dimensional R C array. There are X code chunks C for each column C called code check chunks CC that can be used to reconstruct X or fewer damaged chunks per column. There are Y code chunks C called word check chunks WC for the entire 2 D array. When there are more than X damaged chunks in one or more columns C the word check chunks WC are used in addition to other healthy chunks to reconstruct damaged chunks . Although some examples described in this specification illustrate encoded data blocks i.e. data block and code chunks C i.e. non data chunks C as forming a two dimensional array it is possible for coding techniques to create encoded data blocks configured differently. For instance different columns can have different numbers of code check chunks CC i.e. the code check chunk CC and columns C that contain word check chunks WC can have different numbers of rows than columns that contain data chunks D and code check chunks C.

The codes C can be used to store data across resource hosts by allocating each column C of data chunks D to a data center. Each chunk within the columns C can be allocated to a resource host within a data center. Then if X or fewer chunks are lost at a data center the chunks can be reconstructed using only intra data center communication e.g. so no other data centers have to provide data in performing reconstruction . If more than X chunks are lost in one or more data centers then the Y word check chunks WC are used to attempt reconstruction. Thus inter data center communication which may be more expensive e.g. slower than intra data center communication is only needed when more than X chunks are damaged within a single data center.

The codes can also be used within a single data center. Instead of allocating different columns C to different data centers the encoding system stores all of the columns at a single data center. The data chunks D and code chunks C can be stored at distinct resource hosts within that data center. This is useful for example where reading data from resource hosts during reconstruction is expensive e.g. time consuming so that the encoding system can read fewer chunks during reconstruction than would be needed using conventional coding techniques. Small numbers of damaged chunks can be reconstructed by reading small numbers of other chunks code check chunks CC and other data chunks D in the column C and large numbers of damaged chunks can be reconstructed using the word check chunks WC when needed.

Referring to in some implementations a nested coding technique shows data chunks D and code chunks C that form a codeword. As shown the nested coding technique is a two dimensional 2D nested coding technique but a three dimensional 3D nested coding technique may also be applied. A 2D nested code is created from an arbitrary linear MDS code in systematic form. Word check chunks WC that are based on a data block are partitioned into two groups the first group including X code chunks C and the second group including N code chunks C. The block of data is viewed as forming an array of columns C and X code chunks C in the first group are used to create X column chunks per column by splitting them into separate components per column split code check chunks CC . The N code chunks C in the second group form word check chunks WC.

For example shows data block D D where D D are data chunks D and code chunks C C C that are based on the data block D D . The data chunks D D D and the code chunks C C C form a codeword. The code chunks C are partitioned into a first group that includes C C and a second group that includes C C. C C are split to form split code check chunks CC. C C are used as word check chunks WC.

Referring to a resulting encoded data block that includes the data block D D and additional code chunks C split code check chunks CC and word check chunks WC is shown. To generate a split code check chunk CC corresponding to C for column j denoted C j C is generated as though all the data chunks D not in column j have the value zero. That is C j has the value that would result from performing the operations to generate C using the full data block of data chunks D but instead using only the column j with all of the other columns zeroed out. For example if a generator matrix would be used to generate C for the full data block then the generator matrix can be modified to generate C j so that it has the value that would result from using the original generator matrix and applying that original generator matrix to the data block with data chunks D in columns other than column j zeroed out.

The split code check chunks CC for C j for each column C are generated similarly but using C instead of C. As a result C is a linear combination of C C and C is a linear Combination of C C . That is C0 C 0 j and 1 C1 C 1 j. 2 The chunks denoted as in can be generated in various ways e.g. as described further below with reference to .

In the example of the resulting encoded data block includes 42 data chunks D and code chunks C. Referring to the original code used to create the encoded block the code chunks C belong to one of two groups as described above X 2 of which are in the first group and N 6 of which are in the second group. Whenever there are two or fewer X or fewer damaged chunks within one of the first seven columns the damaged chunks can be corrected using the healthy chunks of the columns C and the split code check chunks CC for the column C. To see this let j denote the column C including the two or fewer damaged chunks and consider the codeword obtained by zeroing out all the data chunks D from columns C other than j. In that codeword C C j and C C j. As a result the two or fewer damaged chunks in other columns as containing all zero data chunks D and by viewing the word check chunks WC as being damaged.

In the example shown in the word check chunks WC fully fill an entire column C the column to the right . 2D nested codes can be created with an arbitrary number of columns C of word check chunks WC. The columns C of word check chunks WC can have the same number of rows R as the columns of data chunks D or different numbers of rows R and the columns C of word check chunks WC can have different numbers of rows R from each other. Columns C of word check chunks WC can but do not have to have code check chunks CC i.e. code check word check chunks CCWC. Increasing the number of word check chunks WC improves the reliability of the stored data but uses more storage at resource hosts . In general for nested codes columns C include either data chunks D or word check chunks WC and not both.

In general a 2D nested code with X split code check chunks CC per column C and N word check chunks WC can be used to reconstruct X damaged chunks per column C in those columns that include data chunks D while performing only intra columns communication which is typically e.g. intra data center communication . In reconstructing multiple damaged chunks within the block those damaged chunks are typically reconstructed first because intra column communication is less expensive than inter column communication but other damaged chunks may remain. If after reconstructing damaged chunks within columns N X or fewer other chunks are still damaged because they were not able to be reconstructed using intra column communication those other damaged chunks can be reconstructed using the word check chunks WC and the split code check chunks CC. The word check chunks WC in the first group C and C can be determined from the split code check chunks CC e.g. using the formula Ci C i j even though those word check chunks WC are not explicitly stored.

To see this let Z denote the number of word check chunks WC that are damaged and let Y denote the number of word check chunks WC in the first group that cannot be reconstructed from their corresponding split code check chunks CC according to the formula Ci C j to split code check chunks CC being damaged. Using that formula X Y word check chunks WC from the first group can be determined resulting in a codeword e.g. the one shown in with Y damaged word check chunks WC in the first group and Z damaged word check chunks WC in the second group. Because there are at most N X total damaged chunks there are at most N X Y Z damaged data chunks D. Thus it is possible to use the resulting codeword to reconstruct all of the damaged chunks as it includes at most N X Y Z Y Z N X damaged chunks .

Referring to in some implementations the resulting encoded block includes code check chunks CC for the word check chunks WC i.e. code check word check chunks CCWC . Compared to the encoded block of the block of includes the code check chunks C and C CC in place of the locations marked with in . This is one way to provide for reconstructing damaged word check chunks WC without relying on inter column communication. The code check chunks C and C CC can be generated in various ways. For example those code check chunks CC can be generated based on C C in the same manner that C and C are generated based on D D. The resulting encoded block of using the example nested code can be used to reconstruct up to eight damaged chunks after performing intra column reconstruction. Code check chunks C can be added for any number of columns that include word check chunks WC.

Referring to in some implementations the curator distributes data using a nested code . The system receives a data block step . The data block can include m ndata chunks C mis a number of data rows and nis a number of data columns and mand nare greater than or equal to one. The encoded block includes m n chunks that include m n where m is the total number of rows R of data chunks D and non data chunks C and n is the number of columns C of data chunks D and non data chunks C m and n are greater than or equal to one. The system generates one or more columns C of word check chunks WC using a first linear error correcting code in systematic form and the data chunks D step . The word check chunks WC and the data chunks D of the same row R form a codeword. For each of mrow of data chunks C the system generates one or more split code check chunks CC for the Column C step . The split code check chunks CC are generated so that a linear combination of n split code check chunks CC from different columns C forms a first word check chunk WC of a first codeword including the data chunks D and the m word check chunks WC. The first word check chunk WC and any other word check chunks WC resulting from a linear combination of split code check chunks CC from different columns C forms a codeword with the data chunks D and the word check chunks WC generated in step . For example the split code check chunks CC for each columns C can be generated using a splitting error correcting code and the mdata chunks D or the word check chunks WC wherein the splitting error correcting code includes a splitting generator matrix that codes the same as a generator matrix for the first linear error correcting code applied to the data chunks D with the data chunks D zeroed out for columns C other than the column C.

The system stores the column C of data chunks D and the split code check chunks CC and the word check chunks WC step . In some implementations the system stores all the chunks at a single group of resource hosts . In some other implementations the system allocates each column C to a distinct group of resource hosts . When the system identifies one or more damaged chunks the system can reconstruct the damaged chunks using the split code check chunks CC and the word check chunks WC. Typically the system attempts to reconstruct damaged chunks using the split code check chunks CC and other data chunks in the same column C. If after reconstructing damaged chunks using only the split code check chunks CC some damaged chunks remain the system uses the word check chunks WC for reconstruction including the word check chunks WC that can be determined by determining a linear combination of the split code check chunks CC. In some examples when there are multiple losses the system uses any of the chunks including data chunks D.

The storage system or portions thereof may undergo a system failure for a period of time. The data distributed on the resource hosts of the storage system may not be available for users. For example referring back to a resource host may be undergoing maintenance or has a system failure therefore data e.g. stripe replicas data chunks D and non data chunks C stored on the resource host may not be retrievable i.e. the data is inaccessible . In addition the resource host may take an extended period of time e.g. a week to be functional or for maintenance to be completed. Within the period during which the resource host is not available the storage system recovers the lost data so that the data is available if a user makes a file request .

In some implementations the system segments the data into high availability data and low availability data and determines which data is more at risk of loss due to the maintenance. The system may recover or replicate the high availability data without recovering all or most of the remaining data . Since some of the data may be available shortly after a maintenance or failure the system delays recovering that data and replaces the high availability data instead.

Referring to the curator may determine or receive a system hierarchy of the distributed storage system to identify the levels e.g. levels at which maintenance may occur without affecting a user s access to stored data . Maintenance or failures strict hierarchy non strict hierarchy may include power maintenance failures cooling system maintenance failures networking maintenance failures updating or replacing parts or other maintenance or power outage affecting the distributed storage system . Maintenance may be scheduled and in some examples an unscheduled system failure may occur.

The system hierarchy includes system levels e.g. levels with maintenance units system domains spanning one or more system levels . Each system domain has an active state or an inactive state. A distribution center module includes one or more cells and each cell includes one or more racks of resource hosts . Each cell also includes cell cooling cell power e.g. bus ducts and cell level networking e.g. network switch es . Similarly each rack includes rack cooling rack power e.g. bus ducts and rack level networking e.g. network switch es .

The system levels may include first second third and fourth system levels . The first system level corresponds to resource hosts or host machines of data processing devices non transitory memory devices or network devices e.g. NICs . Each host machine resource host has a system domain . The second system level corresponds to racks and cooling deliverers power deliverers e.g. bus ducts or communication deliverers e.g. network switches and cables of the host machines at the rack level. Each rack or rack level cooling deliverer power deliverer or communication deliverer has a system domain . The third system level corresponds to any cells of the distribution center module and the cell cooling cell power or cell level networking supplied to the associated racks . Each cell or cell cooling cell power or cell level networking has a system domain . The fourth system level corresponds to the distribution center module . Each distribution center module has a system domain .

The curator determines based on the mappings of the hierarchy components which resource hosts are inactive when a hierarchy component is undergoing maintenance. Once the curator maps the system domains to the resource hosts and therefore to their corresponding processor resources and storage resources the curator determines a highest level e.g. levels at which maintenance can be performed while maintaining processor or data availability.

A system domain includes a hierarchy component undergoing maintenance and any hierarchy components depending therefrom. Therefore when one hierarchy component undergoes maintenance that hierarchy component is inactive and any other hierarchy components in the system domain of the hierarchy component are also inactive. For example when a resource host is undergoes maintenance a level system domain which includes the storage device the data processor and the NIC is in the inactive state. When a rack undergoes maintenance a level system domain which includes the rack and any resource hosts depending from the rack is in the inactive state. When a cell for example to any one of the cell cooling component the bust duct and or the network switch of the cell undergoes maintenance a level system domain which includes the cell and any hierarchy components in levels and that depend from the cell is in the inactive state. Finally when a distribution center module undergoes maintenance a level system domain which includes the distribution center module and any hierarchy components in levels to depending from the distribution center module is in the inactive state.

In some examples as shown in a non strict hierarchy component may have dual feeds i.e. the hierarchy component depends on two or more other hierarchy components . For example a cell may have a feed from two distribution center modules and or a rack may have a dual feed from two cells . As shown a level system domain may include two racks where the second rack includes two feeds from two cells . Therefore the second rack is part of two system domains and . Therefore the lower levels of the system hierarchy are maintained without causing the loss of the higher levels of the system hierarchy . This causes a redundancy in the system which allows for data accessibility. In particular the distribution center module may be maintained without losing any of the cells depending from it. In some examples the racks include a dual powered rack that allows the maintenance of the bus duct without losing power to the dual powered racks depending from it. In some examples system domains that may be maintained without causing outages are ignored when distributing chunks to allow for maintenance however the ignored system domains may be included when distributing the chunks since an unplanned outage may still cause the loss of chunks .

In some examples a cooling device such as the cell cooling and the rack cooling are used to cool the cells and the racks respectively. The cell cooling component may cool one or multiple cells . Similarly a rack cooling component may cool one or more racks . The curator stores the association of the resource hosts with the cooling devices i.e. the cell cooling and the rack cooling . In some implementations the curator considers all possible combinations of maintenance that might occur within the storage system to determine a system hierarchy or a combination of maintenance hierarchies . For example a system hierarchy where one or more cooling devices fail or a system hierarchy where the network devices fail or a system hierarchy where the power distribution fails.

Therefore when a hierarchy component in the storage system undergoes maintenance that hierarchy component and any hierarchy components that are mapped to or depending from that hierarchy component are in an inactive state. A hierarchy component in an inactive state is inaccessible by a user while a hierarchy component in an active state is accessible by a user allowing the user to process access data stored supported maintained by that hierarchy component . As previously mentioned during the inactive state a user is incapable of accessing the resource host associated with the system domains undergoing maintenance and therefore the client is incapable of accessing the files i.e. chunks which include stripe replicas data chunks D and non data chunks C .

In some implementations the curator restricts a number of chunks distributed to storage devices of any one system domain e.g. based on the mapping of the hierarchy components . Therefore if a level system domain is inactive the curator maintains accessibility to the file or stripe although some chunks may be inaccessible. In some examples for each file or stripe the curator determines a maximum number of chunks that may be placed within any storage device within a single system domain so that if a system domain associated with the storage device storing chunks for a file is undergoing maintenance the curator may still retrieve the file . The maximum number of chunks ensures that the curator is capable of reconstructing the file although some chunks may be unavailable. In some examples the maximum number of chunks is set to a lower threshold to accommodate for any system failures while still being capable of reconstructing the file from the chunks . When the curator places chunks on the storage devices the curator ensures that within a stripe no more than the maximum number of chunks are inactive when a single system domain undergoes maintenance. Moreover the curator may also restrict the number of processing jobs on a data processor of a resource host within a system domain e.g. based on the mapping of the hierarchy components . Therefore if a level system domain is inactive the curator maintains accessibility to the jobs although some of the processors of the resource hosts are inactive.

In some implementations and as previously discussed the system may undergo maintenance or unplanned failures. Some data stored on the storage devices may have a higher risk of becoming fully unavailable when the storage device it is stored on is in an inactive state. The curator may segment the data so that data that has a greater risk of loss due to the maintenance event may be recovered or replicated without the curator having to replicate or recover the rest of the data that does not have a greater risk of loss .

In some implementations the curator identifies chunks as high availability chunks or low availability chunks . The high availability chunks have a higher priority and take precedent over the low availability chunks . In some examples the curator determines the availability of the chunks based on the data or based on the owner or user of the data .

The curator may determine an effective redundancy value for each stripe of a file to determine if the system should prioritize the stripe for recovery i.e. adding replicas or storing reconstructed chunks portions of the file to improve durability and availability of the file . The effective redundancy value is based on the chunks and any system domains that are associated with the corresponding stripe . In some implementations the effective redundancy is based on the number of system domains that can be lost so that the data in the stripe becomes unavailable or inaccessible e.g. un reconstructable . Alternatively the effective redundancy is the number of chunks in a stripe that need to be lost so that the data in the stripe becomes unavailable or inaccessible e.g. un reconstructable . This may apply when all of the chunks of a stripe are on different memory devices and the curator determines the effective redundancy value at the disk level system level . If the number of lost system domains or alternatively lost chunks is equal or greater to the effective redundancy the system would not be able to reconstruct the stripe i.e. the system is not able to read the data of the stripe . For example an effective redundancy equaling one indicates that there is at least one chunk in a stripe such that if it becomes unavailable the system would not be able to read the data of the stripe . An effective redundancy equaling two indicates that a minimum number of two chunks are needed to become unavailable for the system not to be able to read the data of the stripe .

In some implementations the curator determines the effective redundancy value at various system hierarchy levels e.g. levels . The curator considers a specific stripe and the level at which it may determine the effective redundancy value. The curator simulates removing chunks from the stripe to determine the effective redundancy value. The curator hypothetically removes chunks from the stripe by removing those chunks for which the largest number of chunks depend on a single node in the selected level of the system hierarchy . The curator keeps hypothetically removing chunks until a number of remaining chunks is incapable of reconstructing the data within the specific stripe . The number of rounds of removal is the effective redundancy.

For replicated encodings discussed in the effective redundancy value is the number of unique system domains at the selected level of the system hierarchy that contain at least one healthy e.g. accessible chunk because as long as the system retains one chunk e.g. replicated chunks in a system domain then the data is redundant. For example if the stripe has one or more replica chunks in a single system domain then the effective redundancy equals 1. If the stripe has at least one available accessible chunk in one system domain and another available accessible chunk in another system domain then the effective redundancy equals 2. Referring to the example shown in consider a stripe having three replica chunks stored on a first storage device a second storage device and a third storage device . Each storage device is located on a different rack . The first storage device is located on a first rack the second storage device is located on a second rack and the third storage device is located on a third rack . As shown the first and second racks share the same first cell e.g. bus duct and the third rack depends on a second cell . Thus the replica chunks are located in two level system domains . The replica chunks are also located in three level system domains and three level system domains . In addition the first cell and the second cell both depend on one distribution center module . The effective redundancy of the stripe at the level of the storage devices at level equals 3 since replica chunks are placed on three different storage devices . The effective redundancy of the stripe at the level of the racks level equals 3 as well since the replica chunks are placed on storage devices depending from three different racks . The effective redundancy of the stripe at the level of the cells level is 2 since the data is placed on storage devices depending from two bus ducts feeding the first and second cells . The effective redundancy of the stripe at the level of the distribution center module level is 1. Therefore the effective redundancy value may be considered as the number of unique system domains at the selected level of the system hierarchy that contain at least one healthy e.g. accessible chunk because as long as the system retains one chunk the data is redundant for the replication .

Referring to the effective redundancy for erasure encoded files is more complicated since the system may not simply reduce the chunks and count the number of unique system domains in the stripe .

The curator may determine the effective redundancy at the cell level i.e. bus duct level or level by hypothetically removing the cell that contain the largest number of chunks of a stripe to assume the worst case compared to the other cell s having the remaining chunks of the stripe . Referring to the example shown in the curator hypothetically removes the cell containing the largest number of chunks compared to the other cells . In this case the first cell system domain and the third cell system domain have the largest number of chunks so the curator selects e.g. randomly or deterministically which cell to remove for determining the effective redundancy at that level. Once the curator removes one of the cells or level system domains the system is left with either the first cell or the third cell and is still capable of reconstructing the stripe . The system then considers removing the other one of the first cell or the third cell which results in four unavailable chunks i.e. 4 3 3 being the number of unavailable chunks that render the system incapable of reading the stripe which means the stripe is unreadable after losing two cells or two system domains . Therefore the effective redundancy at the cell level level equals 2 because the system is incapable of reconstructing the stripe when two cells or level system domains are in an inactive state.

Next the curator considers the effective redundancy at the distribution center module level . The same considerations applied to level are also applied to level . Therefore the system removes the distribution center modules that include the largest number of chunks compared to the other distribution center modules that have the remaining chunks of the stripe . Referring back to the example of if the system removes the first distribution center module then three chunks of the stripe are no longer available for reconstructing the stripe causing the system to be incapable of reading the stripe . Therefore the effective redundancy at the distribution center module level equals one since after losing one distribution center module the stripe is unreadable.

At each system level for the nested encoded file the curator may determine the effective redundancy for column based reconstruction or for the stripe based reconstruction or both. For column based reconstruction the curator only considers data chunks D and code check chunks CC. In some examples the curator may ignore the code check chunks CC when determining the effective redundancy of the entire stripe . In some implementations the curator considers which code check chunks CC goes with which columns C when determining the effective redundancy of the entire stripe . For example if the system loses a data chunk D in a first column C and a code check chunk CC in a second column C the stripe is fine and reconstructable. Accordingly the curator may consider the geometry of the chunks in the nested stripe when determining the effective redundancy value ER.

At level i.e. the rack level the curator determines the effective redundancy of a column C by considering data chunks D stored on storage devices associated with the first through fourth racks and code check chunks CC stored on storage devices associated with the fifth and sixth racks . As stated in the example each column C includes two data chunks D and one code check chunk CC totaling three chunks . Thus to reconstruct the column C the system needs two available chunks i.e. chunks stored on devices in the active state . The system is incapable of reconstructing the column C if two or more chunks are not available. Therefore the curator determines that the effective redundancy for column based reconstruction at level system domains equals 2 since the system is not capable of reconstructing the column if 2 racks are inactive. Similarly the curator makes the same considerations for determining the column based reconstruction at level and level system domains . An effective redundancy for column based reconstruction at level system domains equals 2 assuming the data chunks from each column C are stored in different cells and an effective redundancy for column based reconstruction at level system domains equals 1.

Moreover the curator determines the effective redundancy of the stripe by considering data chunks D stored on storage devices associated with the first through fourth racks and word check chunks WCC stored on storage devices associated with the seventh through ninth racks . In some examples the curator ignores code check chunks C when determining the effective redundancy value ER in general while in other examples the curator considers code check chunks C when determining a column based effective redundancy value ER. Thus to reconstruct the stripe the system needs at least four available chunks i.e. chunks stored on devices in the active state . The system is incapable of reconstructing the stripe if four or more chunks are not available. Therefore the system determines that the effective redundancy for stripe based reconstruction at level system domains equals 4 since the system is not capable of reconstructing the stripe if four racks are inactive. The curator may consider code chunks C as well when determining the effective redundancy of the stripe . Moreover the curator may consider the geometry of the chunks e.g. which column C the chunks reside in when determining the effective redundancy value ER because a loss of a data chunk D in the first column C and a code check chunk CC in the second column C assuming no other losses still allows reconstruction of the stripe based on a column C alone whereas losing a data chunk D and a code check chunk CC in the same column C may prevent reconstruction of the stripe . Some geometries of losses i.e. losses of chunks in various columns C may disallow column based reconstructions which is faster than other types of reconstructions.

Similarly the curator makes the same considerations for determining the stripe based reconstruction at level and level system domains . In the example shown an effective redundancy for stripe based reconstruction at level system domains equals 2 and an effective redundancy for stripe based reconstruction at level system domains equals 1.

Referring to in some implementations the system e.g. the curator prioritizes data for recovery. The data stored on the storage devices may have one of three states an available state an unavailable state and a missing state. When the data is initially stored on the storage devices it is in the available state as shown in block . When data is in the available state a user client may access the data . However when the data is in the unavailable state or the missing state the user client may not access the data and the system may reconstruct the data to make it available to the user client . The unavailable and missing states of the data may be due to the inactive state of the system domain . Therefore if data is associated with a system domain that is in the inactive state the data may be in the unavailable state or the missing state. Data may become unavailable or missing for other reasons as well. Referring to decision block the curator determines if the data becomes unavailable. Unavailable data is any data that is not available for a time period greater than zero. The unavailable state at block is a transition state between the available state block and the missing state block . Data that is unavailable for a threshold period of time Tis unavailable data e.g. 0

In some implementations the curator identifies data as either high availability data or low availability data and reconstructs high availability data that has a current effective redundancy value ERless than a threshold effective redundancy value ER i.e. effective redundancy value without considering the threshold time Tbefore the data is considered missing. In effect this speeds up reconstructing certain high availability data that may be at risk of becoming lost. Therefore considering block when t Tis false i.e. the data is still in the unavailable state the curator determines if the data is high availability data and if the effective redundancy ER of the data i.e. of the stripe is less than the threshold effective redundancy value ER ER ER at block . If the data is high availability data and ER ER then the curator initiates reconstruction of the data bypassing the condition at block t T . In some examples the curator does not update the state of the data before reconstructing the data while in other examples the curator may transition the data from the unavailable state to the missing state at block which may flag the data for reconstruction. However if the data is not high availability data and the current effective redundancy value ERof the data is not less than or equal to the threshold effective redundancy value ER then the data remains in the unavailable state at block . Bypassing the t Tcondition of block allows the curator to prioritize the data that is high availability by allowing the system to reconstruct high availability data that has reached the threshold effective redundancy value ERbefore low availability data and before the high availability data is lost without being reconstructed. The threshold effective redundancy value ERmay be a configuration option of the system . In general the effective redundancy value ER may be based on a system level its state the encoding type of the stripe and the state of the chunks in the stripe e.g. available unavailable or missing . The effective redundancy value ER may not indicate how many chunks of a stripe may be lost but rather how many system domains can be lost while still maintaining redundancy allowing reconstruction of the stripe . Losing a single additional chunk may not impact the current effective redundancy value ER.

In some implementations the effective redundancy value ER for a stripe when classified as high availability is always less than or equal to the effective redundancy value ER for that stripe when classified as low availability. As an example consider three replicated chunks of a stripe in different system domains . A first chunk is unavailable for t T. A second chunk is unavailable for t

Referring to in some implementations a method of prioritizing data for recovery in a distributed storage system includes for each stripe of a file that includes a plurality of stripes having chunks determining using a data processing device whether the stripe includes high availability chunks or low availability chunks . Files may be classified as high availability or low availability. The stripes of a file and corresponding chunks of the stripes may be classified according as high availability or low availability. The method also includes determining using the data processing device an effective redundancy value ER for each stripe of the file . The effective redundancy value ER is based on the chunks and any system domains associated with the corresponding stripe . The distributed storage system has a system hierarchy including system domains . Moreover each system domain has an active state or an inactive state. Chunks of a stripe associated with a system domain in the active state are accessible whereas chunks of a stripe associated with a system domain in the inactive state are inaccessible. Chunks may become inaccessible for other reasons as well and inaccessibility may not be directly correlated to the active inactive state of a system domain .

When chunks of a stripe become inaccessible the method includes reconstructing substantially immediately using the data processing device inaccessible high availability chunks having an effective redundancy value ER less than a threshold effective redundancy value ER. Moreover when chunks of a stripe become inaccessible the method also includes reconstructing after a threshold period of time 1 inaccessible low availability chunks and 2 inaccessible high availability chunks having an effective redundancy value ER greater than or equal to the threshold effective redundancy ER. Therefore certain high availability chunks at high risk of becoming lost based on the effective redundancy value ER receive relatively quicker reconstruction than low availability chunks or other high availability chunks that are not at high risk of becoming lost. Other parameters may be used in addition to or as an alternative to the threshold period of time to trigger reconstruction of an inaccessible chunk . For example a client may request data e.g. chunks from a memory device of a host machine and not find the data . In response to not finding the data the client may notify the curator of the inaccessible data . If the curator confirms that the data is inaccessible e.g. unavailable for t

In some implementations the method further includes updating the effective redundancy value ER for each stripe of the file associated with a system domain when the system domain is in the inactive state. The threshold period of time may be between about 15 minutes and about 30 minutes.

The system hierarchy may include system levels. The system levels include first second third and fourth system levels. The first system level level corresponds to host machines of data processing devices non transitory memory devices or network interface controllers . Each host machine has a system domain . The second system level level corresponds to power deliverers communication deliverers or cooling deliverers of racks housing the host machines . Each power deliverer communication deliverer or cooling deliverer of the rack has a system domain . The third system level level corresponds to power deliverers communication deliverers or cooling deliverers of cells having associated racks . Each power deliverer communication deliverer or cooling deliverer of the cell has a system domain . The fourth system level level corresponds to a distribution center module of the cells each distribution center module has a system domain .

In some examples for each stripe the method includes determining the effective redundancy value ER for each system level. Determining the effective redundancy value for a replicated stripe having replica chunks includes identifying a number of unique system domains having at least one available replica chunk at a corresponding system level. Determining the effective redundancy value for an encoded stripe having data chunks D and code chunks C at the second system level includes identifying a number of unique system domains within the second system level capable of being inactive while maintaining data accessibility. In addition determining the effective redundancy value for an encoded stripe having data chunks C and code chunks C at the third or fourth system level includes identifying a system domain within the third or fourth level capable of being inactive while maintaining data accessibility and having the largest number of chunks compared to the remaining system domains . Additionally when more than one system domains has the largest number of chunks the method includes randomly selecting one of the system domains .

In some implementations determining the effective redundancy value for a nested code stripe having data chunks D code check chunks CC and word check chunks WCC further includes determining one of a column effective redundancy or a stripe effective redundancy. Determining a column effective redundancy includes identifying a number of unique chunks within a column C capable of being reconstructed from other chunks within the column C and determining a stripe effective redundancy including identifying a number of unique chunks within a stripe capable of being reconstructed from other chunks within the stripe .

Referring to in some implementations a method of prioritizing data for recovery in a distributed storage system includes determining using a computing processor an effective redundancy value for each stripe of a file the file divided into stripes having chunks . The effective redundancy value is based on the chunks and any system domains associated with the corresponding stripe . When a system domain is in the inactive state the method includes updating the effective redundancy value for each stripe of the file associated with that system domain . In addition the method includes causing reconstruction of a stripe when its effective redundancy value is less than a threshold effective redundancy value. In some examples the method further includes for each stripe determining the effective redundancy value for each system level.

In some implementations the method further includes determining whether the stripe includes high availability chunks or low availability chunks and delaying reconstruction of low availability chunks e.g. delaying reconstruction of low availability chunks for a period of between about 15 minutes and about 30 minutes .

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter affecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

