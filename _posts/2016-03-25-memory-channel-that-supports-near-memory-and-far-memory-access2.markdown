---

title: Memory channel that supports near memory and far memory access
abstract: A semiconductor chip comprising memory controller circuitry having interface circuitry to couple to a memory channel. The memory controller includes first logic circuitry to implement a first memory channel protocol on the memory channel. The first memory channel protocol is specific to a first volatile system memory technology. The interface also includes second logic circuitry to implement a second memory channel protocol on the memory channel. The second memory channel protocol is specific to a second non volatile system memory technology. The second memory channel protocol is a transactional protocol.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09619408&OS=09619408&RS=09619408
owner: Intel Corporation
number: 09619408
owner_city: Santa Clara
owner_country: US
publication_date: 20160325
---
This application is a divisional of and claims the benefit of U.S. patent application Ser. No. 13 977 603 titled MEMORY CHANNEL THAT SUPPORTS NEAR MEMORY AND FAR MEMORY ACCESS filed Sep. 16 2013 which further claims the benefit of a 371 International Application No. PCT US2011 54421 entitled MEMORY CHANNEL THAT SUPPORTS NEAR MEMORY AND FAR MEMORY ACCESS filed on Sep. 30 2011 and which are incorporated by reference in its entirety.

This invention relates generally to the field of computer systems. More particularly the invention relates to an apparatus and method for implementing a multi level memory hierarchy including a non volatile memory tier.

One of the limiting factors for computer innovation today is memory and storage technology. In conventional computer systems system memory also known as main memory primary memory executable memory is typically implemented by dynamic random access memory DRAM . DRAM based memory consumes power even when no memory reads or writes occur because it must constantly recharge internal capacitors. DRAM based memory is volatile which means data stored in DRAM memory is lost once the power is removed. Conventional computer systems also rely on multiple levels of caching to improve performance. A cache is a high speed memory positioned between the processor and system memory to service memory access requests faster than they could be serviced from system memory. Such caches are typically implemented with static random access memory SRAM . Cache management protocols may be used to ensure that the most frequently accessed data and instructions are stored within one of the levels of cache thereby reducing the number of memory access transactions and improving performance.

With respect to mass storage also known as secondary storage or disk storage conventional mass storage devices typically include magnetic media e.g. hard disk drives optical media e.g. compact disc CD drive digital versatile disc DVD etc. holographic media and or mass storage flash memory e.g. solid state drives SSDs removable flash drives etc. . Generally these storage devices are considered Input Output I O devices because they are accessed by the processor through various I O adapters that implement various I O protocols. These I O adapters and I O protocols consume a significant amount of power and can have a significant impact on the die area and the form factor of the platform. Portable or mobile devices e.g. laptops netbooks tablet computers personal digital assistant PDAs portable media players portable gaming devices digital cameras mobile phones smartphones feature phones etc. that have limited battery life when not connected to a permanent power supply may include removable mass storage devices e.g. Embedded Multimedia Card eMMC Secure Digital SD card that are typically coupled to the processor via low power interconnects and I O controllers in order to meet active and idle power budgets.

With respect to firmware memory such as boot memory also known as BIOS flash a conventional computer system typically uses flash memory devices to store persistent system information that is read often but seldom or never written to. For example the initial instructions executed by a processor to initialize key system components during a boot process Basic Input and Output System BIOS images are typically stored in a flash memory device. Flash memory devices that are currently available in the market generally have limited speed e.g. 50 MHz . This speed is further reduced by the overhead for read protocols e.g. 2.5 MHz . In order to speed up the BIOS execution speed conventional processors generally cache a portion of BIOS code during the Pre Extensible Firmware Interface PEI phase of the boot process. The size of the processor cache places a restriction on the size of the BIOS code used in the PEI phase also known as the PEI BIOS code .

Phase change memory PCM also sometimes referred to as phase change random access memory PRAM or PCRAM PCME Ovonic Unified Memory or Chalcogenide RAM C RAM is a type of non volatile computer memory which exploits the unique behavior of chalcogenide glass. As a result of heat produced by the passage of an electric current chalcogenide glass can be switched between two states crystalline and amorphous. Recent versions of PCM can achieve two additional distinct states.

PCM provides higher performance than flash because the memory element of PCM can be switched more quickly writing changing individual bits to either 1 or 0 can be done without the need to first erase an entire block of cells and degradation from writes is slower a PCM device may survive approximately 100 million write cycles PCM degradation is due to thermal expansion during programming metal and other material migration and other mechanisms .

In the following description numerous specific details such as logic implementations opcodes means to specify operands resource partitioning sharing duplication implementations types and interrelationships of system components and logic partitioning integration choices are set forth in order to provide a more thorough understanding of the present invention. It will be appreciated however by one skilled in the art that the invention may be practiced without such specific details. In other instances control structures gate level circuits and full software instruction sequences have not been shown in detail in order not to obscure the invention. Those of ordinary skill in the art with the included descriptions will be able to implement appropriate functionality without undue experimentation.

References in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other.

Bracketed text and blocks with dashed borders e.g. large dashes small dashes dot dash dots are sometimes used herein to illustrate optional operations components that add additional features to embodiments of the invention. However such notation should not be taken to mean that these are the only options or optional operations components and or that blocks with solid borders are not optional in certain embodiments of the invention.

Memory capacity and performance requirements continue to increase with an increasing number of processor cores and new usage models such as virtualization. In addition memory power and cost have become a significant component of the overall power and cost respectively of electronic systems.

Some embodiments of the invention solve the above challenges by intelligently subdividing the performance requirement and the capacity requirement between memory technologies. The focus of this approach is on providing performance with a relatively small amount of a relatively higher speed memory such as DRAM while implementing the bulk of the system memory using significantly cheaper and denser non volatile random access memory NVRAM . Embodiments of the invention described below define platform configurations that enable hierarchical memory subsystem organizations for the use of NVRAM. The use of NVRAM in the memory hierarchy also enables new usages such as expanded boot space and mass storage implementations as described in detail below.

There are many possible technology choices for NVRAM including PCM Phase Change Memory and Switch PCMS the latter being a more specific implementation of the former byte addressable persistent memory BPRAM storage class memory SCM universal memory Ge2Sb2Te5 programmable metallization cell PMC resistive memory RRAM RESET amorphous cell SET crystalline cell PCME Ovshinsky memory ferroelectric memory also known as polymer memory and poly N vinylcarbazole ferromagnetic memory also known as Spintronics SPRAM spin transfer torque RAM STRAM spin tunneling RAM magnetoresistive memory magnetic memory magnetic random access memory MRAM and Semiconductor oxide nitride oxide semiconductor SONOS also known as dielectric memory .

 1 It maintains its content even if power is removed similar to FLASH memory used in solid state disks SSD and different from SRAM and DRAM which are volatile 

 4 rewritable and erasable at a lower level of granularity e.g. byte level than FLASH found in SSDs which can only be rewritten and erased a block at a time minimally 64 Kbyte in size for NOR FLASH and 16 Kbyte for NAND FLASH 

 6 capable of being coupled to the processor over a bus using a transactional protocol a protocol that supports transaction identifiers IDs to distinguish different transactions so that those transactions can complete out of order and allowing access at a level of granularity small enough to support operation of the NVRAM as system memory e.g. cache line size such as 64 or 128 byte . For example the bus may be a memory bus e.g. a DDR bus such as DDR3 DDR4 etc. over which is run a transactional protocol as opposed to the non transactional protocol that is normally used. As another example the bus may one over which is normally run a transactional protocol a native transactional protocol such as a PCI express PCIE bus desktop management interface DMI bus or any other type of bus utilizing a transactional protocol and a small enough transaction payload size e.g. cache line size such as 64 or 128 byte and

As mentioned above in contrast to FLASH memory which must be rewritten and erased a complete block at a time the level of granularity at which NVRAM is accessed in any given implementation may depend on the particular memory controller and the particular memory bus or other type of bus to which the NVRAM is coupled. For example in some implementations where NVRAM is used as system memory the NVRAM may be accessed at the granularity of a cache line e.g. a 64 byte or 128 Byte cache line notwithstanding an inherent ability to be accessed at the granularity of a byte because cache line is the level at which the memory subsystem accesses memory. Thus when NVRAM is deployed within a memory subsystem it may be accessed at the same level of granularity as the DRAM e.g. the near memory used in the same memory subsystem. Even so the level of granularity of access to the NVRAM by the memory controller and memory bus or other type of bus is smaller than that of the block size used by Flash and the access size of the I O subsystem s controller and bus.

NVRAM may also incorporate wear leveling algorithms to account for the fact that the storage cells at the far memory level begin to wear out after a number of write accesses especially where a significant number of writes may occur such as in a system memory implementation. Since high cycle count blocks are most likely to wear out in this manner wear leveling spreads writes across the far memory cells by swapping addresses of high cycle count blocks with low cycle count blocks. Note that most address swapping is typically transparent to application programs because it is handled by hardware lower level software e.g. a low level driver or operating system or a combination of the two.

The far memory of some embodiments of the invention is implemented with NVRAM but is not necessarily limited to any particular memory technology. Far memory is distinguishable from other instruction and data memory storage technologies in terms of its characteristics and or its application in the memory storage hierarchy. For example far memory is different from 

Far memory may be used as instruction and data storage that is directly addressable by a processor and is able to sufficiently keep pace with the processor in contrast to FLASH magnetic disk optical disc applied as mass storage. Moreover as discussed above and described in detail below far memory may be placed on a memory bus and may communicate directly with a memory controller that in turn communicates directly with the processor .

Far memory may be combined with other instruction and data storage technologies e.g. DRAM to form hybrid memories also known as Co locating PCM and DRAM first level memory and second level memory FLAM FLASH and DRAM . Note that at least some of the above technologies including PCM PCMS may be used for mass storage instead of or in addition to system memory and need not be random accessible byte addressable or directly addressable by the processor when applied in this manner.

For convenience of explanation most of the remainder of the application will refer to NVRAM or more specifically PCM or PCMS as the technology selection for the far memory . As such the terms NVRAM PCM PCMS and far memory may be used interchangeably in the following discussion. However it should be realized as discussed above that different technologies may also be utilized for far memory. Also that NVRAM is not limited for use as far memory.

 Near memory is an intermediate level of memory configured in front of a far memory that has lower read write access latency relative to far memory and or more symmetric read write access latency i.e. having read times which are roughly equivalent to write times . In some embodiments the near memory has significantly lower write latency than the far memory but similar e.g. slightly lower or equal read latency for instance the near memory may be a volatile memory such as volatile random access memory VRAM and may comprise a DRAM or other high speed capacitor based memory. Note however that the underlying principles of the invention are not limited to these specific memory types. Additionally the near memory may have a relatively lower density and or may be more expensive to manufacture than the far memory .

In one embodiment near memory is configured between the far memory and the internal processor caches . In some of the embodiments described below near memory is configured as one or more memory side caches MSCs to mask the performance and or usage limitations of the far memory including for example read write latency limitations and memory degradation limitations. In these implementations the combination of the MSC and far memory operates at a performance level which approximates is equivalent or exceeds a system which uses only DRAM as system memory. As discussed in detail below although shown as a cache in the near memory may include modes in which it performs other roles either in addition to or in lieu of performing the role of a cache.

Near memory can be located on the processor die as cache s and or located external to the processor die as caches e.g. on a separate die located on the CPU package located outside the CPU package with a high bandwidth link to the CPU package for example on a memory dual in line memory module DIMM a riser mezzanine or a computer motherboard . The near memory may be coupled in communicate with the processor using a single or multiple high bandwidth links such as DDR or other transactional high bandwidth links as described in detail below .

The caches illustrated in may be dedicated to a particular system memory address range or a set of non contiguous address ranges. For example cache is dedicated to acting as an MSC for system memory address range 1 and caches and are dedicated to acting as MSCs for non overlapping portions of system memory address ranges 2 and 3 . The latter implementation may be used for systems in which the SPA space used by the processor is interleaved into an address space used by the caches e.g. when configured as MSCs . In some embodiments this latter address space is referred to as a memory channel address MCA space. In one embodiment the internal caches perform caching operations for the entire SPA space.

System memory as used herein is memory which is visible to and or directly addressable by software executed on the processor while the cache memories may operate transparently to the software in the sense that they do not form a directly addressable portion of the system address space but the cores may also support execution of instructions to allow software to provide some control configuration policies hints etc. to some or all of the cache s . The subdivision of system memory into regions may be performed manually as part of a system configuration process e.g. by a system designer and or may be performed automatically by software.

In one embodiment the system memory regions are implemented using far memory e.g. PCM and in some embodiments near memory configured as system memory. System memory address range 4 represents an address range which is implemented using a higher speed memory such as DRAM which may be a near memory configured in a system memory mode as opposed to a caching mode .

As indicated near memory may be implemented to operate in a variety of different modes including a first mode in which it operates as a cache for far memory near memory as cache for FM B a second mode in which it operates as system memory A and occupies a portion of the SPA space sometimes referred to as near memory direct access mode and one or more additional modes of operation such as a scratchpad memory or as a write buffer . In some embodiments of the invention the near memory is partitionable where each partition may concurrently operate in a different one of the supported modes and different embodiments may support configuration of the partitions e.g. sizes modes by hardware e.g. fuses pins firmware and or software e.g. through a set of programmable range registers within the MSC controller within which for example may be stored different binary codes to identify each mode and partition .

System address space A in is used to illustrate operation when near memory is configured as a MSC for far memory B. In this configuration system address space A represents the entire system address space and system address space B does not exist . Alternatively system address space B is used to show an implementation when all or a portion of near memory is assigned a portion of the system address space. In this embodiment system address space B represents the range of the system address space assigned to the near memory A and system address space A represents the range of the system address space assigned to NVRAM .

In addition when acting as a cache for far memory B the near memory may operate in various sub modes under the control of the MSC controller . In each of these modes the near memory address space NMA is transparent to software in the sense that the near memory does not form a directly addressable portion of the system address space. These modes include but are not limited to the following 

In this mode all or portions of the near memory acting as a FM cache B is used as a cache for the NVRAM far memory FM B. While in write back mode every write operation is directed initially to the near memory as cache for FM B assuming that the cache line to which the write is directed is present in the cache . A corresponding write operation is performed to update the NVRAM FM B only when the cache line within the near memory as cache for FM B is to be replaced by another cache line in contrast to write through mode described below in which each write operation is immediately propagated to the NVRAM FM B .

In this mode all reads and writes bypass the NM acting as a FM cache B and go directly to the NVRAM FM B. Such a mode may be used for example when an application is not cache friendly or requires data to be committed to persistence at the granularity of a cache line. In one embodiment the caching performed by the processor caches A and the NM acting as a FM cache B operate independently of one another. Consequently data may be cached in the NM acting as a FM cache B which is not cached in the processor caches A and which in some cases may not be permitted to be cached in the processor caches A and vice versa. Thus certain data which may be designated as uncacheable in the processor caches may be cached within the NM acting as a FM cache B.

This is a variation of the above mode where read caching of the persistent data from NVRAM FM B is allowed i.e. the persistent data is cached in the near memory as cache for far memory B for read only operations . This is useful when most of the persistent data is Read Only and the application usage is cache friendly.

This is a variation of the near memory read cache write bypass mode where in addition to read caching write hits are also cached. Every write to the near memory as cache for FM B causes a write to the FM B. Thus due to the write through nature of the cache cache line persistence is still guaranteed.

When acting in near memory direct access mode all or portions of the near memory as system memory A are directly visible to software and form part of the SPA space. Such memory may be completely under software control. Such a scheme may create a non uniform memory address NUMA memory domain for software where it gets higher performance from near memory relative to NVRAM system memory . By way of example and not limitation such a usage may be employed for certain high performance computing HPC and graphics applications which require very fast access to certain data structures.

In an alternate embodiment the near memory direct access mode is implemented by pinning certain cache lines in near memory i.e. cache lines which have data that is also concurrently stored in NVRAM . Such pinning may be done effectively in larger multi way set associative caches.

Thus as indicated the NVRAM may be implemented to operate in a variety of different modes including as far memory B e.g. when near memory is present operating whether the near memory is acting as a cache for the FM via a MSC control or not accessed directly after cache s A and without MSC control just NVRAM system memory not as far memory because there is no near memory present operating and accessed without MSC control NVRAM mass storage A BIOS NVRAM and TPM NVRAM . While different embodiments may specify the NVRAM modes in different ways describes the use of a decode table .

By way of example operation while the near memory as cache for FM B is in the write back caching is described. In one embodiment while the near memory as cache for FM B is in the write back caching mode mentioned above a read operation will first arrive at the MSC controller which will perform a look up to determine if the requested data is present in the near memory acting as a cache for FM B e.g. utilizing a tag cache . If present it will return the data to the requesting CPU core or I O device through I O subsystem . If the data is not present the MSC controller will send the request along with the system memory address to an NVRAM controller . The NVRAM controller will use the decode table to translate the system memory address to an NVRAM physical device address PDA and direct the read operation to this region of the far memory B. In one embodiment the decode table includes an address indirection table AIT component which the NVRAM controller uses to translate between system memory addresses and NVRAM PDAs. In one embodiment the AIT is updated as part of the wear leveling algorithm implemented to distribute memory access operations and thereby reduce wear on the NVRAM FM B. Alternatively the AIT may be a separate table stored within the NVRAM controller .

Upon receiving the requested data from the NVRAM FM B the NVRAM controller will return the requested data to the MSC controller which will store the data in the MSC near memory acting as an FM cache B and also send the data to the requesting processor core or I O Device through I O subsystem . Subsequent requests for this data may be serviced directly from the near memory acting as a FM cache B until it is replaced by some other NVRAM FM data.

As mentioned in one embodiment a memory write operation also first goes to the MSC controller which writes it into the MSC near memory acting as a FM cache B. In write back caching mode the data may not be sent directly to the NVRAM FM B when a write operation is received. For example the data may be sent to the NVRAM FM B only when the location in the MSC near memory acting as a FM cache B in which the data is stored must be re used for storing data for a different system memory address. When this happens the MSC controller notices that the data is not current in NVRAM FM B and will thus retrieve it from near memory acting as a FM cache B and send it to the NVRAM controller . The NVRAM controller looks up the PDA for the system memory address and then writes the data to the NVRAM FM B.

In the NVRAM controller is shown connected to the FM B NVRAM mass storage A and BIOS NVRAM using three separate lines. This does not necessarily mean however that there are three separate physical buses or communication channels connecting the NVRAM controller to these portions of the NVRAM . Rather in some embodiments a common memory bus or other type of bus such as those described below with respect to is used to communicatively couple the NVRAM controller to the FM B NVRAM mass storage A and BIOS NVRAM . For example in one embodiment the three lines in represent a bus such as a memory bus e.g. a DDR3 DDR4 etc bus over which the NVRAM controller implements a transactional protocol to communicate with the NVRAM . The NVRAM controller may also communicate with the NVRAM over a bus supporting a native transactional protocol such as a PCI express bus desktop management interface DMI bus or any other type of bus utilizing a transactional protocol and a small enough transaction payload size e.g. cache line size such as 64 or 128 byte .

In one embodiment computer system includes integrated memory controller IMC which performs the central memory access control for processor which is coupled to 1 a memory side cache MSC controller to control access to near memory NM acting as a far memory cache B and 2 a NVRAM controller to control access to NVRAM . Although illustrated as separate units in the MSC controller and NVRAM controller may logically form part of the IMC .

In the illustrated embodiment the MSC controller includes a set of range registers which specify the mode of operation in use for the NM acting as a far memory cache B e.g. write back caching mode near memory bypass mode etc described above . In the illustrated embodiment DRAM is used as the memory technology for the NM acting as cache for far memory B. In response to a memory access request the MSC controller may determine depending on the mode of operation specified in the range registers whether the request can be serviced from the NM acting as cache for FM B or whether the request must be sent to the NVRAM controller which may then service the request from the far memory FM portion B of the NVRAM .

In an embodiment where NVRAM is implemented with PCMS NVRAM controller is a PCMS controller that performs access with protocols consistent with the PCMS technology. As previously discussed the PCMS memory is inherently capable of being accessed at the granularity of a byte. Nonetheless the NVRAM controller may access a PCMS based far memory B at a lower level of granularity such as a cache line e.g. a 64 bit or 128 bit cache line or any other level of granularity consistent with the memory subsystem. The underlying principles of the invention are not limited to any particular level of granularity for accessing a PCMS based far memory B. In general however when PCMS based far memory B is used to form part of the system address space the level of granularity will be higher than that traditionally used for other non volatile storage technologies such as FLASH which can only perform rewrite and erase operations at the level of a block minimally 64 Kbyte in size for NOR FLASH and 16 Kbyte for NAND FLASH .

In the illustrated embodiment NVRAM controller can read configuration data to establish the previously described modes sizes etc. for the NVRAM from decode table or alternatively can rely on the decoding results passed from IMC and I O subsystem . For example at either manufacturing time or in the field computer system can program decode table to mark different regions of NVRAM as system memory mass storage exposed via SATA interfaces mass storage exposed via USB Bulk Only Transport BOT interfaces encrypted storage that supports TPM storage among others. The means by which access is steered to different partitions of NVRAM device is via a decode logic. For example in one embodiment the address range of each partition is defined in the decode table . In one embodiment when IMC receives an access request the target address of the request is decoded to reveal whether the request is directed toward memory NVRAM mass storage or I O. If it is a memory request IMC and or the MSC controller further determines from the target address whether the request is directed to NM as cache for FM B or to FM B. For FM B access the request is forwarded to NVRAM controller . IMC passes the request to the I O subsystem if this request is directed to I O e.g. non storage and storage I O devices . I O subsystem further decodes the address to determine whether the address points to NVRAM mass storage A BIOS NVRAM or other non storage or storage I O devices. If this address points to NVRAM mass storage A or BIOS NVRAM I O subsystem forwards the request to NVRAM controller . If this address points to TMP NVRAM I O subsystem passes the request to TPM to perform secured access.

In one embodiment each request forwarded to NVRAM controller is accompanied with an attribute also known as a transaction type to indicate the type of access. In one embodiment NVRAM controller may emulate the access protocol for the requested access type such that the rest of the platform remains unaware of the multiple roles performed by NVRAM in the memory and storage hierarchy. In alternative embodiments NVRAM controller may perform memory access to NVRAM regardless of which transaction type it is. It is understood that the decode path can be different from what is described above. For example IMC may decode the target address of an access request and determine whether it is directed to NVRAM . If it is directed to NVRAM IMC generates an attribute according to decode table . Based on the attribute IMC then forwards the request to appropriate downstream logic e.g. NVRAM controller and I O subsystem to perform the requested data access. In yet another embodiment NVRAM controller may decode the target address if the corresponding attribute is not passed on from the upstream logic e.g. IMC and I O subsystem . Other decode paths may also be implemented.

The presence of a new memory architecture such as described herein provides for a wealth of new possibilities. Although discussed at much greater length further below some of these possibilities are quickly highlighted immediately below.

According to one possible implementation NVRAM acts as a total replacement or supplement for traditional DRAM technology in system memory. In one embodiment NVRAM represents the introduction of a second level system memory e.g. the system memory may be viewed as having a first level system memory comprising near memory as cache B part of the DRAM device and a second level system memory comprising far memory FM B part of the NVRAM .

According to some embodiments NVRAM acts as a total replacement or supplement for the flash magnetic optical mass storage B. As previously described in some embodiments even though the NVRAM A is capable of byte level addressability NVRAM controller may still access NVRAM mass storage A in blocks of multiple bytes depending on the implementation e.g. 64 Kbytes 128 Kbytes etc. . The specific manner in which data is accessed from NVRAM mass storage A by NVRAM controller may be transparent to software executed by the processor . For example even through NVRAM mass storage A may be accessed differently from Flash magnetic optical mass storage A the operating system may still view NVRAM mass storage A as a standard mass storage device e.g. a serial ATA hard drive or other standard form of mass storage device .

In an embodiment where NVRAM mass storage A acts as a total replacement for the flash magnetic optical mass storage B it is not necessary to use storage drivers for block addressable storage access. The removal of storage driver overhead from storage access can increase access speed and save power. In alternative embodiments where it is desired that NVRAM mass storage A appears to the OS and or applications as block accessible and indistinguishable from flash magnetic optical mass storage B emulated storage drivers can be used to expose block accessible interfaces e.g. Universal Serial Bus USB Bulk Only Transfer BOT 1.0 Serial Advanced Technology Attachment SATA 3.0 and the like to the software for accessing NVRAM mass storage A.

In one embodiment NVRAM acts as a total replacement or supplement for firmware memory such as BIOS flash and TPM flash illustrated with dotted lines in to indicate that they are optional . For example the NVRAM may include a BIOS NVRAM portion to supplement or replace the BIOS flash and may include a TPM NVRAM portion to supplement or replace the TPM flash . Firmware memory can also store system persistent states used by a TPM to protect sensitive system information e.g. encryption keys . In one embodiment the use of NVRAM for firmware memory removes the need for third party flash parts to store code and data that are critical to the system operations.

Continuing then with a discussion of the system of in some embodiments the architecture of computer system may include multiple processors although a single processor is illustrated in for simplicity. Processor may be any type of data processor including a general purpose or special purpose central processing unit CPU an application specific integrated circuit ASIC or a digital signal processor DSP . For example processor may be a general purpose processor such as a Core i3 i5 i7 2 Duo and Quad Xeon or Itanium processor all of which are available from Intel Corporation of Santa Clara Calif. Alternatively processor may be from another company such as ARM Holdings Ltd of Sunnyvale Calif. MIPS Technologies of Sunnyvale Calif. etc. Processor may be a special purpose processor such as for example a network or communication processor compression engine graphics processor co processor embedded processor or the like. Processor may be implemented on one or more chips included within one or more packages. Processor may be a part of and or may be implemented on one or more substrates using any of a number of process technologies such as for example BiCMOS CMOS or NMOS. In the embodiment shown in processor has a system on a chip SOC configuration.

In one embodiment the processor includes an integrated graphics unit which includes logic for executing graphics commands such as 3D or 2D graphics commands. While the embodiments of the invention are not limited to any particular integrated graphics unit in one embodiment the graphics unit is capable of executing industry standard graphics commands such as those specified by the Open GL and or Direct X application programming interfaces APIs e.g. OpenGL 4.1 and Direct X 11 .

The processor may also include one or more cores although a single core is illustrated in again for the sake of clarity. In many embodiments the core s includes internal functional blocks such as one or more execution units retirement units a set of general purpose and specific registers etc. If the core s are multi threaded or hyper threaded then each hardware thread may be considered as a logical core as well. The cores may be homogenous or heterogeneous in terms of architecture and or instruction set. For example some of the cores may be in order while others are out of order. As another example two or more of the cores may be capable of executing the same instruction set while others may be capable of executing only a subset of that instruction set or a different instruction set.

The processor may also include one or more caches such as cache which may be implemented as a SRAM and or a DRAM. In many embodiments that are not shown additional caches other than cache are implemented so that multiple levels of cache exist between the execution units in the core s and memory devices B and B. For example the set of shared cache units may include an upper level cache such as a level 1 L1 cache mid level caches such as level 2 L2 level 3 L3 level 4 L4 or other levels of cache an LLC and or different combinations thereof. In different embodiments cache may be apportioned in different ways and may be one of many different sizes in different embodiments. For example cache may be an 8 megabyte MB cache a 16 MB cache etc. Additionally in different embodiments the cache may be a direct mapped cache a fully associative cache a multi way set associative cache or a cache with another type of mapping. In other embodiments that include multiple cores cache may include one large portion shared among all cores or may be divided into several separately functional slices e.g. one slice for each core . Cache may also include one portion shared among all cores and several other portions that are separate functional slices per core.

The processor may also include a home agent which includes those components coordinating and operating core s . The home agent unit may include for example a power control unit PCU and a display unit. The PCU may be or include logic and components needed for regulating the power state of the core s and the integrated graphics unit . The display unit is for driving one or more externally connected displays.

As mentioned in some embodiments processor includes an integrated memory controller IMC near memory cache MSC controller and NVRAM controller all of which can be on the same chip as processor or on a separate chip and or package connected to processor . DRAM device may be on the same chip or a different chip as the IMC and MSC controller thus one chip may have processor and DRAM device one chip may have the processor and another the DRAM device and these chips may be in the same or different packages one chip may have the core s and another the IMC MSC controller and DRAM these chips may be in the same or different packages one chip may have the core s another the IMC and MSC controller and another the DRAM these chips may be in the same or different packages etc.

In some embodiments processor includes an I O subsystem coupled to IMC . I O subsystem enables communication between processor and the following serial or parallel I O devices one or more networks such as a Local Area Network Wide Area Network or the Internet storage I O device such as flash magnetic optical mass storage B BIOS flash TPM flash and one or more non storage I O devices such as display keyboard speaker and the like . I O subsystem may include a platform controller hub PCH not shown that further includes several I O adapters and other I O circuitry to provide access to the storage and non storage I O devices and networks. To accomplish this I O subsystem may have at least one integrated I O adapter for each I O protocol utilized. I O subsystem can be on the same chip as processor or on a separate chip and or package connected to processor .

I O adapters translate a host communication protocol utilized within the processor to a protocol compatible with particular I O devices. For flash magnetic optical mass storage B some of the protocols that I O adapters may translate include Peripheral Component Interconnect PCI Express PCI E 3.0 USB 3.0 SATA 3.0 Small Computer System Interface SCSI Ultra 640 and Institute of Electrical and Electronics Engineers IEEE 1394 Firewire among others. For BIOS flash some of the protocols that I O adapters may translate include Serial Peripheral Interface SPI Microwire among others. Additionally there may be one or more wireless protocol I O adapters. Examples of wireless protocols among others are used in personal area networks such as IEEE 802.15 and Bluetooth 4.0 wireless local area networks such as IEEE 802.11 based wireless protocols and cellular protocols.

In some embodiments the I O subsystem is coupled to a TPM control to control access to system persistent states such as secure data encryption keys platform configuration information and the like. In one embodiment these system persistent states are stored in a TMP NVRAM and accessed via NVRAM controller .

In one embodiment TPM is a secure micro controller with cryptographic functionalities. TPM has a number of trust related capabilities e.g. a SEAL capability for ensuring that data protected by a TPM is only available for the same TPM. TPM can protect data and keys e.g. secrets using its encryption capabilities. In one embodiment TPM has a unique and secret RSA key which allows it to authenticate hardware devices and platforms. For example TPM can verify that a system seeking access to data stored in computer system is the expected system. TPM is also capable of reporting the integrity of the platform e.g. computer system . This allows an external resource e.g. a server on a network to determine the trustworthiness of the platform but does not prevent access to the platform by the user.

In some embodiments I O subsystem also includes a Management Engine ME which is a microprocessor that allows a system administrator to monitor maintain update upgrade and repair computer system . In one embodiment a system administrator can remotely configure computer system by editing the contents of the decode table through ME via networks .

For convenience of explanation the remainder of the application sometimes refers to NVRAM as a PCMS device. A PCMS device includes multi layered vertically stacked PCM cell arrays that are non volatile have low power consumption and are modifiable at the bit level. As such the terms NVRAM device and PCMS device may be used interchangeably in the following discussion. However it should be realized as discussed above that different technologies besides PCMS may also be utilized for NVRAM .

It should be understood that a computer system can utilize NVRAM for system memory mass storage firmware memory and or other memory and storage purposes even if the processor of that computer system does not have all of the above described components of processor or has more components than processor .

In the particular embodiment shown in the MSC controller and NVRAM controller are located on the same die or package referred to as the CPU package as the processor . In other embodiments the MSC controller and or NVRAM controller may be located off die or off CPU package coupled to the processor or CPU package over a bus such as a memory bus like a DDR bus e.g. a DDR3 DDR4 etc a PCI express bus a desktop management interface DMI bus or any other type of bus.

As discussed above in various configurations near memory can be configured as a caching layer for far memory. Here specific far memory storage devices e.g. specific installed PCMS memory chips may be reserved for specific e.g. a specific range of system memory addresses. As such specific near memory storage devices e.g. specific installed DRAM memory chips may be designed to act as a caching layer for the specific far memory storage devices. Accordingly these specific near memory storage devices should have the effect of reducing the access times of the most frequently accessed system memory addresses that the specific far memory storage devices are designed to provide storage for.

According to a further approach observed in the near memory devices are configured as a direct mapped cache for their far memory counterparts. As is well understood in the art a direct mapped cache is designed such that each entry in the cache is reserved for a unique set of entries in the deeper storage. That is in this case the storage space of the far memory can be viewed as being broken down into different storage sets     . . .  N where each set is allocated an entry in the cache . As such as observed in entry   is reserved for any of the system memory addresses associated with set   entry   is reserved for any of the system memory addresses associated with set   etc. Generally any of the structural logic blocks that appear in as well as any of and may be largely if not entirely implemented with logic circuitry.

For example according to one exemplary implementation the cache line size is 64 bytes cache is implemented with approximately 1 Gigabyte GB of DRAM storage and far memory storage is implemented with approximately 16 Gigabytes GB of PCMS storage. Address portions and correspond to 34 bits of address space A 33 0 . Here lower ordered bits correspond to address bits A 5 0 set address bits correspond to address bits A 29 6 and tag address bits correspond to address bits A 33 30 .

From this arrangement note that the four tag bits specify a value within a range of 1 to 16 which corresponds to the ratio of DRAM storage to PCMS storage. As such each entry in cache will map to i.e. provide cacheable support across sixteen different far memory cache lines. This arrangement essentially defines the size of each set in far memory 16 cache lines per set . The number of sets which corresponds to the number of entries in cache is defined by set bits . In this example set bits corresponds to 24 bits of address space address bits A 29 6 which in turn corresponds to 16 777 216 cache entries sets. A 64 byte cache line therefore corresponds to approximately 1 GB of storage within cache 16 777 216 64 bytes 1 073 741 824 bytes .

If the size of the cache were doubled to include 2 GB of DRAM there would be eight cache lines per set instead of sixteen because the DRAM PCMS ratio would double to 2 16 1 8. As such the tag would be expressed with three bits A 33 31 instead of four bits. The doubling of the DRAM space is further accounted for by providing an additional most significant bit to set bits i.e. address bits A 30 6 instead of A 29 6 which essentially doubles the number of sets.

The far memory storage observed in may correspond to only a subset of the computer system s total far memory storage. For example a complete system memory for a computing system may be realized by incorporating multiple instances of the near far memory sub system observed in e.g. one instance for each unique subset of system memory addresses . Here according to one approach higher ordered bits are used to indicate which specific instance amongst the multiple near far memory subsystems apply for a given system memory access. For example if each instance corresponds to a different memory channel that stems from a host side or more generally a host higher ordered bits would effectively specify the applicable memory channel. In an alternate approach referred to as a permuted addressing approach higher order bits are not present. Rather bits represent the highest ordered bits and bits within lowest ordered bit space are used to determine which memory channel is to be utilized for the address. This approach is thought to give better system performance by effectively introducing more randomization into the specific memory channels that are utilized over time. Address bits can be in any order.

As observed in in an embodiment each cache entry includes along with its corresponding data an embedded tag a dirty bit and ECC information . The embedded tag identifies which cache line in the entry s applicable set in far memory is cached in cache . The dirty bit indicates whether the cached entry is the only valid copy for the cache line. ECC information as is known in the art is used to detect and possibly correct for errors that occurred writing and or reading the entry from to the cache .

After the cached entry for the applicable set is read with the near memory cache interface logic the MSC hit miss logic compares the embedded tag of the just read entry against the tag of the address of the write transaction note that the entry read from the cache may be stored in a read buffer . If they match the cached entry corresponds to the target of the transaction cache hit . Accordingly the hit miss logic causes the near memory cache interface logic to write over the just read cache entry in the cache with the new data received for the transaction. The MSC control logic in performing the write keeps the value of the embedded tag unchanged. The MSC control logic also sets the dirty bit to indicate that the newly written entry corresponds to the only valid version the cache line and calculates new ECC data for the cache line. The cache line read from the cache in read buffer is discarded. At this point the process ends for a cache hit.

If the embedded tag of the cache line read from cache does not match the tag of the transaction address cache miss as with a cache hit the hit miss logic causes the near memory cache interface logic to write the new data associated with the transaction into the cache with the set bits specified as the address to effectively write over the cache line that was just read from the cache . The embedded tag is written as the tag bits associated with the transaction. The dirty bit is written to indicate that the cached entry is the only valid copy for this cache line. The memory controller s ECC logic calculates ECC information for the cache line received with the transaction and the near memory cache interface logic writes it into cache along with the cache line.

With respect to the cache line that was just read from the cache and is stored in the read buffer the near memory hit miss logic checks its associated dirty bit and if the dirty bit indicates that the cache line in the read buffer is the only valid version of the cache line the dirty bit is set the hit miss logic causes the NVRAM controller through its far memory interface logic to write the cache line into its appropriate far memory location using the set bits of the transaction and the embedded tag bits of the cache line that was just read as the address . Here far memory interface logic is responsible for implementing a protocol including the generation reception of electrical signals specific to the far memory e.g. PCMS on memory channel . If the dirty bit of the cache line in the read buffer indicates that the cache line in the read buffer is not the only valid version of the cache line the cache line in the read buffer is discarded.

Here during moments where the interfaces to the near memory cache and far memory are not busy the MSC control logic may read cache line entries from the cache and for those cache line entries having its dirty bit set the memory controller will rewrite it into far memory and clear its associated dirty bit to indicate that the cache line in cache is no longer the only valid copy of the cache line.

Moreover it is pertinent to point out that the respective near memory cache and far memory interfaces can be completely isolated from one another or have some overlap with respect to one another. Here overlap corresponds to aspects of the respective near and far memory protocols and or signaling that are the same e.g. same clocking signals same on die termination signals same addressing signals etc. and therefore may use the same circuitry for access to near memory cache and far memory. Non overlapping regions correspond to aspects of the two protocols and or signaling that are not the same and therefore have circuitry applicable to only one of near memory cache and far memory.

The architecture described above can be used in implementations where the MSC control logic is coupled to the near memory cache over a different isolated memory channel than the memory channel through which the NVRAM controller and far memory are coupled through. Here for any specific channel one of interfaces is enabled while the other is disabled depending on whether near memory cache or far memory is coupled to the channel. Likewise one of MSC control logic and NVRAM controller is enabled while the other is disabled. In an embodiment a configuration register associated with the memory controller not shown which for example may be written to by BIOS determines which configuration is to be enabled.

The same architecture above may also support another configuration in which near memory cache and far memory are coupled to the same channel . In this case the integration of interfaces can be viewed as a single interface to the channel . According to this configuration both interfaces and both controllers are enabled but only one set interface and controller for near memory and interface and controller for far memory is able to use the channel at any particular instant of time. Here the usage of the channel over time alternates between near memory signaling and far memory signaling. This configuration may be established with for instance a third setting in the aforementioned configuration register. It is to this setting that the below discussion mostly pertains.

Here by being able to use the same channel for both near memory accesses and far memory accesses the near memory cache that is plugged into the channel can be used as the near memory cache for the far memory storage that is plugged into the same channel. Said another way specific system memory addresses may be allocated to the one single channel. The far memory devices that are plugged into the channel provides far memory storage for these specific system memory addresses and the near memory storage that is plugged into the same channel provides the cache space for these far memory devices. As such the above described transactions that invoke both near memory and far memory e.g. because of a cache miss and or a dirty bit that is set can transpire over the same channel.

According to one approach the channel is designed to include mechanical receptacles connectors that individual planar board cards having integrated circuits disposed on them e.g. DIMMs can plug into. Here the cards have corresponding receptacles connectors that mate with the channel s receptacles connectors. One or more cards having only far memory storage can be plugged into a first set of connectors to effect the far memory storage for the channel. One or more cards having only near memory storage can be plugged into the same channel and act as near memory cache for the far memory cards.

Here where far memory storage is inherently denser than near memory storage but near memory storage is inherently faster than far memory storage channels can be designed with a speed vs. density tradeoff in mind. That is the more near memory cards plugged into the channel the faster the channel will perform but at the cost of less overall storage capacity supported by the channel. Contra wise the fewer near memory cards plugged into to the channel the slower the channel will perform but with the added benefit of enhanced storage capacity supported by the channel. Extremes may include embodiments where only the faster memory storage technology e.g. DRAM is populated in the channel in which case it may act like a cache for far memory on another channel or not act like a cache but instead is allocated its own specific system memory address space or only the slower memory storage technology e.g. PCMS is populated in the channel.

In other embodiments near memory and far memory are disposed on a same card in which case the speed density tradeoff is determined by the card even if a plurality of such cards are plugged into the same channel.

After the cached entry for the applicable set is read with the cache interface logic the hit miss logic compares the embedded tag of the just read entry against the tag of the address of the read transaction . If they match the cached entry corresponds to the target of the transaction cache hit . Accordingly the read process ends. If the embedded tag of the cache line read from cache does not match the tag of the transaction address cache miss the hit miss logic causes the far memory interface logic to read the far memory storage at the address specified in the transaction . The cache line read from far memory is then written into the cache and if the dirty bit was set for the cache line that was read from near memory cache in step the cache line that was read from near memory cache is written into far memory .

Although the MSC controller may perform ECC checking on the read data that was read from far memory as described in more detail below according to various embodiments ECC checking may be performed by logic circuitry that resides local to the far memory device s e.g. affixed to a same DIMM card that PCMS device s are affixed to . This same logic circuitry may also calculate the ECC information for a write transaction in the case of a cache miss and the dirty bit is set .

Moreover in embodiments where the same memory channel is used to communicate near memory signaling and far memory signaling logic circuitry can be utilized to speed up the core write and read processes described above. Some of these speed ups are discussed immediately below.

Referring to note that the near memory storage devices     . . .  N such as a plurality of DRAM chips are coupled to a channel independently of the coupling of far memory logic circuitry and its associated far memory storage devices     . . .  M such as a plurality of PCMS chips to the same channel .

Said another way a near memory platform and a far memory platform are separately connected to the same channel independently of one another. This approach can be realized for example with different DIMMS having different respective memory storage technologies plugged into a same memory channel e.g. near memory platform corresponds to a DRAM DIMM and far memory platform corresponds to a PCMS DIMM . This approach can also be realized for example with a same DIMM that incorporates different respective memory storage technologies e.g. near memory platform corresponds to one side of a DIMM and far memory platform corresponds to the other side of the DIMM .

According to one approach explained in more detail below the tag bits and read write information are snuck on unused row or column addresses of the near memory address bus. In a further embodiment more column address bits are used for this purpose than row address bits. According to an even further approach the sneaked information is provided over a command bus component of channel which is used for communicating addressing information to the near memory storage device and potentially the far memory devices as well .

Because remote control logic circuitry is connected to the channel it can snarf 1 the tag bits from the original request and indication of a read transaction when they are snuck on the channel 2 the read address applied to the near memory cache and 3 the cache line and its associated embedded tag bits dirty bit and ECC information when read from the near memory cache . Here the snarfing is understood to include storing any all of these items of information locally e.g. in register space embedded on logic circuitry .

As such far memory control logic circuitry which also includes its own hit miss logic can determine whether there is a cache hit or cache miss concurrently with the memory controller s hit miss logic . In the case of a cache hit the far memory control logic circuitry takes no further action and the memory controller performs the ECC calculation on the data read from cache and compares it with the embedded ECC information to determine whether or not the cache read data is valid.

However in the case of a cache miss and with knowledge that the overall transaction is a read transaction e.g. from snuck information the logic circuitry will recognize that a read of its constituent far memory storage will be needed to ultimately service the original read request. As such according to one embodiment logic circuitry can automatically read its associated far memory resources to retrieve the desired read information perform an ECC calculation on the cache line read from far memory which also has embedded ECC information and if there is no corruption in the data provide the desired far memory read information.

In order to perform this kind of automatic read as alluded to just above logic circuitry should be informed by the memory controller in some manner that the overall transaction is a read operation as opposed to a write operation if the above described transaction were a write transaction logic circuitry would not need to perform a read of far memory . According to one embodiment as already mentioned above read write information that is indicative as to whether a write transaction or a read transaction is at play is snuck to logic circuitry e.g. along with the tag information of the original transaction request .

Concurrently with the far memory control logic automatically reading far memory the memory controller can schedule and issue a read request on the channel to the far memory control logic . As described in more detail below in an embodiment the memory controller is configured to communicate two different protocols over channel i a first protocol that is specific to the near memory devices e.g. an industry standard DDR DRAM protocol and ii a second protocol that is specific to the far memory devices e.g. a protocol that is specific to PCMS devices . Here the near memory cache read request is implemented with the first protocol and by contrast the read request to far memory is implemented with the second protocol.

In a further embodiment as described in more detail further below because the time needed by the far memory devices to respond to the read request cannot be predicted with certainty an identifier of the overall read transaction transaction id is sent to the far memory control logic along with the far memory read request sent by the memory controller. When the data is finally read from far memory it is eventually sent to the memory controller . In an embodiment the transaction identifier is returned to the memory controller as part of the transaction on the channel that sends the read data to the memory controller .

Here the inclusion of the transaction identifier serves to notify the memory controller of the transaction to which the read data pertains to. This may be especially important where as described in more detail below the far memory control logic maintains a buffer to store multiple read requests from the memory controller and the uncertainty of the read response time of the far memory leads to out of order OOO read responses from far memory a subsequent read request may be responded to before a preceding read request . In a further embodiment a distinctive feature of the two protocols used on the channel is that the near memory protocol treats devices as slave devices that do not formally request use of the channel because their timing is well understood and under the control of the memory controller . By contrast the far memory protocol permits far memory control logic to issue a request to the memory controller for the sending of read data to the memory controller . As a further point of distinction the tag and r w information that is snuck onto the channel during the near memory cache read is snuck in the sense that this information is being transported to the far memory control logic circuitry and is pertinent to a potential far memory access even though technically the near memory protocol is in play.

Alternatively to the automatic read discussed above with respect to the far memory control logic circuitry can be designed to refrain from automatically reading the needed data and instead wait for a read request and corresponding address from the memory controller in the case of a cache miss. In this case logic circuitry need not snarf the address when the near memory cache is read nor does any information concerning whether the overall transaction is a read transaction or a write transaction need to be snuck to logic circuitry . The sending of a transaction ID with the read request to the far memory control logic may still be needed if far memory control logic can service read requests out of order.

Regardless as to whether or not the logic circuitry automatically performs a needed far memory read on a cache miss as observed in in the case of a cache miss detected by the far memory control logic circuitry the hit miss logic circuitry of far memory control logic circuitry can be designed to check if the dirty bit is set in the snarfed cache line . If so the snarfed cache line will need to be written to far memory . As such logic circuitry can then automatically store the snarfed cache line into its constituent far memory storage resources without a formal request from the memory controller including the recalculation of the ECC information before it is stored to ensure the data is not corrupted .

Here depending on implementation for the write operation to the far memory platform logic circuitry can construct the appropriate write address either by snarfing the earlier read address of the near memory cache read as described above and combining it with the embedded tag information of the cache line that was read from the near memory cache. Alternatively if logic circuitry does not snarf the cache read address it can construct the appropriate write address by combining the tag information embedded in the snarfed cache line with a read address provided by the memory controller when it requests the read of the correct information from far memory. Specifically logic circuitry can combine the set and lowered ordered bits portions of the read request with the embedded tag on the snarfed cache line to fully construct the correct address.

Automatically performing the write to the far memory platform as described above eliminates the need for the memory controller to request the write to the far memory platform but also and in furtherance completely frees the channel of any activity related to the write to the far memory platform. This may correspond to a noticeable improvement in the speed of the channel.

It is pertinent to point that the pair of speed ups described just above automatic read of far memory and automatic write to far memory can be implemented in any combination both just one depending on designer choice.

As a matter of contrast a basic read transaction without any speedup offered by the presence of the far memory controller nominally includes six atomic operations for a read transaction that suffers a cache miss when the dirty bit is set. These are cache read request cache read response far memory read request far memory read response near memory write request cache update and far memory write request load cache line read from cache into far memory because dirty bit is set .

By contrast with both of the speedups of automatic read of far memory and automatic write to far memory being implemented the overall transaction can be completed with only four atomic operations on the channel. That is the far memory read request and far memory write request can be eliminated.

The above discussion concerned read transaction processes when the near memory is in front of the far memory control logic. In the case of a write transaction process referring to in response to the receipt of a write transaction the memory controller initiates a near memory cache read and sneaks tag information and information indicating that the overall transaction is a write and not a read as described above . After the read of near memory is complete the memory controller writes the new data over the old data in cache . In an embodiment the memory controller checks to see if there is a cache hit and or if the dirty bit is set to understand what action the far memory control logic circuitry will take e.g. for channel scheduling but otherwise takes no further action on the channel.

Far memory control logic circuitry snarfs the address used to access the cache the sneaked information and the cache line read from cache with its associated information and detects the cache miss on its own accord as described above. If there is a cache hit far memory control logic takes no further action. If there is a cache miss depending on design implementation similar to the processes described above logic circuitry can also detect whether the dirty bit is set and write the snarfed cache line into far memory automatically without a request from the memory controller .

In an alternate approach the memory controller after detecting a cache miss and that the dirty bit is set sends a request to the far memory control logic including the write address to write the cache line read from the cache into far memory. The memory controller can also send the cache line read from cache to the far memory control logic over the channel .

Referring to which depicts a near memory behind architecture note that the near memory storage devices     . . .  N such as a plurality of DRAM chips are coupled to at least a portion of the channel through the far memory control logic circuitry at least to some extent. Here whereas the far memory control logic for a near memory in front of approach includes distinct interfaces for the channel and far memory by contrast the far memory control logic for the near memory behind approach includes distinct interfaces for the channel far memory and near memory. According to one embodiment the channel can be viewed as having three principle sub components 1 a command bus over which read and write requests and their corresponding addresses are sent 2 a data bus over which read and write data is sent and 3 control signals e.g. select signal s clock enable signal s on die termination signal s .

As depicted in the particular approach of the data bus of the near memory storage platform may be independently coupled to the data bus but is coupled to the command bus and control signals components through logic circuitry . The far memory storage platform is coupled to all three subcomponents through logic circuitry . In an alternate embodiment the data bus of the near memory storage platform like the far memory storage platform is coupled to the channel s data bus component through logic circuitry . The near memory behind architecture may at least be realized for example with the logic circuitry near memory storage devices and far memory storage devices all being implemented on a same physical platform e.g. a same DIMM card that plugs into the channel where multiple such DIMM cards can be plugged into the channel .

Logic circuitry in response to the received read request presents the associated address on the local near memory address bus to effect a cache read operation to the near memory platform. The appropriate cache line from the near memory platform is subsequently presented on the data bus either directly by the near memory platform in which case the memory controller performs the ECC calculation or through the far memory control logic in which case both logic and memory controller may perform ECC calculations.

Because far memory control logic circuitry is connected to the channel it can snarf or otherwise locally store e.g. in its own register space any of 1 the tag bits that were snuck on the channel 2 the address information used to address the near memory cache and 3 the cache line from near memory and its associated embedded tag bits dirty bit and ECC information when provided by the near memory platform .

In response the hit miss logic of logic circuitry can determine whether there is a cache hit or cache miss concurrently with the memory controller s hit miss logic . In the case of a cache hit the information read from near memory is provided to the memory controller and logic circuitry takes no further action. In an embodiment where the near memory cache platform is connected to the data bus without going through logic circuitry the memory controller performs the ECC calculation on the cache line read from near memory cache. In another embodiment where the near memory cache platform connects to the data bus through logic circuitry the ECC calculation on the cache line read from near memory cache is calculated on both logic circuitry and the memory controller .

In the case of a cache miss detected by the logic circuitry the cache hit miss logic circuitry will recognize that a read of the far memory storage platform will be needed to ultimately service the original read request. As such according to one embodiment the logic circuitry can automatically read from the far memory platform to retrieve the desired read information and perform an ECC calculation.

Concurrently with the far memory control logic automatically reading far memory recalling that the memory controller has already been provided with the cache line read from near memory the memory controller can likewise detect the cache miss and in response schedule and issue a read request on the channel to the far memory control logic . As alluded to above and as described in more detail below in an embodiment the memory controller is able to communicate two different protocols over channel i a first protocol that is specific to the near memory devices e.g. an industry standard DDR DRAM protocol and ii a second protocol that is specific to the far memory devices e.g. a protocol that is specific to PCMS devices . Here the near memory cache read is implemented with a first protocol over channel and by contrast the read request to far memory is implemented with the second protocol.

In a further embodiment as alluded to above and as described in more detail further below because the time needed by the far memory devices to respond to the read request cannot be predicted with certainty an identifier of the overall read transaction transaction id is sent to the far memory control logic along with the far memory read request sent by the memory controller. When the data is finally read from far memory it is eventually sent to the memory controller . In an embodiment the transaction identifier is returned to the memory controller as part of the transaction on the channel that sends the read data to the memory controller .

Here the inclusion of the transaction identifier serves to notify the memory controller of the transaction to which the read data pertains to. This may be especially important where as described in more detail below the far memory control logic maintains a buffer to store multiple read requests from the memory controller and the uncertainty of the read response time of the far memory leads to out of order OOO read responses from far memory a subsequent read request may be responded to before a preceding read request .

In a further embodiment where two different protocols are used on the channel a distinctive feature of the two protocols is that the near memory protocol treats devices as slave devices that do not formally request use of the channel because the timing of the near memory devices is well understood and under the control of the memory controller . By contrast the far memory protocol permits far memory control logic to issue a request to the memory controller for the sending of read data to the memory controller . As an additional point of distinction the tag information that is snuck onto the channel during the near memory cache read is snuck in the sense that this information is being transported to the far memory control logic circuitry for a potential far memory read even though technically the near memory protocol is in play.

Alternatively to automatically performing the far memory read the far memory control logic circuitry can be designed to refrain from automatically reading the needed data in far memory and wait for a read request and corresponding address from the memory controller . In this case logic circuitry does not need not to keep the address when the near memory cache is read nor does it need any sneaked information concerning whether the overall transaction is a read transaction or a write transaction from the memory controller .

Regardless as to whether or not the logic circuitry automatically performs a far memory read in the case of a cache miss as observed in the process of the hit miss logic circuitry of logic circuitry can be designed to write the cache line that was read from near memory cache into far memory when a cache miss occurs and the dirty bit is set. In this case at a high level the process is substantially the same as that observed in except that the write to near memory is at least partially hidden from the channel in the sense that the near memory platform is not addressed over the channel. If the data bus of the near memory platform is not directly coupled to the data bus of the channel but is instead coupled to the data bus of the channel through the far memory control logic the entire far memory write can be hidden from the channel .

Automatically performing the write to the far memory platform in this manner not only eliminates the need for the memory controller to request the write but also completely frees the channel of any activity related to the write to the far memory platform . This should correspond to a noticeable improvement in the speed of the channel.

Additional efficiency may be realized if the far memory control logic circuitry is further designed to update the near memory cache platform with the results of a far memory read operation in the case of a cache miss in order to effect the cache update step. Here as the results of the far memory read operation correspond to the most recent access to the applicable set these results also need to be written into the cache entry for the set in order to complete the transaction. By updating the cache with the far memory read response a separate write step over the channel to near memory to update the cache is avoided. Here some mechanism e.g. additional protocol steps may need to be implemented into the channel so that the far memory control logic can access the near memory e.g. if the usage of the near memory is supposed to be scheduled under the control of the memory controller .

It is pertinent to point that the speed ups described just above automatic read of far memory automatic write to far memory and cache update concurrent with read response may be implemented in any combination all any two just one depending on designer choice.

In the case of a write transaction process according to one approach where the near memory data bus is directly coupled to the channel data bus the process described above with respect to can be performed. Another approach presented in may be used where the near memory data bus is coupled to the channel data bus through the far memory control logic .

According to the process of in response to the receipt of a write transaction the memory controller sends a write command to the far memory control logic including the corresponding address and data and sneaks the write transaction s tag information over the channel. In response the far memory control logic performs a read of the near memory cache platform and determines from the embedded tag information and the sneaked tag information whether a cache miss or cache hit has occurred . In the case of a cache hit or a cache miss when the dirty bit is not set the new write data received with the write command is written to near memory cache . In the case of a cache miss and the dirty bit is set the far memory control logic circuitry writes the new write data received with the write command into near memory cache and writes the evicted cache line just read from near memory into far memory .

Recall from the discussion of the read transaction of that information indicative of whether the overall transaction is a read or write does not need to be snuck to the far memory control logic in a near memory behind approach. This can be seen from which show the memory controller initially communicating a near memory read request in the case of an overall read transaction or initially communicates a near memory write transaction in the case of an overall write transaction .

As observed in communications between the memory controller and near memory devices may be carried over a same channel that communications between the memory controller and far memory devices are communicated. Further as mentioned above near memory and far memory may be accessed by the memory controller with different protocols e first protocol for accessing near memory and a second protocol for accessing far memory. As such two different protocols may be implemented for example on a same memory channel. Various aspects of these protocols are discussed immediately below.

Two basic approaches for accessing near memory were presented in the sections above a first where the near memory storage devices reside in front of the far memory control logic and a second where the near memory storage devices reside behind the far memory control logic.

At least in the case where the near memory devices are located in front of the far memory control logic it may be beneficial to preserve or otherwise use an existing known protocol for communicating with system memory. For example in the case where near memory cache is implemented with DRAM devices affixed to a DIMM card it may be beneficial to use a memory access protocol that is well established accepted for communicating with DRAM devices affixed to a DIMM card e.g. either a presently well established accepted protocol or a future well established accepted protocol . By using a well established accepted protocol for communicating with DRAM economies of scale may be achieved in the sense that DIMM cards with DRAM devices that were not necessarily designed for integration into a computing system having near and far memory levels may nevertheless be plugged into the memory channel of such a system and utilized as near memory.

Moreover even in cases where the near memory is located behind the far memory control logic when attempting to access near memory the memory controller may nevertheless be designed to communicate to the far memory control logic using well established known DRAM memory access protocol so that the system as a whole may offer a number of different system configuration options to a user of the system. For example a user can choose between using 1 DRAM only DIMM cards for near memory or 2 DIMM cards having both DRAM and PCMS devices integrated thereon with the DRAM acting as the near memory for the PCMS devices located on the same DIMM .

Implementation of a well established known DRAM protocol also permits a third user option in which a two level memory scheme near memory and far memory is not adopted e.g. no PCMS devices are used to implement system memory and instead only DRAM DIMMs are installed to effect traditional DRAM only system memory. In this case the memory controller s configuration would be set so that it behaved as a traditional memory controller that does not utilize any of the features described herein to effect near and far memory levels .

As such logic circuitry that causes the memory controller to behave like a standard memory controller would be enabled whereas logic circuitry that causes the memory controller to behave in a manner that contemplates near and far memory levels would be disabled. A fourth user option may be the reverse where system memory is implemented only in an alternative system memory technology e.g. only PCMS DIMM cards are plugged in . In this case logic may be enabled that causes the memory controller to execute basic read and write transactions only with a different protocol that is consistent with the alternative system memory technology e.g. PCMS specific signaling .

According to one embodiment of the operation of channel for near memory accesses 1 the command bus carries packets in the direction from the memory controller toward the near memory storage devices where each packet includes a read or write request and an associated address and 2 the data bus carries write data to targeted near memory devices and carries read data from targeted near memory devices.

As observed in the data bus is composed of additional lines beyond actual read write data lines  . Specifically the data bus also includes a plurality of ECC lines   and strobe lines  . As well known ECC bits are stored along with a cache line s data so that data corruption errors associated with the reading writing of the cache line can be detected. For example a 64 byte 64 B cache line may additionally include 8 bytes 8 B of ECC information such that the actual data width of the information being stored is 72 bytes 72 B . Strobes lines   are typically assigned on a per data line basis e.g. a strobe line pair is assigned for every 8 or 4 bits of data ECC . In a double data rate approach information can be written or read on both rising and falling edges of the strobes  .

With respect to the control lines in an embodiment these include select signals   clock enable lines   and on die termination lines  . As is well known multiple DIMM cards can be plugged into a same memory channel. Traditionally when a memory controller reads or writes data at a specific address it reads or writes the data from to a specific DIMM card e.g. an entire DIMM card or possibly a side of a DIMM card or other portion of a DIMM card . The select signals   are used to activate the particular DIMM card or portion of a DIMM card that is the target of the operation and deactivate the DIMM cards that are not the target of the operation.

Here the select signals   may be determined from the bits of the original read or write transaction e.g. from the CPU which effectively specify which memory channel of multiple memory channels stemming from the memory controller that is the target of the transaction and further which DIMM card of multiple DIMM cards plugged into the identified channel is the target of the transaction. Select signals   could conceivably be configured such that each DIMM card or portion of a DIMM plugged in a same memory channel receives its own one unique select signal. Here the particular select signal sent to the active DIMM card or portion of a DIMM card for the transaction is activated while the select signals sent to the other DIMM cards are deactivated. Alternatively the signal signals are routed as a bus to each DIMM card or portion of a DIMM card . The DIMM card or portion of a DIMM card that is selected is determined by the state of the bus.

The clock enable lines   and on die termination lines   are power saving features that are activated before read write data is presented on the channel s data bus and deactivated after read write data is presented on the channel s data bus  .

In various embodiments such as near memory cache constructed from DRAM the timing of near memory transactions are precisely understood in terms of the number of clock cycles needed to perform each step of a transaction. That is for near memory transactions the number of clock cycles needed to complete a read or write request is known and the number of clock cycles needed to satisfy a read or write request is known.

Note that the process of although depicting atomic operations to near memory in a future memory protocol can also be construed consistently with existing DDR protocol atomic operations. Moreover future systems that include near memory and far memory may access near memory with an already existing DDR protocol or in with a future DRAM protocol that systems of the future that only have DRAM system memory technology access DRAM system memory with.

Specifically in an implementation where the DRAM near memory cache is in front of the far memory control logic and where the far memory control logic circuitry does not update the DRAM near memory cache on a read transaction having a cache miss the memory controller will drive signals on the channel in performing steps and provide the write data on the data bus for a write transaction in step . In this case the memory controller may behave much the same as existing memory controllers or memory controllers of future systems that only have DRAM system memory. The same may be said for the manner in which the memory controller behaves with respect to when i cache is first read for either a read or a write transaction and ii cache is written after a cache hit for either a read or a write transaction.

Further still in implementations where the DRAM near memory cache is behind the far memory control logic for either a read or write of near memory cache near memory may still be accessed with a protocol that is specific to the near memory devices. For example the near memory devices may be accessed with a well established current or future DRAM DDR protocol. Moreover even if the near memory devices themselves are specifically signaled by the far memory control logic with signals that differ in some way from a well established DRAM protocol the memory controller may nevertheless in ultimately controlling the near memory accesses apply a well established DRAM protocol on the channel in communicating with the far memory control logic to effect the near memory accesses.

Here the far memory control logic may perform the local equivalent i.e. behind the far memory control logic rather than on the channel of any all of steps or aspects thereof in various combinations. In addition the memory controller may also perform each of these steps in various combinations with the far memory control logic including circumstances where far memory logic circuitry is also performing these same steps. For example the far memory control logic may be designed to act as a forwarding device that simply accepts signals from the channel originally provided by the memory controller and re drives them to its constituent near memory platform.

Alternatively the far memory control logic may originally create at least some of the signals needed to perform at least some of steps or aspects thereof while the memory controller originally creates signals needed to perform others of the steps. For instance according to one approach in performing a cache read the memory controller may initially drive the select signals on the channel in performing step . In response to the receipt of the select signals the far memory control logic may simply re drive these signals to its constituent near memory platform or may process and comprehend their meaning and enable disable the near memory platform or a portion thereof according to a different selection signaling scheme than that explicitly presented on the channel by the memory controller. The select signals may also be provided directly to the near memory platform from the channel and also routed to the far memory control logic so the far memory control logic can at least recognize when its constituent near memory platform or portion thereof is targeted for the transaction.

In response to recognizing that at least a portion of its constituent near memory devices are targeted for the transaction the far memory control logic may originally and locally create any all of the clock enable signals and or on die termination signals in step behind the far memory control logic between the control logic and the near memory storage devices. These signals may be crafted by the far memory control logic from a clock signal or other signal provided on the channel by the memory controller. Any clock enable signals or on die termination signals not created by the far memory control logic may be provided on the channel by the memory controller and driven to the near memory platform directly or re driven by the near memory control logic.

For near memory cache read operations the memory controller may perform step by providing a suitable request and address on the command bus of the channel. The far memory control logic may receive the command from the channel and locally store its pertinent address information . It may also re drive or otherwise present the read command and address to the near memory platform. With respect to step the memory controller will also receive the cache read data. The read data may be presented on the channel s data bus by the far memory control logic circuitry in re driving the read data provided by the near memory platform or the read data may be driven on the channel s data bus by the near memory platform directly.

With respect to near memory channel operations that occur after a cache read such as a write to cache after a cache hit for a write transaction the far memory control logic circuitry or the memory controller may perform any of steps in various combinations consistent with the principles described just above. At one extreme the far memory control logic circuitry performs each of steps and independently of the memory controller. At another extreme the memory controller performs each of steps and and the far memory control logic circuitry re drives all or some of them to the near memory platform or receives and comprehends and then applies its own signals to the near memory platform in response. In between these extremes the far memory control logic may perform some of steps and or aspects thereof while the memory controller performs others of these steps or aspects thereof.

The atomic operations described just above may be integrated as appropriate with the embodiments disclosed above in the preceding sections.

Recall that where near memory cache is constructed from DRAM for example the timing of near memory transactions are precisely understood in terms of the number of clock cycles needed to perform each step of a transaction. That is for near memory transactions the number of clock cycles needed to complete a read or write request is known and the number of clock cycles needed to satisfy a read or write request is known. As such near memory accesses may be entirely under the control of the memory controller or at least the memory controller can precisely know the time spent for each near memory access e.g. for scheduling purposes .

By contrast for far memory transactions although the number of clock cycles needed to complete a read or write request over the command bus may be known because the memory controller is communicating to the near memory control logic circuitry the number of clock cycles needed to satisfy any such read or write request to the far memory devices themselves is unknown. As will be more apparent in the immediately following discussion this may lead to the use of an entirely different protocol on the channel for far memory accesses than that used for near memory accesses.

In an attempt to keep the reliability of the various storage cells approximately equal logic circuitry and or interface circuitry may include wear out leveling algorithm circuitry that at appropriate moments moves the data content of more frequently accessed storage cells to less frequently accessed storage cells and likewise moves the data content of less frequently accessed storage cells to more frequently accessed storage cells . When the far memory control logic has a read or write command ready to issue to the far memory platform a wear out leveling procedure may or may not be in operation or if in operation the procedure may have only just started or may be near completion or anywhere in between.

These uncertainties as well as other possible timing uncertainties stemming from the underlying storage technology such as different access times applied to individual cells as a function of their specific past usage rates lead to the presence of certain architectural features. Specifically with respect to the near memory control logic a far memory write buffer exists to hold write requests to far memory and a far memory read buffer exists to hold far memory read requests. Here the presence of the far memory read and write buffers permits the queuing or temporary holding of read and write requests.

If a read or write request is ready to issue to the far memory devices but the far memory devices are not in a position to receive any such request e.g. because a wear leveling procedure is currently in operation the requests are held in their respective buffers until the far memory devices are ready to accept and process them. Here the read and write requests may build up in the buffers from continued transmissions of such requests from the memory controller and or far memory control logic e.g. in implementations where the far memory control logic is designed to automatically access near memory as described above until the far memory devices are ready to start receiving them.

A second architectural feature is the ability of the memory controller to interleave different portions of read and write transactions e.g. from the CPU on the channel to enhance system throughput. For example consider a first read transaction that endures a cache miss which forces a read from far memory. Because the memory controller does not know when the read request to far memory will be serviced rather than potentially idle the channel waiting for a response the memory controller is instead free to issue a request that triggers a cache read for a next read or write transaction. The process is free to continue until some hard limit is reached.

For example the memory controller is free to initiate a request for a next read transaction until it recognizes that either the far memory control logic s read buffer is full because a cache miss would create a need for a far memory read request or the far memory control logic s write buffer is full because a set dirty bit on a cache miss will create a need for a far memory write request . Similarly the memory controller is free to initiate a request for a next write transaction until it recognizes that the far memory control logic s write buffer is full because a set dirty bit on a cache miss will create a need for a far memory write request .

In an embodiment the memory controller maintains a count of credits for each of the write buffer and the read buffer . Each time the write buffer or read buffer accepts a new request its corresponding credit count is decremented. When the credit count falls below or meets a threshold such as zero for either of the buffers the memory controller refrains from issuing on the channel any requests for a next transaction. As described in more detail below the memory controller can comprehend the correct credit count for the read buffer by 1 decrementing the read buffer credit count whenever a read request is understood to be presented to the read buffer either by being sent by the memory controller over the channel directly or understood to have been created and entered automatically by the far memory control logic and 2 decrementing the read buffer credit whenever a read response is presented on the channel for the memory controller.

Moreover again as described in more detail below the memory controller can comprehend the correct credit count for the write buffer by 1 decrementing the write buffer credit count whenever a write request is understood to be presented to the write buffer e.g. by being sent by the memory controller over the channel directly or understood to have occurred automatically by the far memory control logic and 2 decrementing the write buffer credit whenever a write request is serviced from the write buffer . In an embodiment again as described in more detail below the far memory control logic informs the memory controller of the issuance of write requests from the write buffer to the far memory storage device platform by piggybacking such information with a far memory read request response. Here a read of far memory is returned over the channel to the memory controller. As such each time far memory control logic performs a read of far memory and communicates a response to the memory controller as part of that communication the far memory control logic also informs the memory controller of the number of write requests that have issued from the write buffer since the immediately prior far memory read response.

An additional complication is that in an embodiment read requests may be serviced out of order . For example according to one design approach for the far memory control logic circuitry write requests in the write buffer are screened against read requests in the read buffer . If any of the target addresses between the two buffers match a read request having one or more matching counterparts in the write buffer is serviced with the new write data associated with the most recent pending write request. If the read request is located in any other location than the front of the read buffer queue the servicing of the read request will have the effect of servicing the request out of order with respect to the order in which read requests were entered in the queue . In various embodiments the far memory control logic may also be designed to service requests out of order because of the underlying far memory technology which may at certain times permit some address space to be available for a read but not all address space .

In order for the memory controller to understand which read request response corresponds to which read request transaction in an embodiment when the memory controller sends a read request to the far memory control logic the memory controller also provides an identifier of the transaction TX ID to the near memory control logic. When the far memory control logic finally services the request it includes the transaction identifier with the response.

Recall that and its discussion pertained to an embodiment of a memory channel and its use by a memory controller for accessing near memory cache with a first near memory access protocol. Notably is further enhanced to show information that can be snuck onto the channel by the memory controller as part of the first near memory access protocol but is nevertheless used by the far memory controller to potentially trigger a far memory access. shows the same channel and its use for accessing far memory cache by the memory controller with a second far memory access protocol.

Because in various embodiments the tag information of a cache line s full address is stored along with the data of the cache line in near memory cache e.g. embedded tag information note that indicates that when the channel is used to access near memory cache read or write some portion of bits lines   that are nominally reserved for ECC are instead used for the embedded tag information . Stealing ECC lines to incorporate the embedded tag information rather than extending the size of the data bus permits for example DIMM cards manufactured for use in a traditional computer system to be used in a system having both near and far levels of storage. That is for example if a DRAM only DIMM were installed in a channel without any far memory and thus does not act like a cache for the far memory the full width of the ECC bits would be used for ECC information. By contrast if a DIMM having DRAM were installed in a channel with far memory and therefore the DRAM acts like a cache for the far memory when the DRAM is accessed some portion of the ECC bits   would actually be used to store the tag bits of the address of the associated cache line on the data bus. The embedded tag information is present on the ECC lines during step of when the data of a near memory cache line is being written into near memory or being read from near memory.

Also recall from above that in certain embodiments the far memory control logic may perform certain acts automatically with the assistance of the additional information that is snuck to the far memory controller on the memory channel as part of a near memory request. These automatic acts may include 1 automatically detecting a cache hit or miss 2 an automatic read of far memory upon recognition of a cache miss and recognition that a read transaction is at play and 3 an automatic write to far memory upon recognition of a cache miss coupled with recognition that the dirty bit is set.

As discussed in preceding sections in order to perform 1 2 and 3 above the cache hit or miss is detected by sneaking the transaction s tag information to the far memory control logic as part of the request that triggers the near memory cache access and comparing it to the embedded tag information that is stored with the cache line and that is read from near memory.

In an embodiment referring to and the transaction s tag information is snuck to the far memory control logic over the command bus in step command phase in locations that would otherwise be reproduced as unused column and or row bits on the near memory address bus e.g. more so column than row . The snarf of the embedded tag information by the far memory control logic can be made in step of when the cache line is read from near memory by snarfing the stolen ECC bits as described above . The two tags can then be compared.

Moreover in order to perform 2 or 3 above the far memory control logic should be able to detect the type of transaction at play read or write . In the case where near memory is in front of the far memory control logic again referring to and the type of transaction at play can also be snuck to the far memory control logic over the command bus in a manner like that described for 1 just above for a transaction s tag information e.g. on the command bus during command phase . In the case where the near memory is behind the far memory control logic it is possible for the far memory control logic to detect whether the overall transaction is a read or write simply by keying off of the transaction s original request from the memory controller e.g. compare . Otherwise the same operation as for the near memory in front approach can be effected.

Additionally in order to perform 3 above referring to and the far memory control logic should be able to detect whether the dirty bit is set. Here since the dirty bit is information that is embedded with the data of a cache line in near memory another ECC bit is stolen as described just above with respect to the embedded tag information . As such the memory controller writes the dirty bit by presenting the appropriate value in one of the ECC bit locations   of the channel during step of a near memory write access. Similarly the far memory control logic can detect the dirty bit by snarfing this same ECC location during a near memory read access.

Referring to and in order to address out of order issues a transaction identifier can be sent to the far memory control logic circuit as part of a far memory read request. This can also be accomplished by presenting the transaction identifier on the command bus during the command phase of the far memory read request.

Referring to and a read request having a far memory read address is issued by the memory controller over the command bus . The read request issued over the command bus also includes a transaction identifier that is kept e.g. in a register by the far memory control logic .

The request is placed in a read buffer . Write requests held in a write buffer are analyzed to see if any have a matching target address . If any do the data for the read request response is taken from the most recently created write request . If none do eventually the read request is serviced from the read buffer read data is read from the far memory platform and ECC information for the read data is calculated and compared with the ECC information stored with the read data . If the ECC check fails an error is raised by the far memory control logic . Here referring to the error may be signaled over one of the select   clock enable   or ODT   lines.

If the read response was taken from the write buffer or the ECC check was clean the far memory control logic informs the memory controller that it has a read response ready for transmission . In an embodiment as observed in this indication is made over one of a select signal line   clock enable signal line   or an on die termination line   of the channel that is usurped for this purpose. When the memory controller which in various embodiments has a scheduler to schedule transactions on the channel decides it can receive the read response it sends an indication to the far memory control logic that it should begin to send the read response . In an embodiment as observed in this indication is also made over one of a select line   clock enable signal line   or an on die termination line   of the channel that is usurped for this purpose.

The far memory control logic then determines how many write requests have issued from the write buffer since the last read response was sent write buffer issue count . The read data is then returned over the channel along with the transaction identifier and the write buffer issue count . In an embodiment since the ECC calculation was made by the far memory control logic the data bus lines that are nominally used for ECC are essentially free . As such as observed in the transaction identifier and write buffer issue count are sent along the ECC lines   of the channel from the far memory controller to the memory controller. Here the write buffer issue count is used by the memory controller to calculate a new credit count so as to permit the sending of new write requests to the far memory control logic . The memory controller can self regulate its sending of read requests by keeping track of the number of read requests that have been entered into the read buffer and the number of read responses that have been returned.

Enhanced write process were discussed previously with respect to near memory in front and near memory behind . Here the operation of the far memory control logic and embodiments of specific components of the channel for effecting these write processes have already been discussed above. Notably however in addition with respect to the enhanced write process of the memory controller can determine from the cache read information whether a write to far memory is needed in the case of a cache miss and the dirty bit is set. In response the memory controller can increment its write buffer count as it understands the far memory control logic will automatically perform the write into far memory but will also automatically enter a request into the write buffer in order to do so. With respect to the enhanced write process of the memory controller can also receive the cache read information and operate as described just above.

Of course the far memory atomic operations described above can be utilized as appropriate over a channel that has only far memory technology e.g. a DDR channel only having DIMMs plugged into whose storage technology is only PCMS based .

The far memory control logic as described above can be implemented on one or more semiconductor chips. Likewise the logic circuitry for the memory controller can be implemented on one or more semiconductor chips.

Although much of the above discussion was directed to near memory system memory and far memory system memory devices that were located external to the CPU die and CPU package e.g. on DIMM cards that plug into a channel that emanates from the CPU package architecturally the above embodiments and processes could nevertheless also be implemented within a same CPU package e.g. where a channel is implemented with conductive traces on a substrate that DRAM and PCMS devices are mounted to along with the CPU die in a same CPU package far memory control logic could be designed into the CPU die or another die mounted to the substrate or even on the CPU die itself e.g. where besides logic circuitry to e.g. implement the CPU and memory controller the CPU die also has integrated thereon DRAM system memory and PCMS system memory and the channel is implemented with e.g. multi level on die interconnect wiring .

Training is an embedded configuration scheme by which communicatively coupled semiconductor devices can figure out what the appropriate signaling characteristics between them should be. In the case where only DRAM devices are coupled to a same memory channel the memory controller is trained to the read data provided by each rank of DRAM. The memory controller is also trained to provide properly timed write data to each rank. Training occurs on an 8 bit basis for 8 DRAMs and on a 4 bit basis for 4 DRAMs. Differences in trace lengths between 4 or 8 bit groups require this training resolution within the 4 or 8 bit group the traces are required to be matched . The host should do the adjustments because the DRAMs no not have adjustment capability. This saves both cost and power on the DRAMs.

When snarfing is to be done because PCMS and DRAM are coupled to a same channel the far memory controller must be trained also. For reads from near memory the far memory controller must be trained to accept the read data. If read data is to be snarfed by the DRAMs from the far memory controller the far memory controller must be trained to properly time data to the DRAMs which are not adjustable followed by the host being trained to receive the resulting data. In the case of the far memory controller snarfing write data a similar two step procedure would be used.

