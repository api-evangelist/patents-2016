---

title: Dialog system with automatic reactivation of speech acquiring mode
abstract: Embodiments of the disclosure generally relate to a dialog system allowing for automatically reactivating a speech acquiring mode after the dialog system delivers a response to a user request. The reactivation parameters, such as a delay, depend on a number of predetermined factors and conversation scenarios. The embodiments further provide for a method of operating of the dialog system. An exemplary method comprises the steps of: activating a speech acquiring mode, receiving a first input of a user, deactivating the speech acquiring mode, obtaining a first response associated with the first input, delivering the first response to the user, determining that a conversation mode is activated, and, based on the determination, automatically re-activating the speech acquiring mode within a first predetermined time period after delivery of the first response to the user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09570090&OS=09570090&RS=09570090
owner: Google Inc.
number: 09570090
owner_city: Mountain View
owner_country: US
publication_date: 20160601
---
This application is a Continuation in Part of U.S. patent application Ser. No. 14 721 012 entitled METHOD FOR USER COMMUNICATION WITH INFORMATION DIALOGUE SYSTEM filed on May 26 2015 which claims priority to Russian Federation Patent Application No. 2012150996 filed on Nov. 28 2012 U.S. patent application Ser. No. 14 721 044 entitled METHOD FOR USER TRAINING OF INFORMATION DIALOGUE SYSTEM filed on May 26 2015 which claims priority to Russian Federation Patent Application No. 2012150997 filed on Nov. 28 2012 U.S. patent application Ser. No. 14 775 729 entitled SELECTIVE SPEECH RECOGNITION FOR CHAT AND DIGITAL PERSONAL ASSISTANT SYSTEMS filed on Sep. 14 2015 U.S. patent application Ser. No. 14 901 026 entitled GENERATING DIALOG RECOMMENDATIONS FOR CHAT INFORMATION SYSTEMS BASED ON USER INTERACTION AND ENVIRONMENTAL DATA filed on Dec. 27 2015 which are incorporated herein by reference in their entireties for all purposes.

This disclosure relates generally to dialog systems conversational agents and chat information systems and more particularly to dialog systems having the feature of automatic reactivation of speech acquiring mode enabling a user in certain conversation scenarios to make an additional request after an answer to a user s previous request is delivered by the dialog system.

The approaches described in this section could be pursued but are not necessarily approaches that have been previously conceived or pursued. Therefore unless otherwise indicated herein the approaches described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

Today dialog systems are widely used in the information technology industry especially as mobile applications for wireless telephones and tablet computers. Generally a dialog system refers to a computer based agent having a human centric interface for accessing processing managing and delivering information. Dialog systems are also known as chat information systems spoken dialog systems conversational agents chatter robots chatterbots chatbots chat agents digital personal assistants and automated online assistants to name a few. All these terms are within the scope of the present disclosure and referred to as a dialog system for simplicity.

Traditionally a dialog system interacts with its users in natural language to simulate an intelligent conversation and provide personalized assistance to the users. For example a user may generate requests to the dialog system in the form of conversational questions such as Where is the nearest hotel or What is the weather like in Arlington and receive corresponding answers from the dialog system in the form of an audio and or displayable message. The users may also provide voice commands to the dialog system so as to perform certain functions including for example generating e mails making phone calls searching particular information acquiring data navigating providing notifications and reminders and so forth. Thus dialog systems are now very popular and are of great help especially for holders of portable electronic devices such as smart phones cellular phones tablet computers gaming consoles and the like.

Historically dialog systems and especially spoken dialog systems require a user to activate a dialog system before he makes a request. It is obvious that otherwise dialog systems acquire all spoken words made by the user which will lead to the dialog system overloading excessive use of computing resources inaccuracy of dialog system responses and so forth. Accordingly users need to manually activate dialog systems every time they want to make a request. This may be very annoying and inconvenient for the users. Additionally the need for manual reactivation of dialog systems does not allow the dialog systems to be user friendly and keep human like speech conversations which significantly limits their application.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

The present disclosure relates to embodiments of a dialog system and a method of its operation which overcome at least some drawbacks of prior art systems. In particular the embodiments provide for a speech acquiring mode for a dialog system that can be manually activated deactivated and automatically activated deactivated. The embodiments provide for a feature allowing for automatically reactivating the speech acquiring mode after the dialog system has delivered a response to a user request. The reactivation is intelligent in terms of its dependence from a number of predetermined criteria rules and or conversation scenarios. In some conversation scenarios the reactivation is performed with a delay which may depend on a number of factors. In other conversation scenarios the reactivation is intentionally suppressed based upon a type of a request or response delivered by the dialog system. It yet other conversation scenarios the reactivation can last for a predetermined period of time which may be also intelligently selected based on a number of factors. These and other embodiments are further summarized in the following aspects of the present disclosure. These aspects however are not intended to limit the scope of the subject matter.

According to an aspect of the embodiments of present disclosure a method is provided for operating of a dialog system. The method can be implemented by means of a computing device having one or more processors and a memory in association with the one or more processors. The method comprises the steps of activating a speech acquiring mode receiving a first input of a user deactivating the speech acquiring mode obtaining a first response associated with the first input delivering the first response to the user determining that a conversation mode is activated and based on the determination automatically re activating the speech acquiring mode within a first predetermined time period after delivering of the first response to the user.

In certain embodiments the first input of the user may comprise a speech based input. The first input of the user may also comprise a text based input. In certain embodiments the method may further comprise the steps of receiving a second input of the user obtaining a second response associated with the second input and delivering the second response to the user. Further the method may comprise the step of deactivating the speech acquiring mode after receiving of the second input of the user based on an instruction from a remote server or a type of response.

In certain embodiments the method may further comprise the steps of determining that a voice activation mode is deactivated and based on the determination that that the voice activation mode is deactivated the step of delivering the first response to the user may comprise displaying a displayable message associated with the first response to the user.

In yet more embodiments the conversation mode may enable the computing device to record the first input of the user. The conversation mode may also enable the computing device to recognize the first input of the user. The conversation mode may also enable the computing device to transmit the first input of the user to a remote server.

In certain embodiments the method may further comprise the steps of recognizing the first input of the user to generate a first recognized input and generating the first response to the recognized input. In certain embodiments the method may further comprise the step of selecting the first predetermined time period depending on a type of the first input of the user. In certain embodiments the method may further comprise the step of selecting the first predetermined time period depending on a type of the first response. In certain embodiments the method may further comprise the step of selecting the first predetermined time period depending on a length of the first response. The first predetermined time period can be fixed between 1 and 10 seconds.

In certain embodiments the method may further comprise the steps of setting an automatic recognition flag to the first response based on a type of the first response or a type of the first input and suppressing the speech acquiring mode based on the automatic recognition flag. The first response may comprise metadata which set the first predetermined time period. The speech acquiring mode can be re activated for a second predetermined time period. In yet more embodiments the first response may comprise metadata which may set the second time period. The step of activating of the speech acquiring mode may be performed in response to determining an activation of an actionable button by the user.

According to another aspect of the embodiments of the present disclosure a dialog system is provided. The dialog system may comprise a speech recognition module configured to receive a first user input recognize at least a part of the first user input and generate a first recognized input when the speech recognition module is in a speech acquiring mode. The dialog system may further comprise a dialog manager configured to activate and deactivate the speech acquiring mode and to generate a first response to the first recognized input. The dialog system may comprise a user output module configured to deliver the first response to a user. The dialog manager is further configured to automatically re activate the speech acquiring mode within a first predetermined time period after delivering of the first response to the user when a conversation mode is activated.

According to yet another aspect of the embodiments of the present disclosure a non transitory processor readable medium having instructions stored thereon is provided. The instructions when executed by one or more processors may cause the one or more processors to implement a method for operating of a dialog system. The method may comprise the steps of activating a speech acquiring mode receiving a first input of a user deactivating the speech acquiring mode obtaining a first response associated with the first input delivering the first response to the user determining that a conversation mode is activated and based on the determination automatically re activating the speech acquiring mode within a first predetermined time period after delivering of the first response to the user.

In further example embodiments and aspects modules systems subsystems or devices can be adapted to perform the recited method steps. Other features and exemplary embodiments are described below.

In this document the terms a or an are used as is common in patent documents to include one or more than one. In this document the term or is used to refer to a nonexclusive or such that A or B includes A but not B B but not A and A and B unless otherwise indicated.

The following detailed description includes references to the accompanying drawings which form a part of the detailed description. The drawings show illustrations in accordance with example embodiments. These example embodiments which are also referred to herein as examples are described in enough detail to enable those skilled in the art to practice the present subject matter. The embodiments can be combined other embodiments can be utilized or structural logical and electrical changes can be made without departing from the scope of what is claimed. The following detailed description is therefore not to be taken in a limiting sense and the scope is defined by the appended claims and their equivalents.

The techniques of the embodiments disclosed herein may be implemented using a variety of technologies. For example the methods described herein may be implemented in software executing on a computer system or in hardware utilizing either a combination of microprocessors or other specially designed application specific integrated circuits ASICs programmable logic devices or various combinations thereof. In particular the methods described herein may be implemented by a series of computer executable instructions residing on a storage medium such as a disk drive or computer readable medium. It should be noted that methods disclosed herein can be implemented by a computer e.g. a desktop computer tablet computer laptop computer game console handheld gaming device cellular phone smart phone smart television system and so forth.

As outlined above embodiments of the present disclosure provide for a dialog system having the feature for intelligent reactivating of a speech acquiring mode depending on a number of factors and conversation scenarios. These embodiments will now be described with reference to figures of the accompanying drawings.

As shown in the user may commence interaction with the exemplary spoken dialog system at operation with activating a speech acquiring mode. This mode activates the dialog system to receive user requests such as spoken or typed questions or commands. The speech acquiring mode is typically activated by pressing a dedicated button which is usually shown as a microphone image. Notably the speech acquiring mode is activated temporally e.g. specifically for the time period required to record a user oral request .

Accordingly at operation the user provides a speech based audio user input e.g. using one or more microphones . The user input may refer to an information request such as What is the weather like today or a speech command such as to send a text message or the like. Once the user completes his request or command the speech acquiring mode is deactivated and the dialog system may start its processing.

At operation an automatic speech recognizer ASR of the dialog system may recognize the user input so as to translate spoken words phrases and sentences into text which is referred herein to as a recognized input. The recognized input is then analyzed at operation by a natural language processing NLP module for example morphological analysis part of speech tagging or shallow parsing can be performed . The NLP module may also map the recognized input or its parts to one or more meaning representations e.g. semantic frame from which the dialog act user goal and named entities are extracted by a semantic parser or statistical model.

At operation a dialog manager generates an output response based at least in part on the recognized input and located meaning representations. For example the dialog manager may retrieve specific information from one or more resources e.g. electronic calendar database website remote or local resources remote or local services etc. . At operation the output is provided to the user. Generally the output can be delivered in the form of text image audio or video message. For these ends an output renderer may be utilized which may transform text into speech and provide the output as a machine generated audio signal. Alternatively the output may be presented as a text message and shown on a display of the user device.

The process illustrated by diagram can be performed cyclically. More specifically if the user wants to keep a conversation with the dialog system open and provide an additional request or clarify anything in response to the dialog system output the speech acquiring mode needs to be re activated so as to complete the operations . The embodiments of the present disclosure enable automatically re activating the speech acquiring mode depending on a number of factors. Among such factors are a type or parameter of first user request type or parameter of dialog system response and a conversation scenario to name a few. Notably the re activation can be delayed for a time period after the operation is completed with the time period being predetermined or intelligently selected based on the above factors or other criteria. Moreover the speech acquiring mode can be activated for another time period which again can be predetermined e.g. 1 to 10 seconds or intelligently selected based on the above factors or other criteria. Accordingly this feature enables the user to keep the conversation with the dialog system in a natural manner which is much less time consuming and annoying especially when the user needs to manually reactivate the speech acquiring mode or the dialog system itself.

The above given user interaction process is further illustrated in which shows a high level architecture of exemplary spoken dialog system according to an example embodiment. It should be noted that every module of the dialog system or associated architecture includes hardware components software components or a combination thereof. The dialog system may be embedded or installed in the user device or server or may be presented as a cloud computing module and or a distributed computing module.

In the embodiment shown the dialog system includes an ASR configured to receive and process speech based user inputs into a sequence of parameter vectors. The ASR further converts the sequence of parameter vectors into a recognized input i.e. a textual input having one or more words phrases or sentences . The ASR includes one or more speech recognizers such as a pattern based speech recognizer free dictation recognizer address book based recognizer dynamically created recognizer and so forth.

Further the dialog system includes a NLP module for understanding spoken language input. Specifically the NLP module may disassemble and parse the recognized input to produce utterances which are then analyzed utilizing for example morphological analysis part of speech tagging shallow parsing and the like and then map recognized input or its parts to meaning representations.

The dialog system further includes a dialog manager which coordinates the activity of all components controls dialog flows and communicates with external applications devices services or resources. The dialog manager may play many roles which include discourse analysis knowledge database query and system action prediction based on the discourse context. In some embodiments the dialog manager may contact one or more task managers not shown that may have knowledge of specific task domains. In some embodiments the dialog manager may communicate with various computing logic or storage resources which may include for example a content storage rules database recommendation database push notification database electronic address book email or text agents dialog history database disparate knowledge databases map database points of interest database geographical location determiner clock wireless network detector search engines social networking websites blogging websites news feeds services and many more. The dialog manager may employ multiple disparate approaches to generate outputs in response to recognized inputs. Some approaches include the use of statistical analysis machine learning algorithms e.g. neural networks heuristic analysis and so forth. The dialog manager is one of the central components of dialog system . The major role of the dialog manager is to select the correct system actions based on observed evidences and inferred dialog states from the results of NLP e.g. dialog act user goal and discourse history . In addition the dialog manager should be able to handle errors when the user input has ASR and NLP errors caused by noises or unexpected inputs.

The dialog system may further include an output renderer for transforming the output of the dialog manager into a form suitable for providing to the user. For example the output renderer may employ a text to speech engine or may contact a pre recorded audio database to generate an audio message corresponding to the output of the dialog manager . In certain embodiments the output renderer may present the output of the dialog manager as a text message an image or a video message for further displaying on a display screen of the user device.

As shown in the user device includes the following hardware components one or more processors memory one or more storage devices one or more input modules one or more output modules network interface and optional geo location determiner . The user device also includes the following software or virtual components an operating system dialog system rules database and user profile settings database . The dialog system provides a human centric interface for accessing and managing information as discussed herein.

The processor s is are in some embodiments configured to implement functionality and or process instructions for execution within the user device . For example the processor s may process instructions stored in memory and or instructions stored on storage devices . Such instructions may include components of an operating system and dialog system . The user device may also include one or more additional components not shown in such as a housing power supply and communication bus among others. These elements are omitted not to burden the description of the present embodiments.

Memory according to one example embodiment is configured to store information within the user device during operation. Memory in some example embodiments may refer to a non transitory computer readable storage medium or a computer readable storage device. In some examples memory is a temporary memory meaning that a primary purpose of memory may not be long term storage. Memory may also refer to a volatile memory meaning that memory does not maintain stored contents when memory is not receiving power. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples memory is used to store program instructions for execution by the processors . Memory in one example embodiment is used by software e.g. the operating system or dialog system executing on user device to temporarily store information during program execution. One or more storage devices can also include one or more transitory or non transitory computer readable storage media and or computer readable storage devices. In some embodiments storage devices may be configured to store greater amounts of information than memory . Storage devices may further be configured for long term storage of information. In some examples the storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs optical discs solid state discs flash memories forms of electrically programmable memories EPROM or electrically erasable and programmable memories and other forms of non volatile memories known in the art.

Still referencing the user device includes one or more input modules . The input modules are configured to receive user inputs. Examples of input modules include a microphone keyboard keypad mouse trackball touchscreen touchpad or any other device capable of detecting an input from a user or other source in the form of speech audio or tactile actions and relaying the input to the user device or components thereof. The output modules in some example embodiments are configured to provide output to users through visual or auditory channels. Output modules may include a video graphics adapter card liquid crystal display LCD monitor light emitting diode LED monitor sound card speaker or any other device capable of generating output that may be intelligible to a user.

The user device in certain example embodiments includes network interface . The network interface can be utilized to communicate with external devices servers and networked systems via one or more communications networks such as one or more wired wireless or optical networks including for example the Internet intranet local area network LAN wide area network WAN cellular phone networks e.g. Global System for Mobile GSM communications network packet switching communications network circuit switching communications network Bluetooth radio and an IEEE 802.11 based radio frequency network among others. The network interface may be a network interface card such as an Ethernet card optical transceiver radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G 4G and WiFi radios in mobile computing devices as well as Universal Serial Bus USB .

The user device may further include a geo location determiner for determining a current geographical location of the user device. The geo location determiner may utilize a number of different methods for determining geographical location including for example receiving and processing signals of Global Positioning Systems GPS GLONASS satellite navigation systems or the Galileo satellite navigation system utilizing multilateration of radio signals between radio towers base stations or utilizing geolocation methods associated with Internet Protocol IP addresses Media Access Control MAC addresses Radio Frequency Identification RFID or other technologies.

The operating system may control one or more functionalities of user device or components thereof. For example the operating system may interact with the dialog system rules database and user profile settings database and may further facilitate one or more interactions between elements and one or more of processors memory storage devices input modules and output modules . As shown in the operating system may interact with or be otherwise coupled to the dialog system and components thereof. In some embodiments the dialog system can be included into the operating system . Notably the user device and its components such as the dialog system may also interact with one or more remote storage or computing resources including for example web resources web sites social networking websites blogging websites news feeds email servers web calendars event databases ticket aggregators map databases points of interest databases and so forth.

In certain embodiments the rules database which can be optional in many designs includes one or more of the following rules program codes instructions parameters specifications and other data to enable the dialog system to operate in a designed way. The user profile settings database which can also be optional in many designs includes user profiles personalized settings user preferences user data and so forth. In other words the dialog system may operate relying on instructions stored in the rules database and or user preferences set in the user profile settings database .

In this example embodiment the processing of user requests is performed solely using hardware and software virtual components of the user device itself. The user device may however communicate with the web resources and or web services in order to obtain requested information or perform a requested action.

In this example the user device is configured to receive or capture user audio inputs e.g. via one or more microphones or tactile based inputs and deliver them to the dialog system via a communications network . Once the user input is processed and a response is generated it is delivered from the dialog system to the user device via the communications network . Once the response is received the user device simply plays back or displays it.

The web resources and or web services shown in refer one or more of the following webpages websites Internet search engines databases data storage data aggregators e.g. ticket aggregators file sharing websites e mail servers e mail services instant messaging services social networking websites blogging websites micro blogging websites news services news Rich Site Summaries RSS map services online banking services online calendar services online appointment scheduling services location determining services weather services time services Application Programming Interfaces APIs and so forth. Notably the dialog system and its components are configured to operatively communicate with these or similar web resources and or web services to exchange data depending on a particular application and user request. For example the dialog system may communicate with the at least one web resource and or web service so as to generate responses to user inputs e.g. to retrieve download weather information . In another example the dialog system may communicate with the at least one web resource and or web service so as to identify triggering events e.g. identifying a new status of the user s friend via at least one social networking site . In yet another embodiment the recommendation manager may communicate with the at least one web resource and or web service so as to generate one or more dialog recommendations for the user e.g. communicating with a map service to identify points of interests located near a current location of the user or provide navigation .

Generally the user interface is the first screen displayable when the user starts the operation of the dialog system . In the example embodiment shown the user interface includes a container may also refer to a window widget or output element where an avatar is displayed. Optionally the user can select an avatar of his preference or construct it from disparate elements e.g. select clothes eyes hair etc. .

The user interface also includes a number of actionable clickable buttons including a microphone button a settings button and a speaker button . The microphone button when pressed by a user manually activates a speech acquiring mode of a dialog system. In this mode the user device records user speech inputs or allows the user to make an input using a keypad. Typically the mode is active as long as the user makes an input and once the input is completed the speech acquiring mode can be automatically or manually deactivated. After its deactivation the dialog system starts processing the recorded input to generate a corresponding answer. That said if the user wants to make a request to the dialog system he needs to press the microphone button provide an input and wait for a response.

In certain embodiments a first press on the microphone button activates the speech acquiring mode while a secondary press on the microphone button deactivates the speech acquiring mode and or cancels just made user input. In some embodiments there may be also provided a cancellation button not shown enabling the user to stop or cancel a current action mode performed by the dialog system .

Still referencing the setting button when pressed by the user brings a settings user interface shown in . The setting interface enables the user to adjust parameters of the dialog system establish or edit his profile create or edit specific rules and criteria and so forth. In one example the user may activate or deactivate a conversation mode. Generally the conversation mode enables the dialog system to keep conversations with the user by automatically re activating the speech acquiring mode in certain time instances.

Further the speaker button controls the method of delivering dialog system responses to the user. Namely this button enables a user to select whether a responses are provided as audio messages through a speaker or as displayable messages only.

Notably the graphical user interface may include other containers widgets icons or buttons such as a button to manually activate or deactivate the conversation mode. Moreover in certain embodiments the user may also activate or interact with the user interface by actuating traditional buttons present on the user device e.g. a main menu button or a key pad . In yet other embodiments the user may activate or interact with the user interface by giving a gesture based command.

In particular the user interface includes the container with the avatar clickable buttons i.e. the microphone button settings button and speaker button and also a container . The container is used for displaying messages i.e. text image or video based messages generated by the dialog system . As shown in the container may display a user input in the form of a text message after it was input by the user using a keyboard or after it was spoken and recognized by the dialog system . Further the container may show a response of the dialog system to a particular user input such as the user input . Similarly the response can be in the text message. Notably the container may display multiple user inputs and multiple dialog system responses one after another to reflect a conversation of the user with the dialog system .

In the example shown there are the following control buttons or radio buttons Conversation Mode radio button Voice Activation radio button Voice Settings clickable button and Customization clickable button . More specifically Conversation Mode radio button enables the user to manually activate or deactivate the conversation mode. When this mode is activated the dialog system keeps the dialog with the user by automatically re activating the speech acquiring mode after a response is delivered to the user and when certain predetermined criteria are met. Voice Activation radio button enables the user to activate or deactivate the mode of voice activation i.e. wake up of the dialog system. For example the dialog system may be activated i.e. woken up after the user says a phrase such as Hay Assistant Voice Settings clickable button enables the user to set characteristics for presenting audio message e.g. to select a voice . Finally Customization clickable button enables the user to customize his profile specific rules or parameters of the dialog system and so forth. Some or all of the settings can be stored in the user profile settings database for further reference by the dialog system .

This section of the document provides multiple examples of operation methods. First with reference to there are described generalized operation methods of dialog system and further with reference to there are provided more specific examples i.e. conversation scenarios .

As shown in the figure the process commences at the decision block when the dialog system determines whether or not a voice activation mode is activated. The step can be performed each time when the dialog system is started or after the dialog system just delivered a response to a user request. If it is determined that the voice activation mode is not activated by the user the method proceeds to the step which instructs the dialog system that a speech acquiring mode shall not be automatically activated after a response is delivered to the user. Otherwise if it is determined that the voice activation mode is already activated by the user in settings the method proceeds to the step which instructs the dialog system to determine whether or not the conversation mode is activated.

If it is determined that the conversation mode is not activated by the user the method proceeds to the step which instructs the dialog system to determine whether or not a previously delivered response includes metadata with an automatic recognition flag with a true value. If it is determined that the automatic recognition flag does not have a true value i.e. it has false value the method proceeds to the step which instructs the dialog system that the speech acquiring mode shall not be automatically activated after a response is delivered to the user. Otherwise if it is determined that the automatic recognition flag has the true value the method proceeds to the step which instructs the dialog system that the speech acquiring mode shall be automatically re activated after a response is delivered to the user.

If in the step it is determined that the conversation mode was activated by the user the method proceeds to the step which instructs the dialog system to determine whether or not a previously delivered response includes metadata with a automatic recognition flag having a false value. If it is determined that the automatic recognition flag in the metadata indeed has the false value the method proceeds to the step which instructs the dialog system that the speech acquiring mode shall not be automatically activated after a response is delivered to the user. Otherwise if it is determined that the automatic recognition flag does not have the false value i.e. it has true value the method proceeds to the step which instructs the dialog system to determine whether or not a previously delivered response has a suppression instruction for example it is determined if a previously delivered response includes a dedicated instruction to suppress the speech acquiring mode . Accordingly if it is determined that a previously delivered response included a suppression instruction the method proceeds to the step which instructs the dialog system that the speech acquiring mode shall not be automatically activated after a response is delivered to the user. Otherwise if it is determined that a previously delivered response did not include a suppression instruction the method proceeds to the step which instructs the dialog system that the speech acquiring mode shall be automatically re activated after a response is delivered to the user.

The method starts at operation with activating a speech acquiring mode of the dialog system . This can be done manually by pressing the microphone button or when the dialog system is started e.g. by the a user tap or by an activation word or an activation phrase a user opening push message a reminder a notification and so forth. In an example embodiment a listed upon startup option may be active for a microphone according to which the microphone may be activated upon the start wake up of the dialog system . Once the speech acquiring mode is activated the user is invited to provide an oral request. Accordingly at operation the dialog system receives a first input of the user and the speech acquiring mode is deactivated. In some embodiments while the speech acquiring mode is active the dialog system or the user device records the user input but after the speech acquiring mode is deactivated the recorded user input is processed by the dialog system .

Still referencing at operation the dialog system processes the first input and then obtains and delivers a first response to the first input. In one example embodiment the delivery of the first response can include presenting an audio message to a user via a speaker or headphones of the user device . In another example embodiment the delivery of the first response can include presenting a displayable message on a display of the user device and the message can include text image video or any other displayable content. In yet more embodiments the response can be delivered as both an audio message and displayable message.

Further at operation the dialog system determines that a conversation mode is activated. As mentioned above the conversation mode can be enabled by the user in the settings user interface . At operation based on the determination the dialog system automatically re activates the speech acquiring mode within a first predetermined time period after delivering of the first response to the user. In an example embodiment the dialog system may determine whether the mode when the dialog system responses are presented in the form of audio messages in addition to displayable messages is activated mute off mode or deactivated mute on mode . In the mute off mode the activation of the speech acquiring mode is done immediately after the dialog system finishes speaking unless there is an instruction to do otherwise . In the mute on mode a plurality of predetermined rules may be used to determine how much time is needed for the user to read understand the first response e.g. if graphics are presented in the first response .

In one example embodiment the first predetermined time period is fixed. For example the first predetermined time period may be anywhere between 0.1 second to 20 seconds but more preferably it is between 1 second and 10 seconds. In one instance it can be 2 seconds but in another instance it can be 5 seconds depending on other factors.

In another embodiment the first predetermined time period is dynamically selected or determined based on predetermined criteria or rules. For example the first predetermined time period may depend on a type of first user input. In another example the first predetermined time period may depend on a type of first response delivered to the user e.g. when a response relates to reading of an email the first predetermined time period may be longer in contrast than when a response relates to reading of current weather parameters . In yet another example the first predetermined time period may depend on a length of the first response delivered to the user. The length may be determined by a number of words or letters in the response although other complexity criteria can be employed. For example if the response is less than 10 words long the first predetermined time period can be just 1 second if the response is between 10 and 20 words the first predetermined time period is 3 seconds if the response is between 20 and 30 words the first predetermined time period is 5 seconds and so forth. It should be clear that the first predetermined time period may depend on a number of factors and predetermined rules pre selected for certain conversation scenarios. In yet more embodiments the first predetermined time period can be set by metadata associated with the response.

Notably the speech acquiring mode is re activated for a second predetermined time period when the dialog system awaits and records the next user input. Similarly to above the second predetermined time period may depend on a number of factors. For example in one example embodiment the second predetermined time period is fixed. It can be anywhere between 1 second to 20 seconds but more preferably it is between 1 second and 10 seconds. In some examples the second predetermined time period can be set by metadata associated with the response. In yet another example embodiment the second predetermined time period is dynamically selected or determined based on disparate predetermined criteria or rules. For example the second predetermined time period may depend on a type of first user input. In another example the second predetermined time period may depend on a type of first response delivered to the user. In another example the second predetermined time period may depend on a length of the first response delivered to the user. The length may be determined by a number of words or letters in the response although other complexity criteria can be employed. It should be clear that the second predetermined time period may depend on a number of factors and predetermined rules pre selected for certain conversation scenarios.

Still referencing to at operation the dialog system receives a second input of the user and deactivates the speech acquiring mode which was activated at the operation . Similarly to above at operation the dialog system obtains and delivers a second response associated with the second input of the user.

Accordingly the operations of method may be cyclically repeated over and over again so that the user may experience a natural language conversation with the dialog system . The use of first and second predetermined time periods allows the dialog system to intelligently expect when a next user request will be made to provide the most natural dialog interaction experience to the user and without excessive consumption of computational and power resources.

As already outlined above in certain embodiments the response of the dialog system may include metadata which may include additional instructions for example in the form of the automatic recognition flag. This flag can be used to initiate recognition if the flag true or forcefully suppress the re activation of the speech acquiring mode after the response is delivered to the user. In other embodiments the metadata may set the first and or second predetermined time periods equal to zero. In this case the user will not be able to provide an input without manual reactivation of the speech acquiring mode.

In some additional embodiments metadata may also include additional instructions setting emotional characteristics to a corresponding response to be delivered to the user. The emotional characteristics may relate to the appearance of the avatar and or tone voice intonation volume or other parameters of the audio message associated with the dialog system response.

In other embodiments metadata may also include instructions to certain hardware or software resources of the user device . For example there may be instructions to acquire current geographical location of the user store or retrieve certain data to the user profile and so forth.

This exemplary conversation scenario illustrates automatic activation of the speech acquiring mode after a response is delivered to a user. In this scenario the voice activation mode is activated and a conversation mode is also activated e.g. through the settings user interface .

At step a user presses the microphone button to activate the speech acquiring mode. At step the user provides an input and the dialog system acquires recognizes and processes the user input. At step the dialog system generates and delivers a response to the user. At step the dialog system automatically re activates the speech acquiring mode within a first predetermined time period and for a second predetermined time period.

This exemplary conversation scenario illustrates temporary suppressing of the conversation mode upon input using a keyboard. In this scenario the voice activation mode is activated and a conversation mode is also activated e.g. through the settings user interface .

At step a user makes a first input to the dialog system using a keyboard or touchscreen. At step the dialog system acquires and processes the first input. At step the dialog system generates and delivers a response to the first input in the form of an audio and or text message. Further the user may make a second input. If the second input is in a text form e.g. when the user uses the keyboard the dialog system does not activate the speech acquiring mode. Otherwise if the second input is in a speech form the dialog system activates the speech acquiring mode and further processing is performed in accordance with Scenario 1.

This exemplary conversation scenario illustrates suppression of the speech acquiring mode based on a dialog system response. In this scenario the voice activation mode is activated and a conversation mode is also activated e.g. through the settings user interface .

At step a user presses the microphone button to activate the speech acquiring mode. At step the user provides an input and the dialog system acquires recognizes and processes the user input. At step the dialog system generates and delivers a response to the user. The response includes metadata having the automatic recognition flag with the false value. At step the dialog system does not automatically re activate the speech acquiring mode as prescribed by the metadata.

This exemplary conversation scenario illustrates automatic activation of the speech acquiring mode based on a dialog system response. In this scenario the voice activation mode is activated and a conversation mode is not activated e.g. through the settings user interface .

At step a user presses the microphone button to activate the speech acquiring mode. At step the user provides an input and the dialog system acquires recognizes and processes the user input. At step the dialog system generates and delivers a response to the user. The response includes metadata having the automatic recognition flag with the true value. At step the dialog system automatically re activates the speech acquiring mode within a first predetermined time period and for a second predetermined time period.

This exemplary conversation scenario illustrates operation of the dialog system when the speech acquiring mode is not activated due to suppression based on a suppression instruction. Accordingly in this scenario the voice activation mode is not activated and a conversation mode is also not activated e.g. through the settings user interface .

At step a user provides an input using a keyboard or touchscreen. At step the dialog system acquires and processes the user input. At step the dialog system generates and delivers a response to the user in the form of displayable message only for example opening a webpage launching an application initiating media playback and so forth. At step the dialog system does not automatically re activate the speech acquiring mode.

Thus the dialog system and method of its operation have been described. Although embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes can be made to these example embodiments without departing from the broader spirit and scope of the present application. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

