---

title: Systems and methods for performing data replication
abstract: Performing data management operations on replicated data in a computer network. Log entries are generated for data management operations of an application executing on a source system. Consistency point entries are used to indicate a time of a known good, or recoverable, state of the application. A destination system is configured to process a copy of the log and consistency point entries to replicate data in a replication volume, the replicated data being a copy of the application data on the source system. When the replicated data represents a known good state of the application, as determined by the consistency point entries, the destination system(s) may perform a storage operation (e.g., snapshot, backup) to copy the replicated data and to logically associate the copied data with a time information (e.g., time stamp) indicative of the source system time when the application was in the known good state.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09639294&OS=09639294&RS=09639294
owner: Commvault Systems, Inc.
number: 09639294
owner_city: Tinton Falls
owner_country: US
publication_date: 20160302
---
Any and all applications for which a foreign or domestic priority claim is identified in the Application Data Sheet or any correction thereto are hereby incorporated by reference under 37 CFR 1.57.

The present application is also related to the following applications filed on even date herewith each of which is hereby incorporated herein by reference in its entirety 

The present disclosure relates to performing copy and or data management operations in a computer network and in particular to systems and methods for performing data replication in a storage management system.

Computers have become an integral part of business operations such that many banks insurance companies brokerage firms financial service providers and a variety of other businesses rely on computer networks to store manipulate and display information that is constantly subject to change. Oftentimes the success or failure of an important transaction may turn on the availability of information that is both accurate and current. Accordingly businesses worldwide recognize the commercial value of their data and seek reliable cost effective ways to protect the information stored on their computer networks.

Many approaches to protecting data involve creating a copy of the data such as backing up and or replicating data on one or more storage devices. When creating a copy of such data certain factors are generally considered. First a copy of data should not contain data files that are corrupt or terminated improperly. Second a copy of data should be current enough to avoid data staleness by avoiding too much time between copying such that the copied data is still useful should it be needed. For certain applications such as networks that store financial transactions copies a week old may be useless and much more frequent copying may be needed.

In an attempt to accommodate such storage requirements certain systems through all the files in a computer network or through a selected set of critical files and check the time information of each file. If data has been written to the file since the last time the system checked the file s status then a copy of the file is sent to a storage system. One problem with such systems is that they typically do not work for data kept in very large files. For example assuming that a copy could be made of the very large database the time needed to make copies of such a large database may render data shadowing impractical. Making numerous copies of a large database not only takes a tremendous amount of time but also requires a tremendous amount of storage space.

Another approach that has been attempted in order to overcome some of these limitations is a process whereby a time sequence of data is captured and saved. For example many systems incorporate disk mirroring or duplexing. In disk mirroring or duplexing changes made to a primary mass storage system are sent to other backup or secondary mass storage systems. In other words when a data block is written to the primary mass storage system the same data block is written to a separate secondary mass storage system. By copying each write operation to a second mass storage system two mass storage systems may be kept synchronized so that they are virtually identical at approximately the same time. Because an entire disk volume is being copied however mirroring also requires a tremendous amount of storage space and utilizes a large amount of processing resources.

Furthermore each of the above described processes for copying or backing up data can have a significant impact on the source or primary system. For example processing resources of the source system may be expended in copying data to a destination system rather than being used to process application requests.

In view of the foregoing a need exists for improved systems and methods for the copying and or replication of data in computing systems. In particular a need exists for systems and methods that reduce the impact e.g. processing load on a source or primary system when performing one or more data management and or storage operations on data such as for example application specific data.

In certain embodiments of the invention systems and methods are disclosed for performing substantially continuous replication of application specific data in a computer network environment. In certain embodiments systems and methods may further perform one or more data management operations on the replicated data in order to avoid burdening the source system. For example one or more storage operations may be performed on replicated data that represents a recoverable state or known good state of a particular application running on the source system.

For instance in certain embodiments known good replication copies may be viewed as copies of production volume data. This feature allows a management component in the computing system to directly access copy restore backup or otherwise manipulate the replication copies of production data as if the data was the production data of the source system thereby improving various system performance characteristics such as access time reducing memory requirements and reducing impact on source or client applications.

In certain embodiments the replication copies of the production data include time information such as one or more time stamps that indicates the client system time when the production data was modified and or when the subject application was in a known good state. Such time stamps are then associated with the replication data and or copies of the replicated data thereby allowing for the synchronization of time references between the production data and copies of the replicated data.

Accordingly certain embodiments of the invention may recognize points within application data that represent a known good state of the application. This information is then used by the replication system to intelligently replicate sections of application data that represent a recoverable state rather than the rote copying of certain blocks of data based on hardware capacity and or criteria that are unrelated to application recoverability. Thus in certain embodiments one benefit of the systems and methods provided herein is the ability to replicate data on an application specific basis rather than merely copying certain physical blocks of information based on buffer size file type or copying other uncoordinated groups of data.

In certain embodiments a method is disclosed for performing data management operations in a computer network. The method comprises monitoring operations associated with an application that are operative to write data to a first storage device inserting in a log file a marker comprising time information identifying a time of a known good state of the application copying the data to a second storage device based at least in part on the operations generating a snapshot of the copied data at a time after the data has been copied to the second storage device and associating the snapshot of the copied data with the time information identifying the time of the known good state of the application.

In certain further embodiments the above disclosed method additionally comprises performing a backup or other storage operation on the snapshot and associating the resulting copy of the snapshot with the time information identifying the time of the known good state of the application.

In certain embodiments a system is disclosed for performing data management operations in a computer network environment. The system comprises at least one computer application configured to execute on a source computer and a first storage device coupled to the source computer to receive data write operations from the at least one computer application. The system further comprises a second storage device and at least one module configured to monitor the data write operations and to generate log entries based on the data write operations at least one of the log entries having a time stamp indicative of a time of a known good state of the at least one computer application. The system also comprises a replication module coupled to the second storage device wherein the replication module is configured to process based on the log entries the data write operations to replicate data to a first location on the second storage device perform a storage operation on the replicated data at the first location on the second storage device to copy data to a second location different than the first location and logically associate the copied data at the second location with the time stamp indicative of the time of the known good state of the at least one computer application.

In certain embodiments a method is disclosed for performing data management operations in a computer network. The method comprises monitoring data operations associated with an application the data operations operative to write data to a first storage device and populating a first log file with data entries indicative of the data operations. The method also comprises inserting in the first log file a marker indicative of a known good state of the application the marker including a time information identifying a time of the known good state of the application replaying to a second storage device the data operations based on the data entries to replicate data to a first location on the second storage device performing a storage operation on the replicated data to copy the replicated data from the first location to a second location and associating the copy of the replicated data at the second location with the time information identifying the time of the known good state of the application.

In certain embodiments a system is disclosed for copying data in a computer network. The system comprises means for monitoring data operations generated by a single computer application that are operative to write data to a first storage device means for storing data entries indicative of the data operations means for inserting in the storing means a marker indicative of a known good state of the computer application the marker including a time stamp associated with a source system time of the known good state of the computer application means for processing based on the data entries in a second storage device the data operations to replicate data to a first location on the second storage device means for performing a storage operation on the replicated data in the second storage device to copy the replicated data to a second location and means for associating the copy of the replicated data with the time stamp associated with the source system time of the known good state of the computer application.

In certain embodiments a method is disclosed for monitoring a computer application to perform data replication in a computer network. The method comprises detecting data operations sent from a computer application to a file system on a source computer the data operations operative to write data to a first storage device populating a first log file with data entries indicative of the data operations detecting a known good state of the computer application quiescing at the known good state of the computer application the sending of the data operations to the file system and inserting in the first log file a marker including time information identifying a time of the known good state of the computer application.

In certain further embodiments quiescing of the above disclosed method is performed at periodic intervals. For instance the method may further include receiving user input for selecting the periodic intervals and or the periodic intervals may be selected automatically. For instance a frequency of the periodic intervals may be based at least in part on a type of data associated with the computer application an average failure rate of at least one computer in the computer network a load of at least one computer in the computer network an availability of at least one computer in the computer network combinations of the same or the like.

In certain embodiments a system is disclosed for generating application data for use in a data replication environment. The system comprises a monitor module configured to monitor data write operations of a computer application and to generate first log entries based on the data write operations the monitor module being further configured to quiesce or buffer additional data write operations at a known good state of the computer application and to generate a second log entry having a time stamp indicative of a time of the known good state of the computer application. The system also comprises at least one log file in communication with the monitor module and configured to store the first and second log entries.

In certain embodiments a system is disclosed for generating application data for use in a data replication environment. The system comprises means for detecting data operations sent from a computer application to a file system the data operations operative to modify data stored on a first storage device means for storing data entries indicative of the data operations means for quiescing at a known good state of the computer application the sending of additional data operations to the file system and means for recording a time stamp identifying a time of the known good state of the computer application.

In certain embodiments a method is disclosed for copying data generated on a source system in a computer network. The method comprises processing with one or more routines at least one log file having a plurality of log entries indicative of operations generated by a computer application executing on a source system the operations being directed to data on a source storage device replaying with the one or more routines the operations on a destination storage device to replicate application specific data to the destination storage device suspending the replaying when the one or more routines encounters a consistency point marker in the at least one log file the consistency point marker being indicative of a known good state of the computer application and performing a storage operation on the replicated data when the replicated data represents the known good state of the computer application.

In certain embodiments a destination system is disclosed for performing data replication in a computer network. The destination system comprises at least one replication log file and a replication module. The at least one replication log file further comprises i a plurality of log entries indicative of data operations generated by a computer application for execution on a source storage device and ii at least one consistency point marker indicative of a known good state of the computer application. The replication module is configured to replicate data to a destination storage device and further comprises a replication agent and at least one process configured to traverse the plurality of log entries in the at least one replication log file and to copy the log entries to execute the data operations on the destination storage device the at least one thread being further configured to notify the replication agent when encountering the at least one consistency point marker.

In certain embodiments a system is disclosed for replicating data generated on a source device in a computer network. The system comprises means for storing a plurality of log entries indicative of modification operations generated by a computer application executing on a source system the modification operations being directed to data on a source storage device means for traversing the storing means and for replaying the modification operations to replicate application specific data to a destination storage device means for suspending the replaying when the traversing and replaying means encounters a consistency point marker in the means for storing the consistency point marker being indicative of a known good state of the computer application and means for performing a storage operation on the replicated data when the replicated data represents the known good state of the computer application.

In certain embodiments a method is disclosed for handling data to be copied in a computer network. The method comprises monitoring operations associated with a single application executing on a source system identifying from the operations a plurality of data modification operations operative to write data to a first storage device buffering a copy of each of the plurality of data modification operations forwarding the copies of the plurality of data modification operations to a destination system to be copied to a second storage device without first writing the copies of the plurality of data modification operations to the first storage device and forwarding the plurality of data modification operations to a file system associated with the first storage device.

In certain embodiments a system is disclosed for selecting application data to be copied in a computer network. The system comprises a buffer at least one computer application configured to generate operations associated with data on a first storage device and a filter module disposed between the at least one computer application and the first storage device the filter module configured to identify from the operations a plurality of data modification operations. The system further comprises a network redirector component in communication with the filter module and configured to temporarily store a copy of the data modification operations in the buffer while allowing the data modification operations to pass through to the first storage device the network redirector component being further configured to transmit the copies of the plurality of data modification operations to a destination system to be copied to a second storage device without first writing the copies of the plurality of data modification operations to the first storage device.

In certain embodiments a system is disclosed for acquiring data for replication on a network storage device. The system comprises means for generating operations associated with data on a first storage device and means for identifying from the operations a plurality of data modification operations the means for identifying being disposed between the means for generating and the first storage device. The system further comprises means for temporarily storing a copy of the data modification operations while allowing the data modification operations to pass through to the first storage device the means for storing being further configured to transmit the copies of the plurality of data modification operations to a destination system to be replicated to a second storage device without writing the copies of the plurality of data modification operations to the first storage device.

In certain embodiments a method is disclosed for performing pathname translation in a data replication system. The method comprises receiving a log entry to be replicated on a destination system that identifies a data management operation and an inode associated with a source system location corresponding to the data management operation the inode being one of a plurality of inodes on the source system accessing a database comprising path data associating each of the plurality of inodes with a short name and a parent inode constructing from the path data an absolute pathname on the destination system that corresponds to the inode of the log entry and forwarding the log entry and the absolute pathname to the destination system.

In certain embodiments a system is disclosed for performing pathname translation during data replication in a computer network. The system comprises a database at least one log entry and a database process. In certain embodiments the database comprises path data that associates each of a plurality of inodes on a source system with a short name and a parent inode. The at least one log entry identifies a data management operation and at least one corresponding inode of the plurality of inodes. The database process is configured to receive the at least one log entry and to access the database to translate the at least one corresponding inode to a pathname identifying a location on a destination system corresponding to the data management operation.

In certain embodiments a system is disclosed for performing translation from an inode to a pathname in a data storage network. The system comprises means for storing path data associating each of a plurality of inodes on a source system with a short name and a parent inode means for identifying data management operations and at least one of the plurality of inodes that corresponds to each data management operation and means for accessing the storing means to translate the at least one inode to a pathname identifying a location on a destination system corresponding to each data management operation.

In certain embodiments a method is disclosed for transmitting data from a source computer to a destination computer in a data replication system. The method comprises monitoring file system requests of an application on a source computer identifying from the file system requests data management operations directed to data on a first storage device and storing in a buffer a plurality of log entries representing the data management operations. The method further comprises sequentially processing each of the plurality of log entries transmitting each processed log entry to a destination computer replaying each processed log entry to replicate the data on the first storage device to a second storage device and when said sequentially processing drops below a predetermined rate disk swapping most recently received log entries from the buffer to a memory.

In certain embodiments a system is disclosed for transmitting data to be replicated in a computer network. The system comprises a queue a buffer and a monitor module configured to monitor file system requests generated by a computer application the monitor module being further configured to populate the queue with log entries indicative of file system requests comprising data management operations to be executed on a first storage device. The system further comprises a first thread configured to transfer the log entries from the queue to the buffer and a second thread configured to retrieve the log entries from the buffer process the log entries and forward the processed log entries to a destination system for replaying the log entries to replicate data on a second storage device. In certain embodiments the monitor module is further configured to throttle the computer application when a rate at which the second thread processes the log entries is below a predetermined rate.

In certain embodiments a system is disclosed for transmitting data to be replicated in a network environment. The system comprises means for monitoring file system requests of an application on a source computer and for identifying from the file system requests data management operations directed to data on a first storage device means for storing a plurality of log entries representing the data management operations means for sequentially processing each of the plurality of log entries means for transmitting each processed log entry to a destination computer means for replaying each processed log entry to replicate the data on the first storage device to a second storage device and means for disk swapping most recently received log entries from the storing means to a memory when said sequentially processing falls below a predetermined rate.

In certain embodiments a system is disclosed for facilitating data synchronization following a network failure in a data replication environment. The system comprises a plurality of log entries representing data management operations generated by an application for data on a first storage device each of the plurality of log entries including a unique identifier a cache memory configured to store a portion of the plurality of log entries such that when a size of the portion of log entries exceeds a storage threshold of the cache memory the cache memory deletes one or more of the least recently received log entries until the size is less than the storage threshold at least one destination computer configured to process the plurality of log entries to replicate data to a second storage device the at least one destination computer being further configured to record the unique identifier of a most recently processed log entry and at least one replication routine configured to sequentially retrieve each of the plurality of log entries from the cache memory and to forward each log entry to the at least one destination computer without waiting for an acknowledgement from the at least one destination computer.

In certain embodiments a method is disclosed for performing data replication in a computer network. The method comprises storing log entries in a first in first out memory wherein each of the log entries includes a unique identification and represents a data management operation generated by an application for data on a first storage device maintaining in the first in first out memory a history of the log entries stored therein such that the storage amount occupied by the history of the log entries is less than a storage threshold transmitting a copy of each log entry from the first in first out memory to at least one destination computer replaying the copies of the log entries received by the at least one destination computer to replicate the data management operations on data on a second storage device and storing on the at least one destination computer the unique identification of the most recently replayed log entry.

In certain embodiments a system is disclosed for facilitating data synchronization following a network failure in a data replication environment. The system comprises means for representing data management operations generated by an application for data on a first storage device each of the means for representing including a unique identifier means for storing a portion of the means for representing such that when a size of the portion of the means for representing reaches a storage threshold of the means for storing the means for storing deletes one or more of the oldest means for representing until the size is less than the storage threshold means for processing the means for representing to replicate data to a second storage device the means for processing being further configured to record the unique identifier of a most recently processed means for representing and means for sequentially retrieving log entries from the means for storing the means for retrieving being further configured to forward each means for representing to the means for processing without waiting for an acknowledgement from the means for processing.

In certain embodiments a system is disclosed for identifying and replicating software application data representative of a known good state. The system comprises a data agent associated with a software application that identifies data to be replicated indicative of the known good state wherein the data representative of the known good state is indicated by a consistency point or marker in the data. The system further includes a replication module that copies the data representative of the known good state to a storage device based on the consistency point.

According to certain embodiments a system and method are provided for creating consistency points in a replication volume whereby the replication volume may be created by application specific replication processes. In other words the user may establish a storage policy that identifies storage operations that may be performed using different types of replication copies. In some instances it may not be advantageous to replicate an entire volume. For example viewable data may be the only data that is replicated e.g. SQL data . Therefore an application specific storage policy associated with replication may be created in combination with journal e.g. all file system logs logging operations for the purposes of generating application specific replication.

For purposes of summarizing the disclosure certain aspects advantages and novel features of the inventions have been described herein. It is to be understood that not necessarily all such advantages may be achieved in accordance with any particular embodiment of the invention. Thus the invention may be embodied or carried out in a manner that achieves or optimizes one advantage or group of advantages as taught herein without necessarily achieving other advantages as may be taught or suggested herein.

As will be seen from the disclosure herein certain embodiments of systems and methods are provided for intelligent data replication. In particular embodiments of the invention include the replication of application specific data from a source system to a destination system. For example one or more modules executing on the source system may monitor data management operations such as data modification operations of a specific application and generate log entries indicative of the operations. The log entries may then be copied to and processed or replayed by a destination system to replicate data in one or more replication volumes the replicated data being a copy of the application data stored on a source storage device.

In certain embodiments the replication systems and methods disclosed herein are further configured to identify a known good or recoverable state of the application. For instance the replication systems and methods may determine appropriate points within certain application data at which certain information is collected to reliably restore the application as of a particular point in time. At certain known good states the replication system quiesces the application and or generates a consistency point marker having time information such as for example a time stamp indicating the source system time of the application s known good state.

When replicating the data the destination system utilizes the consistency point markers to identify when the replicated data represents the known good state of the application. The destination system is further capable of performing one or more data management operations such as for example storage operations e.g. snapshot backup search operations data classification combinations of the same or the like on the replicated data at certain consistency points. Performing data management operations on the replicated data allows for the processing of copies of application data without significantly impacting the resources of the source system. Furthermore when copying the replicated data at consistency points the copied data presumably represents a known good state of the application.

Using the time information of one or more consistency point entries the destination system is further capable of logically associating the copied data e.g. a snapshot of the replicated data with the source system time of the known good state of the application. That is even though the copy of the replicated data occurs at some point after the replication of the production source system data the copied data is associated with the earlier source system time of the consistency point entry e.g. the source system time of the known good state of the application. As a result the copy of the replicated data which is logically associated with the source system time of the known good state of the application appears as if the copy was directly performed on the production source system data. In certain embodiments this method advantageously allows for further processing of the copied data e.g. processing of snapshots of the replicated data without touching the data of the source system.

The features of the systems and methods will now be described with reference to the drawings summarized above. Throughout the drawings reference numbers are re used to indicate correspondence between referenced elements. The drawings associated descriptions and specific implementation are provided to illustrate embodiments of the invention and not to limit the scope of the disclosure.

The illustrated network advantageously comprises any means for communicating data between two or more systems or components. It certain embodiments the network comprises a computer network. For example the network may comprise a public network such as the Internet virtual private network VPN token ring or TCP IP based network wide area network WAN local area network LAN an intranet network point to point link a wireless network cellular network wireless data transmission system two way cable system interactive kiosk network satellite network broadband network baseband network combinations of the same or the like. In embodiments wherein the source system and destination system are part of the same computing device the network may represent a communications socket or other suitable internal data transfer path or mechanism.

As shown the source system comprises one or more applications residing on and or being executed by a computing device. For instance the applications may comprise software applications that interact with a user to process data and may include for example database applications e.g. SQL applications word processors spreadsheets financial applications management applications e commerce applications browsers combinations of the same or the like. For example in certain embodiments the applications may comprise one or more of the following MICROSOFT EXCHANGE MICROSOFT SHAREPOINT MICROSOFT SQL SERVER ORACLE MICROSOFT WORD and LOTUS NOTES.

The source system further comprises one or more processes such as filter drivers that interact with data e.g. production data associated with the applications . For instance the filter driver may comprise a file system filter driver an operating system driver a filtering program a data trapping program an application a module of the application an application programming interface API or other like software module or process that among other things monitors and or intercepts particular application requests targeted at a file system another file system filter driver a network attached storage NAS a storage area network SAN mass storage and or other memory or raw data. In same embodiments the filter driver may reside in the I O stack of the application and may intercept analyze and or copy certain data traveling from the application to a file system.

In certain embodiments the filter driver may intercept data modification operations that include changes updates and new information e.g. data writes with respect to the application s of interest. For example the filter driver may locate monitor and or process one or more of the following with respect to a particular application application type or group of applications data management operations e.g. data write operations file attribute modifications logs or journals e.g. NTFS change journal configuration files file settings control files other files used by the application combinations of the same or the like. In certain embodiments such data may also be gathered from files across multiple storage systems within the source system . Furthermore the filter driver may be configured to monitor changes to particular files such as files identified as being associated with data of the applications .

In certain embodiments multiple filter drivers may be deployed on a computing system each filter driver being dedicated to data of a particular application . In such embodiments not all information associated with the client system may be captured by the filter drivers and thus the impact on system performance may be reduced. In other embodiments the filter driver may be suitable for use with multiple application types and or may be adaptable or configurable for use with multiple applications . For example one or more instances of customized or particularizing filtering programs may be instantiated based on application specifics or other needs or preferences.

The illustrated source system further comprises a source storage device . The source storage device may include any type of media capable of storing data. For example the source storage device may comprise magnetic storage such as a disk or a tape drive or other type of mass storage. In certain embodiments the source storage device may be internal and or external to e.g. remote to the computing device s having the applications and the filter drivers .

As further illustrated in the destination system comprises a replication module and a destination storage device . In certain embodiments the replication module is configured to monitor and or manage the copying of data from the source system to the destination system such as data retrieved by the filter drivers . In yet other embodiments the replication module is a dumb server or terminal that receives and executes instructions from the source system .

The destination storage device may include any type of media capable of storing data such as replication data sent from the source system . For example the destination storage device may comprise magnetic storage such as a disk or a tape drive or other type of mass storage. In certain embodiments the destination storage device may be internal and or external to the computing device s having the replication module .

In certain embodiments the source storage device and or the destination storage device may be implemented as one or more storage volumes that include physical storage disks defining an overall logical arrangement of storage space. For instance disks within a particular volume may be organized as one or more groups of redundant array of independent or inexpensive disks RAID . In certain embodiments either or both of the storage devices may include multiple storage devices of the same or different media.

The illustrated client computer further comprises a file system for organizing files and directories accessible by the client computer . In certain embodiments the file system comprises a data structure usable to keep track of a collection of files and or directories stored on the source storage device . The file system may include for example a local file system a network file system a file server a management program or the like or may include multiple file systems accessible by an operating system. For instance in embodiments wherein the storage device is associated with multiple volumes each volume may be associated with its own file system or a single file system may span across the multiple volumes.

In certain embodiments the file system comprises a write in place file system an example of which is the fast file system. In a write in place file system the locations of the data structures such as data blocks and other descriptive information on disk are typically fixed. Certain data structures are used to store information e.g. metadata about a file whereas the data blocks are structures used to store the actual data for the file. The information contained in certain data structures may include for example ownership of the file access permission for the file size of the file file type references to locations on disk of the data blocks for the file combinations of the same or the like. The references to the locations of the file data may be provided by pointers which may further reference indirect blocks that in turn reference the data blocks depending upon the quantity of data in the file. In certain embodiments changes are made in place in accordance with the write in place file system. If an update to a file extends the quantity of data for the file an additional data block is allocated and the appropriate management files are updated to reference that data block.

The illustrated client computer also comprises one or more data agents . In certain embodiments the data agent comprises a module responsible for performing data and or storage tasks related to the client computer . For example the data agent may manage and or coordinate the compilation of and or transferring of replication data from the source system . In other embodiments the data agent may provide archiving migrating and or recovery of client computer data.

In certain embodiments the client computer comprises a plurality of data agents each of which performs data management operations related to data associated with each application . In such embodiments the data agent may be aware of the various files folders registry files and or system resources that are impacted by a particular application . For instance the data agent may be programmed to detect data management requests by a particular application and determine which files folders and or system resources are associated with the data management requests.

In certain embodiments different individual data agents may be designed to handle MICROSOFT EXCHANGE data LOTUS NOTES data MICROSOFT WINDOWS 2000 file system data MICROSOFT ACTIVE DIRECTORY OBJECTS data and other types of data. In certain further embodiments one or more data agents may be configured to backup migrate and or recover application specific data.

For example in embodiments wherein the source system comprises a MICROSOFT EXCHANGE 2000 server the source system may use multiple data agents to perform storage operations e.g. backup migrate replication or restore operations . For instance a MICROSOFT EXCHANGE 2000 mailbox data agent may be used to replicate EXCHANGE 2000 mailbox data a MICROSOFT EXCHANGE 2000 database data agent may be used to replicate EXCHANGE 2000 database data a MICROSOFT EXCHANGE 2000 public folder data agent may be used to replicate EXCHANGE 2000 public folder data and a MICROSOFT WINDOWS 2000 file system data agent may be used to replicate file system data.

In certain embodiments multiple data agents may be treated as separate data agents even though they reside on the same client computer . In other embodiments the separate data agents may be combined to form a virtual data agent for performing storage operations related to a specific application. Thus the four separate data agents of the above example may be combined as a virtual data agent suitable for performing storage operations related to MICROSOFT EXCHANGE 2000 data.

In certain embodiments the data agent is configured to perform data management operations in accordance with one or more storage policies or other preferences. A storage policy may include a data structure or other information having a set of preferences and other storage criteria for performing a storage operation. The preferences and storage criteria may include but are not limited to information regarding storage locations relationships between system components network pathways retention policies data characteristics compression or encryption requirements preferred system components combinations of the same or the like.

In certain embodiments one or more data agents are configured to perform an initial seeding process of a replication process. For example prior to or concurrently with data replication using one or more filter drivers the data agent may perform a scan of the source system e.g. the source storage device . For instance the data agent may evaluate the folders and or directory structure of the source system to determine which folders are used by a particular application . In certain embodiments the data agent may also identify arrange and queue necessary data of the application to provide a proper platform for replication. For example the data agent may populate source log s with application data that has already been written to the source storage database .

In certain embodiments when the data agent is initially installed or enabled on the client computer the data agent may evaluate the application . For instance the data agent may determine the application s organizational structure which may include for example folder directory and file information. The information gathered by the data agent may be sufficient to define a complete set of information to be replicated such that suitable baseline data representing the current operational state of the application is identified. In some instances this initial process may require the examination and identification of data related to application operations occurring prior to the installation of data agent . The data agent may also be configured to identify general configuration and operational information regarding the application . In certain embodiments the data agent may be configured to access and or monitor particular files folders directories registries preferences and or other like data structures for information to be replicated. All or a portion of the information gathered by the data agent may be copied over to the destination system as part of the initial seeding or initialization process. After the seeding process is complete data replication may occur on a substantially continuous basis based on data transfers occurring between application s and source storage device . In certain embodiments the seeding process may occur substantially concurrently with execution of the application s . For instance data operations from the application s may be temporarily stored in a queue or buffer until the seeding process or a portion thereof is complete.

In certain embodiments the data agent may be configured to evaluate an application and based on certain system management parameters and or other considerations associated with the application e.g. data size frequency of replication system or user preferences etc. the data agent may map or correlate the application data to one or more locations on the destination storage device . In other embodiments the data agent may communicate with other system components when making correlation decisions. For example the data agent may communicate with the replication agent and or an optional storage manager component when deciding how to map particular application data.

For example the data agent may map a certain application to one location on the destination storage device or may parse or separate application data for storage across multiple volumes of the destination storage device depending on preferences or system management objectives. Parsing of data across multiple volumes may occur based on application type certain applications spread across multiple volumes data type temporal considerations e.g. data for a specified time period is stored on a particular volume size considerations e.g. data up to a certain size is stored on one volume relative importance of data conservation of memory space combinations of the same or the like. Any suitable parsing criteria may be used if desired to facilitate recall storage or management of application data.

As shown in the client computer communicates through the file system with the source storage device which further includes a database and database logs . In yet other embodiments the client computer may communicate with NAS or the like. In certain embodiments data referred to the source storage device may be first written to a file in the database logs and subsequently committed to the database in accordance with data management techniques for enhancing storage operation performance. Moreover although only one database and one database log are depicted in it will be understood that the source storage device may comprise additional databases database logs and or other directory and file storage structures to meet the storage needs of the client computer .

As illustrated in the filter driver is advantageously located between the application and the file system . For instance the filter driver may be deployed in the stack as an I O buffer and or process in the data path between the application and the file system . In such embodiments the filter driver may intercept snoop supervise trap process or otherwise be cognizant of some or all operations e.g. data modification operations file modification operations read operations and the like from the application to its associated location s on the source storage device .

For example in certain embodiments the filter driver may communicate with an associated data agent to determine where data for a particular application will be stored e.g. particular folders on the file system . In certain embodiments the filter driver and or the data agent may also monitor and or parse data management operations to determine if new or additional folders are affected by the production volume data of the particular application . In certain embodiments the data agent may monitor data management operations and or other data for other purposes such as for example for satisfying a query or command by a storage manager component or the like.

As further depicted in one or more of the filter drivers and associated data agent s may be grouped together as a single module such as driver module . In yet other embodiments the data agent s may be separate from the driver module .

As discussed above in certain embodiments the filter driver is preferably configured to monitor and or filter data management operations associated with a particular application . The filter driver may be further configured according to predefined criteria to cause particular data to be written to one or more source logs for subsequent replication. For instance the filter driver may be configured to intercept scrub parse and or trap data management operations and to populate the source logs with changes associated therewith.

In certain embodiments the filter driver may examine the data management operation in progress determine whether the type of operation is one of interest for replication purposes and or copy select or all data to source log . For instance as discussed above the filter driver may determine if the data management operation concerns data in one or more files determined as relevant to replication e.g. files that may store data for a particular application . In other embodiments the filter driver may generate log entries for all data management operations.

The filter driver may further process and or traverse the data and copy generate or examine other relevant information such as a log entry number time information e.g. time stamp application type data size and start field combinations of the same or the like that may be useful in the replication process. In other embodiments the filter driver may monitor files on the source storage device for modifications of data relating to the subject application . For instance as disclosed above the filter driver may monitor a select group of files which have been associated with the application or folders to detect changes to data stored therein. In certain embodiments the filter driver or other system component may detect when a data write operation of the application is made to a file or folder not in the select group. The filter driver or other system component may then determine from the properties of the data write modification if the subject folder or file should be added to the select group for subsequent monitoring .

In certain embodiments the filter driver is deployed e.g. by data agent on the client computer prior to the beginning of the replication process. In embodiments wherein the filter driver is deployed after replication begins pertinent application data already stored on the source storage device may be copied to the source logs prior to the replication process e.g. during the initial seeding process described above .

In certain embodiments the filter driver may be enabled and or disabled by the data agent . For instance enabling the filter driver may allows it to populate an associated source log with log entries from application data passed from the application to the source storage device . When the filter driver is disabled data may pass directly through to the source storage device without being copied to the source logs .

The source log comprises any type of memory capable of storing one or more journal or log entries. In certain embodiments the source log comprises a cache memory. The source log may reside on the source system such as for example on the source storage device or at least a portion of the source log may be external to the source system .

In certain embodiments the data agent monitors the storage capacity of the source logs . For instance when one or more of the source logs reach a particular memory threshold the data agent may open a socket and communicate to the destination system that a copy of the source log is ready to be transmitted. In other embodiments the data agent may be configured to copy the source log to the destination system at periodic intervals or in accordance with other predefined criteria.

Although the source system has been described above with reference to particular embodiments other embodiments of the invention may include more or fewer components than those depicted in . For example illustrates a block diagram of a source system according to certain embodiments of the invention. As shown the source system comprises the client computer application filter driver file system source log and source storage device which includes database and database log .

The illustrated source system further comprises a network redirector module located between the filter driver and the file system . In certain embodiments the network redirector module is advantageously capable of improving performance of the data replication system. In particular the network redirector module allows for a data management operation to be sent to a destination system e.g. destination system of prior to and or substantially concurrently with the data management operation being forwarded to the source storage device .

For example with reference to when the application issues a data write operation the filter driver identifies whether or not the data write operation is one of interest such as according to particular criteria established by the data agent and or a storage manager module. If so the filter driver notifies the network redirector module .

The illustrated network redirector module is configured to buffer a copy of the data write operation to a memory . For instance in certain embodiments the memory comprises a queue such as a first in first out queue for receiving copies of the data write operations. In certain preferred embodiments the memory advantageously comprises a cache memory such as a random access memory RAM . In certain further embodiments the memory comprises a memory mapped file that is shared with the filter driver such that the filter driver provides a copy of the data write operation directly to the memory .

In certain embodiments the network redirector module comprises a driver such as a file system driver or an operating system driver that sends data to a particular destination such as a remote device e.g. a destination system a log or the like . For instance the network redirector module may operate entirely in a kernel mode and may sit above the transport layer of a TCP IP protocol. Such a configuration allows the network redirector module to interact directly with the filter driver without the need for user to kernel mode and kernel to user mode context switches.

In certain embodiments the network redirector module is further configured to access the memory and transmit the copy of the data write operation to one or more destination systems through a network e.g. network mirroring . For instance the particular destination system may include a counterpart network component that receives the data write operation from the network redirector module and forwards the replicated data write operation to a second storage device e.g. destination storage device of . During this time the other copy of the data write operation from the application is forwarded to the source storage device for execution.

In certain embodiments the network redirector module improves performance of the source system because data management operations are able to quickly pass through to the file system . That is the network redirector module may quickly cache a copy of the data management operation rather than writing a copy of the operation to disk e.g. in the source log prior to forwarding the operation to the source storage device .

In certain embodiments the source system communicates with the associated destination system to verify that the two systems are synchronized. For instance the source system may receive from the destination system an identification e.g. unique serial number of the data write operation currently being replicated by the destination system. The source system may then compare the received identification with data write operation being forwarded to the source storage device .

In certain embodiments the replication logs contain a copy of the data stored on the source logs of a client system such as the source logs of . The replication logs comprise any type of memory capable of storing data including for example cache memory. In certain embodiments the replication logs may reside on the destination system such as for example on the destination storage device or at least a portion of the replication logs may be external to the destination system . In certain embodiments once the replication logs have been populated with the data from the source logs the data on the source logs is available to be erased and or overwritten to conserve memory space.

The replication module of the destination system further comprises a replication agent and one or more processes such as threads . In certain embodiments the replication agent comprises one or more software modules that coordinate the transfer of data from the replication logs to the destination storage device .

For example in certain embodiments the replication agent instantiates an appropriate number of threads processes or routines for copying data from the replication logs to the destination storage device . In certain embodiments the number of threads is based on one or more of the following factors the number of logs files sent from the source logs to the replication logs information received from the data agent s information generated by the filter driver s and the type s of application data being tracked.

In certain embodiments the replication agent further includes mapping or correlation information that determines when and to where the data from the replication logs is copied by the threads . In certain embodiments such mapping information may be based on system or user defined parameters and or may be automatically generated such as based on the status of the destination storage device .

The one or more threads or processes direct movement of data from replication logs to the appropriate location on the destination storage device . In operation in certain embodiments the threads advantageously process or traverse replication logs for particular types of data and then copy that data to certain locations on one or more replication volumes based on data paths identified by the replication agent and or associated with each thread . For example the thread s may sequentially process each entry in the replication log and write the associated data to the destination storage device .

In certain embodiments one thread may write to one or more volumes of the destination storage device and or multiple threads may write to a single volume in parallel. Furthermore one thread may access one or more replication logs and or multiple threads may access the same replication log .

In certain embodiments each thread is assigned to a hard coded path pair which includes i a source path identifying the location on the source storage device associated with a data management operation e.g. c Folder and ii a destination path identifying the location on the destination storage device to receive the replicated data e.g. D folder from the thread .

As detailed above in certain embodiments the filter driver preferably substantially continuously populates data relating to one or more of the applications to the source logs . As shown in the source logs further comprise a first log file and a second log file . In certain embodiments the filter driver sequentially writes log entries to the source logs and when a certain capacity of the first log file is reached the filter driver begins populating the second log file with log entries.

In yet other embodiments data relating to each application of interest may be written to a particular log file established for that application. For example with reference to the first log file may relate to a first application of interest whereas the second log file may relate to a second application of interest.

In certain embodiments each of the log files of the source logs may be established by the data agent s and or the filter driver s as part of an initial deployment or initialization process. Moreover data may be written to the source logs as determined by preferences stored on or accessed by the client computer in a preference database .

For example as further shown in the first and second log files may comprise a series of entries each having an identifier that indicates the sequence order and or type of entry being made. For instance the illustrated entry identifier L may indicate that the particular entry represents a first database entry in a particular order of operation. The illustrated entry identifier L may indicate a second database entry in a particular order of operation and so forth. The illustrated entry identifier D may indicate that the particular entry represents a first database commit entry in a particular order of operation. Thus in the example described above the log entries identified by L and L may correspond to modifications associated with a particular database transaction and the log entry identified by D may correspond to a commit command for the particular transaction.

In certain embodiments the log entries are populated into the log files sequentially such that the relative order of the log entries corresponds generally to the order in which the associated data management operations were performed by the application . In other embodiments the log entries may be populated into the log files in a non sequential manner.

In certain embodiments filter driver log entries may be copies or portions of some or all of the data operations outputted from application to the source storage device . In yet other embodiments the filter driver and or data agent may construct log entries based solely on information available in the data stream between application and the source storage device . In yet other embodiments the log entries may include other system information such as time stamp information. In still other embodiments the log entries may be enriched with other useful system or application information available on the client computer or within the client system . For example the log entries may include metadata and or data classification information that has been obtained from application data.

It will be understood that although only two log files are shown in more or fewer log files may be used with embodiments of the invention. For instance multiple applications may be monitored by the filter drivers and thus additional log files may be added as necessary or desired. Moreover although in some embodiments each application and each log file in the source logs may have its own associated filter driver in other embodiments a single filter driver may be deployed and configured for use with multiple applications such that there are separate log files for each monitored application .

Additionally it will be understood that it is not necessary for different log files to be established for each application . Rather in certain embodiments one or more global log files may be used with data for each application being placed in the global log file and marked as belonging to a particular application . With this arrangement the replication system may differentiate among data associated with different applications based on data in one or more fields of the log entries as is discussed in more detail below with respect to .

With continued reference to in certain embodiments of the invention the data agent and or filter driver may be advantageously configured to pause or quiesce the application during data replication. For instance the data agent may cause the application to temporarily suspend data management operations to the source storage device once the application reaches a known good stable or recoverable state. In certain embodiments such a state may be defined as when particular computing operations of the application are complete to a point such that further operation recovery and or rolling back of the application may occur based on the recorded data without the loss of critical information or computing operations needed for operation of the application . This point of referential integrity is generally referred to herein as a known good state of the application .

In certain embodiments the data agent instructs the quiescing of the application through an application programming interface API . For instance the data agent may send a command e.g. FLRSNAP.FOO to the application that causes the application to quiesce. When the application has placed itself in a known good state the application may send an acknowledgment to the data agent .

In certain embodiments once the data management operations are suspended the I O buffers in the data path of the application are flushed and or the writes in the queues are flushed and the source logs are populated. For example some or all of the pending data management operations e.g. as of the time of the suspension of the application may be allowed to complete and or percolate through the data path. The filter driver and or data agent then inserts a logical marker or tag in the source log file denoting that a consistency point or consistency recovery point has been reached. In some embodiments the consistency point indicates the time at which the application is at a known good state. For instance in certain embodiments the data agent instructs the filter driver to insert a consistency point entry into the source logs .

Notwithstanding the foregoing it will be understood that in certain embodiments although application is quiesced it need not actually pause or suspend operation during the quiescent period. Rather the application may continue to operate substantially normally but may queue internally or otherwise buffer data management operations intended for the source storage device . After the quiescent period the buffered modification operations may be allowed to complete i.e. be sent to the source storage device .

In certain embodiments of the invention the application is periodically quiesced and a corresponding consistency point placed in the source log based on particular criteria. For instance the quiescing of the application may be based on one or more system or user defined preferences e.g. every five minutes . The periodic quiescing of the application may be based on the desired frequency of performing replication backup or other data modification operations on the subject data. For instance applications dealing with data sensitive information may necessitate more frequent quiescing and creation of consistency points than other types of applications.

In yet other embodiments policies for the frequency of consistency point entries may be automatically generated. For instance the data agent may be configured to quiesce the application based on the status e.g. capacity of the source logs the replication logs and or the destination storage device . In yet other embodiments quiescing of the application may be performed based on an automatic reporting procedure. For instance a module of the replication system may be configured to gather receive and or analyze information associated with a failure rate and or health of applicable servers. Additional details of such status monitoring are provided in U.S. patent application Ser. No. 11 120 619 filed May 2 2005 now published as US 2006 0053261 A1 which is hereby incorporated herein by reference in its entirety. For example the frequency of consistency points may be selected or adjusted to mitigate risks detected in a storage network. In certain embodiments one or more management components of the replication system may identify a storage related characteristic and may modify the frequency of consistency points if the storage related characteristic satisfies one or more particular risk criteria. For instance risk criteria may be indicated by storage policies storage definitions a service level agreement SLA or other information located in a management component database or other component of the replication system . In certain further embodiments the frequency of consistency points may be based at least in part on a level of data protection or data availability in the replication system .

In certain embodiments one or more log entries in the source logs are preferably associated with time information such as for example assigned a time stamp indicative of the client system time with which the particular log entries are associated. For instance the time information may indicate the time at which the log entry is written to the source log the data management operation is generated by the application the data modification operation is committed to disk or the like. In certain embodiments not all the log entries are assigned a time stamp. Rather particular types of data such as for example consistency point markers and or database commit entries are assigned time stamps.

In certain embodiments of the invention the data agent coordinates with the replication agent to copy log files from the source logs to the replication logs . Such copying may be initiated based on any suitable factor such as for example preset copying intervals capacity thresholds reached in the source logs time lapsed since the last copy operation replication agent requests for a copy operation and or based on specific parameters or requirements associated with a particular application . For instance certain data sensitive applications may be copied more frequently than other applications in order to reduce the amount of potential data loss due to a failure occurring between copy operations.

As further illustrated in the replication logs include a first log file and a second log file . In certain embodiments each of these log files corresponds respectively to the first log file and the second log file of the source logs . For instance data may be transferred between the replication log s and the source log s such that the order in which the data was stored in the source log s is preserved. In addition the log files may be recreated in the replication log s to reflect the organization of source logs . For example the first log file and the second log file in the source logs may be transferred and recreated by the replication agent and or the data agent . In other embodiments however data may be transferred and stored in a different order without preserving source system correlations and or may be rearranged on or during transfer to or upon arrival in replication volumes A B.

In yet other embodiments the transferred data may retain the data structure used in the source logs and or may be changed modified or rearranged to reflect conform or otherwise interoperate with the management particulars of the replication module . For example certain referential indicators associated with the data may be modified or conformed to meet requirements of the replication agent . However in preferred embodiments referential integrity of the data is maintained between the source system and the destination system .

The illustrated destination system further comprises an optional preference database in communication with the replication agent . In certain embodiments the preference database includes storage policies or other preferences usable by the replication agent in managing data. For instance the stored preferences may indicate the desired frequency at which the threads should copy the data from the destination logs to the replication volumes A B. The preference database may also store path information for detailing to which location s on the replication volume s A B the data in the replication log s should be copied. In yet other embodiments the preference database may include storage policies that dictate particular criteria for performing one or more data management operations on the replicated data.

With continued reference to the replication module further comprises one or more processes such as a replication set or a log processing module with a first thread A and a second thread B. In certain embodiments as discussed above the threads A B are instantiated by the replication agent to transfer data from the first and second replication logs to the first replication volume A and or the second replication volume B.

In certain embodiments the threads A B process scan and or traverse the replication logs and scan log entries therein to identify the data or files of concern to the particular thread. In certain embodiments the threads A B scan the log entries in a sequential manner e.g. in the order in which the log entries were written to the log file . When the thread encounters a log entry of interest the thread copies the particular log entry from the replication logs to the replication volumes A B. For instance the log entries may be copied in accordance with a path established based on the correlation or pairing information provided by the data agent to the replication agent .

In certain embodiments the threads A B utilize time stamp or other temporal information that enables processing and or replaying of modification operations. For example based on time stamp information the threads A B may rearrange the replication data such that the data is stored on the one or more replication volumes in the proper order e.g. the order in which the data was intended to be written to the source storage device . In such embodiments the replicated data may be subsequently retrieved recalled or otherwise accessed or processed and may be used to accurately restore the state of the application as it existed at a given point in time. In yet other embodiments other data management operations e.g. searching data classification may be performed on the replicated data.

In certain embodiments instantiated threads A B may operate in parallel or sequentially to scan one or more replication logs for log entries to copy for a certain application . Each thread which may be responsible for replicating certain data of the application may continue to scan the replication log until encountering a consistency point. When such occurs the thread may then notify the replication agent that it has reached a consistency point and or pause operation. When all active threads for a specific application notify the replication agent that a consistency point has been reached the replication agent may identify the data at that point in time as representing a known good state of the application . In certain embodiments at this point the replication agent may suspend further copy operations by the threads A B while the replicated data represents a known good state of the application . In yet other embodiments the replication agent may monitor the operation of the threads A B without waiting for a notification from the threads.

Generally the disclosed systems may handle files that may be needed to restore an application type e.g. MICROSOFT EXCHANGE if a data failure were to occur. This information gathering for the various files e.g. control files configuration files or the like may preserve the referential integrity of one or more predefined application types operating within the storage operation system.

The replication system of provides several advantages for performing copying or other storage operations to data. For example in certain embodiments the replication system may perform storage operations on the replicated data e.g. data stored in the replication volumes A B .

That is creating replication copies allows the replication system to access copies of production volume data without the significant use of client system resources and or interrupting or suspending data operations to the source storage device thereby reducing the impact of data management operations on client applications. In addition consistency point known good state information along with time stamp information may be advantageously used in performing storage operations to logically associate a time stamp of the copied data with the original time of the consistency point entry e.g. the time at which the application was in a known good state . Thus even though the storage operation on the replicated data is performed at a later point in time e.g. in relation to the client system time of the known good state of the application the copied data resulting from the storage operation is associated with the original consistency point time e.g. the client system time of the known good state of the application Furthermore this logical association of the client system time of the application s known good state may be repeated for subsequent copies of the data e.g. a backup of the snapshot of the replicated data .

In certain embodiments instructions for the storage operations are sent from the data agent on the source system . For instance the instructions may be included in the log file entries copied from the source system . In yet other embodiments the storage operations are coordinated by the replication agent e.g. according to storage polices stored in the preference database in combination with or independent of the data agent . In yet other embodiments policies for storage operations may be stored in another system management component e.g. a storage manager module .

Examples of certain storage operations performable on the replicated data in the destination storage device will now be described. For example a storage operation may include a basic copy of data from a first location to a second location.

Another form of a storage operation that may be performed on the replicated data is a backup operation. A backup operation generally includes copying data into a backup format as opposed to a native application format. For example a backup copy may be stored in a backup format that facilitates compression and more efficient long term storage. Backup copies generally have relatively long retention periods and may be stored on media with slower retrieval times than other types of secondary copies and media. In some cases backup copies may be stored at an offsite location.

Another form of a storage operation that may be performed on the replicated data is a snapshot operation. In general a snapshot records the state of a storage device file system or volume at a certain point in time. That is the snapshot may be used to provide a point in time image of a live storage volume. In certain embodiments the snapshot may be used for backing up data and or in the event of failure to restore the storage device or volume to a previous known good state.

Snapshots may be implemented by several different methods. For example in a copy on write method a snapshot of a storage volume is created using a designated space or snapshot volume for the particular snapshot. During the initial creation of the snapshot instead of copying the physical data information is recorded about the location of original data in the storage volume e.g. a logical copy of the data . For instance metadata may be stored with respect to the original data and or pointers may be used to identify the location of the original data. In certain embodiments this initial snapshot may be performed when the storage volume is quiesced or in a frozen state.

Successive snapshots then track changes to the storage volume as the original data is modified. Before original data is modified the original data is copied to a location in the designated snapshot storage. Thus when a read request for an unmodified data block is made to the snapshot volume the request is redirected to the original copy of the data. However if a read request is made for a data block that has been modified the request is directed to data in the specified snapshot storage. In embodiments wherein multiple snapshots are created and maintained concurrently each snapshot may be designated or assigned a different storage space or snapshot volume.

In yet other embodiments other types of snapshot technology may be used such as for example direct on write split mirror copy on write with background copy continuous data protection copies combinations of the same or the like. For instance in certain embodiments a snapshot may not actually create another physical copy of all the data associated with an application but may simply create pointers that are able to map files and directories to specific disk blocks.

In certain embodiments a snapshot of the replication data is preferably performed at a consistency point. The snapshot then represents a set of recoverable application data up to the point in time associated with the consistency point e.g. the time at which the quiesced application suspended normal data modification operations . Thus a snapshot copy created from the replication data at a consistency point may also be consistent with the data stored in the primary volume or source storage device at the time the consistency point was generated.

In certain embodiments a snapshot is taken for each volume in which data is being replicated. For instance with reference to first thread A is writing to the first replication volume A and second thread B is writing to the second replication volume B. In such embodiments when the first and second threads A B arrive at a consistency point log entry a snapshot is taken of the replicated data in each replication volume A B.

In certain preferred embodiments when the snapshot is performed at a particular consistency point the time of the snapshot is advantageously logically associated with the time that the consistency point was generated at the client system e.g. the client system time of the known good state of the application . For instance the time stamp of the consistency point may be used to logically assign a time to the snapshot of the replicated data. In such a process the snapshot of the replicated data then appears as if the snapshot was directly taken of the data in the source system at the time of the consistency point. Such a process allows for the snapshot data to be viewed as a direct copy of the production volume data for a particular application e.g. source storage device at a certain point in time e.g. the time of a known good state of an application .

For example consider production volume data e.g. source system data that represents a known good state of the application as of 3 00 P.M. To identify the known good state of the application a consistency point marker having a time stamp of 3 00 PM is added to an appropriate log file. At 3 05 P.M. the production volume data along with the consistency point is replicated to a destination storage device . Thus at this point in time the destination storage device is an exact copy of the production volume data of the application as of 3 00 P.M.

A snapshot copy of the replicated data on the destination storage device is then performed at 3 10 P.M. During or after performing the snapshot operation the snapshot is then logically associated with the 3 00 time stamp identified by the consistency point. As a result the snapshot will have the same or substantially the same data as if the snapshot were directly taken from the source system data at the known good state of the pertinent application. Moreover through logically associating the 3 00 P.M. time stamp with the snapshot data the snapshot appears to have been directly taken from the source system data even thought the snapshot was taken ten minutes later. However because the source system data is not actually used to perform the snapshot operation the source system resources are not impacted and may be free for client or other use.

In certain embodiments logical associations of time with respect to the snapshot may be stored jointly with the snapshot and or in a separate database. For instance the logical time associations may be stored in an index on one or more replication volumes on the replication module e.g. replication agent in the preference database in a storage manager database index by the data agent combinations of the same or the like. In certain embodiments stored jointly with the updated time data may also be the actual location of the snapshot on the replication volumes A E or other storage device s .

As discussed above the frequency of consistency points may vary and may depend on one or more system factors or user selectable parameters. For instance storage policies stored in one or more databases of the replication system e.g. the preference database may dictate the frequency that consistency points are inserted e.g. frequency of quiescing application and or the frequency at which snapshots are performed. In certain embodiments a snapshot may be performed at each consistency point. In yet other embodiments a snapshot may be performed at some other interval of consistency points e.g. every five consistency points .

Once the snapshot is performed one or more messages to system component s may be sent containing information relating to the time the snapshot was taken the time stamp of the consistency point the location of the snapshot the location of the replication copy the applicable time of the copy combinations of the same or the like. For instance such a completion message may be initially sent to the replication agent which may further optionally notify the data agent or other system management components with the information described above.

While certain embodiments of storage operations have been disclosed as being usable with the replication system of a wide variety of other storage operations may also be performed on the replication data and or in conjunction with consistency point information. For example other copies of the replicated data may be performed such as but not limited to creation storage retrieval migration deletion auxiliary copies incremental copies differential copies Hierarchical Storage Management HSM copies archive copies Information Lifecycle Management ILM copies other types of copies and versions of electronic data or the like.

For instance an HSM copy is generally a copy of the primary copy data but typically includes only a subset of the primary copy data that meets a certain criteria and is usually stored in a format other than the native application format. For example an HSM copy may include only that data from the primary copy that is larger than a given size threshold or that is older than a given age threshold and may be stored in a backup format. Often HSM data e.g. e mail messages is removed from the primary copy and a stub is stored in the primary copy to indicate its new location. When a user requests access to the HSM data that has been removed or migrated systems use the stub to locate the data and often make recovery of the data appear transparent even though the HSM data may be stored at a location different from the remaining primary copy data.

In yet other embodiments the replication system may use the replication data to create a shadow version of the data on the source storage device of the client system . In such embodiments if there is a failure and or overload of the source storage device the system may repoint to the shadowed version.

In yet other embodiments different types of data management operations may be performed on the replication data depending on one or more schedule policies. For example a snapshot storage operation may be scheduled to be performed at consistency point entries every five minutes while a differential backup may be performed on the replication data every hour. Furthermore a full backup may be scheduled to be performed once a day. Such scheduling policies may be advantageously determined to satisfy the needs of the user while reducing the impact on system resources.

In certain embodiments after appropriate storage operations are performed on the replicated data a message may be sent to other system management components e.g. a snapshot manager and or optional storage manager indicating that the replication process is complete up to the time stamp associated with consistency point. At this point the replication agent may instruct copy operations associated with the threads A B to resume.

In certain embodiments the log entry is initially generated by the filter driver and is stored in the source log . For example the log entry may comprise a data word having a plurality of fields. As illustrated the log entry comprises a log entry number field a path field a time stamp field an application type field a write type field a size field a checksum field an offset field and a payload field .

The log entry number field may include information regarding the entry number assigned to the log entry for system management purposes such that entries may be tracked and reordered relative to one another if necessary. For example as mentioned herein log entries may be arranged in a temporally sequential manner based on the application write operation with which the particular log entry is associated. In certain embodiments log entry numbers or other information may be recycled over time once all the numbers in a particular range have been used. In yet other embodiments the log entry number field may be configured to store other types of identification data for labeling the log entry .

The path field may include information regarding the file path on the source storage device with which the data write operation was associated. For example a path of C DIR USER may indicate that the log entry corresponds to an operation writing data to a folder or file on the source storage device having the designated pathname. In certain embodiments the path field may include an absolute file pathname. In other embodiments the path field may include an abbreviated pathname and or an inode e.g. for UNIX based systems .

Moreover the path field may include information relating to the log entry s replication volume destination and thus may be useful in establishing or confirming correlation or pairing information used by the thread s A B. For instance in certain embodiments the file path of a particular log file may be hard coded to one or more particular replication volume s .

The time stamp field may include information relating to the time when the subject data write occurred. In certain embodiments the time stamp is advantageously associated with the time of the client computer on which the application is executing. For instance the filter driver may access the source system time when generating the log entry . In other embodiments the time stamp may be provided by the filter driver and or may be relative to the replication system time.

The application type field may include information identifying the application type with which the log entry is associated e.g. MICROSOFT OUTLOOK data MICROSOFT SHAREPOINT data ORACLE data SQL data MICROSOFT WORD data MICROSOFT INTERNET EXPLORER data or the like .

The write type field may include information regarding the category of write data involved with the log entry . For instance the write type may identify if the log entry is associated with a database modification a log write a database commit command a consistency point or the like. In certain embodiments the information in the write type field is used to implement parallelism between multiple threads when performing data replication. For instance a first thread e.g. thread A may handle log write commands and a second thread e.g. thread B may handle commit database commands. In certain embodiments the data stored in the write type field may be used for prioritizing the processing of various log entries e.g. processing by the threads .

The size field may include information relating to the size e.g. the number of bytes of the data being modified by the data write operation. In yet other embodiments the size field may contain information relating to the size of other or additional segments within the log entry such as for example the size of the payload field .

The checksum field may include information relating to error checking to ensure for example that the log entry when created and subsequently transmitted contains the expected number of bits and has not been corrupted or otherwise impermissibly changed. For instance the checksum field may store data representing the arithmetic sum of some or all of the fields in the log entry .

The offset field may include information relating to the location within a file or portion of data that the data write is occurring. For instance if the subject data write operation is associated with modifying the twentieth through the thirtieth bytes of a file or piece of data fifty bytes long the offset field may store a value of twenty. In such embodiments the information in the offset field may be used jointly with the information in the size field to identify the entire portion of a file being modified. For instance in the above example the size field may store a value of eleven to indicate the length of the modified section i.e. twentieth through thirtieth bytes .

The payload field may include information relating to the data written from the application to the source storage device . This information generally represents the application data captured by the filter driver for replication and may include additional information for the ongoing operation or reconstitution of the application .

It will be understood that the illustrative filter driver log entry shown in merely represents one possible embodiment of a log entry suitable for use with embodiments of the invention and that other embodiments may be used if desired. For example in other embodiments the log entry may comprise more or fewer fields to accommodate the requirements of the particular replication or storage operation system involved and or to achieve certain data or management goals such as conserving memory increasing processing speed and increasing the amount of information in each log entry. For instance in certain embodiments wherein the path determination for a particular log file or log entry is dynamic the log entry may not include the path field . In other embodiments the log entry may include a priority field that may be used for prioritizing replication and or data management operations of data associated with the log entry .

In other embodiments the log entry may concern a file attribute change rather than a data write operation. In such embodiments the write type field may identify the log entry as being associated with a file attribute change. Furthermore the log entry may store information regarding the new file attribute but would not require offset or size values to be stored in the size field and or the offset field .

In certain embodiments the storage manager maintains an index such as a cache for storing information relating to logical relationships and associations between components of the replication system user preferences management tasks and or other useful data. For example the storage manager may use its index to track the location and timestamps of one or more snapshots of the replicated data. In certain embodiments the storage manager may track logical associations between one or more media agents not shown and or storage devices.

The storage manager may also use its index to track the status of data management operations to be performed storage patterns associated with the system components such as media use storage growth network bandwidth Service Level Agreement SLA compliance levels data protection levels storage policy information storage criteria associated with user preferences retention criteria storage operation preferences and other storage related information. The index may typically reside on the storage manager s hard disk and or other database.

As shown in the storage manager further communicates with a database . In certain embodiments the storage manager database comprises a memory for storing system management information relating to the replication of data. For instance the database may be configured to store storage and or restore policies user preferences the status or location of system components or data combinations of the same and the like. In yet other embodiments the database may be configured to store information described above with respect to the index . In yet other embodiments at least a portion of the index may be stored on the database .

In other embodiments the storage manager may alert the user or system when a particular resource of the replication system is unavailable or congested or when components are unavailable due to hardware failure software problems or other reasons. In certain embodiments the storage manager may utilize replication system data to suggest solutions to such problems when they occur or even before they occur. For example the storage manager might alert the user that a storage device in the replication system was full or otherwise congested and then suggest based on job and data storage information contained in its index cache an alternate storage device. In yet further embodiments the storage manager or other system component may take action to remedy the problem at issue. For example the storage manager may perform load balancing error correction or the like based on information received regarding the replication system .

In certain embodiments the storage manager may include other components and or modules. For example the storage manager may include a jobs agent module not shown that monitors the status of storage operations that have been performed that are being performed or that are scheduled to be performed in the replication system .

Moreover the storage manager may include an interface agent module not shown . In certain embodiments the interface agent module may provide presentation logic such as a graphical user interface GUI an application program interface API or other interface by which users and system processes may be able to retrieve information about the status of storage operations and issue instructions to the replication system regarding the performance of storage operations. For example a user may modify the schedule of a number of pending snapshot copies or other types of copies. As another example a user may use the GUI to view the status of all storage operations currently pending in the replication system or the status of particular components in the replication system .

Additional details of storage manager modules useful with embodiments of the replication systems described herein are described in U.S. patent application Ser. No. 09 354 063 filed Jul. 15 1999 which is hereby incorporated herein by reference in its entirety.

The initialization process begins with Block wherein one or more data agent s are installed on the client computer . In certain embodiments the data agent may be installed remotely from other portions of the replication system based on a particular need or to conform to certain directives or resident storage policies. In other embodiments the data agent may be installed locally by a system user as desired. For instance installation of the data agent may include deployment and installation of object code files and supporting software.

In certain embodiments the data agent may be installed for each application of interest or one or more data agents may be installed for a larger number of applications . Furthermore in certain embodiments an installation guide such as a wizard or other program may recommend the appropriate number and type of data agents to install which may be performed substantially automatically based on application and system configuration information .

At Block the installed data agents may perform certain auto discovery routines in order to determine basic system and application information. In some embodiments the auto discovery routines may be considered part of the installation process. For example the data agent may begin the auto discovery process by scanning and evaluating the folder and directory structure of the client computer to determine which folders are used by a particular application . In certain embodiments such information allows the data agent to identify and locate files or other information necessary to replicate the current operating state of the application of interest.

In certain embodiments the scanning and evaluation process may involve scanning multiple physical and or logical volumes associated with the source storage device and or within a given network or enterprise to locate the data and system configuration information necessary for data replication.

After the appropriate resources have been discovered and examined the data agent may identify arrange coordinate and or queue the necessary data within various locations or instances of the application to establish a platform for proper data replication Block . In certain embodiments this process may be a precursor for performing the initial seeding operation described above.

Next at Block the data agent communicates with the replication agent . For instance the data agent may transmit to the replication agent information regarding the replication of data. The data agent may also request information from the replication agent and or other network management components for any information that may bear on or be related to the correlation or mapping of network storage paths for replication data. For example the data agent may consult the preference database of the destination system the preference database of the source system and or a storage manager component for correlation or pairing information. Based on this information data paths may be identified for use by threads when copying data from the replication logs to the replication volumes A B. In certain embodiments one or more data paths may be dynamically coded or determined such as for example based on one or more storage policies and or preferences.

At Block the initialization process includes installing and initializing the filter drivers . In certain embodiments such installation and or initialization is based at least in part on information obtained by the data agent during the discovery process Block . For example in certain embodiments one or more filter drivers may be installed by the data agent in the I O path of the application s .

The replication process begins with Block wherein the filter driver populates the source log s with data associated with the application such as data identified by the data agent . As discussed in more detail above such data may relate to data or file modification operations being passed from the application to the source storage device . In certain embodiments the filter driver populates the source logs in a temporally sequential manner such that operations and data are recorded in time descending or ascending order e.g. first operation at the top and last operation at the bottom .

In certain embodiments the data is populated in the source logs in a format similar to the structure of the log entry of . In other embodiments the data may be populated in other suitable formats to satisfy the requirements of the particular replication system. For instance the log file format may comprise a two or multi column structure wherein the information in a first column may indicate the type of data operation performed and the log entry s position in the log file indicates the order of the operation relative to other operations in the log file. The information in a second column may indicate the payload data associated with the data operation indicated by the first column.

After or concurrently with Block the data agent or other system component pauses or quiesces the application Block . As discussed above such quiescing causes the application to temporarily suspend data modification operations to the source storage device once the application reaches a known good state.

Once new modification operations are suspended and the associated source log is populated based on the modification operations up to the known good state the data agent or other replication system component inserts a logical marker or tag in the source log Block . This consistency point denotes that the state of the data is such that the application may be recovered or that further stable operation from that point going forward is ensured. Once the consistency point is identified and established the data agent may restart the application such that data modification operations from the application to the source storage device resume.

As referenced by Block the data agent or other system management component coordinates the transfer of the data in the source logs . In certain embodiments the data agent coordinates with the replication agent to copy data from the source logs to the replication log s . For instance the replication agent and or data agent may open a network path or a communication socket between the source log s and the replication log s . The log entries of the source log s may then be transferred as described above to populate the replication log s . In certain embodiments as the replication log is populated the replication agent may also obtain configuration information from the data agent or other system management component such as for example a storage manager. Such configuration information may identify aspects of the set of information being transferred as well as identify pairing information that correlates certain types of replication data with certain replication volumes or other storage destinations.

At Block the replication process includes instantiating one or more threads to begin the transfer of data from the replication log s to certain replication volumes A B. In certain embodiments the replication agent is configured to instantiate one or more of the threads A B. In certain embodiments the threads are instantiated and or particularized based on pairing or correlation information received from a management component and or based on certain system configuration information e.g. available replication volumes data path information the type of information in the transferred data set combinations of the same and the like. For example the replication agent may instantiate one or more threads that correlate certain data types with certain data volumes and may specify primary and alternate data paths.

Once instantiated the threads process and or traverse the replication log s until a consistency point is encountered Block . In certain embodiments when reaching a consistency point the thread stops scanning the replication log and notifies the replication agent that the thread has reached the consistency point Block .

In certain embodiments once all active threads associated with traversing the replication logs have notified the replication agent that a consistency point has been reached the replication process moves to Block . At this point the replicated data stored in the replication volumes A B preferably represents a known good state of the application .

At Block the replication agent suspends further operation by the threads . For instance the replication agent may suspend data writes to the destination volumes A B. At this point the replication process proceeds with Block wherein one or more storage operations e.g. snapshots may be performed on the replicated data which are described in more detail above.

As shown the replication system comprises the source system that communicates through the network with a plurality of replication modules and . For simplicity the various sub components of the source system and the replication modules will not be described in detail and it will be understood that such sub components and the functions thereof may be similar to those described with respect to .

In certain embodiments each of the replication modules is configured to receive a portion of replication data from the source system . For example one or more data agents of the source system may communicate with replication agents of the replication modules to coordinate the transfer data from source logs to replication logs. In certain embodiments one data agent may communicate with one or more replication modules or multiple data agents may communicate with the same replication module.

For instance in certain embodiments each replication module may be configured to receive a particular type of application data from the source system . As an example replication module may be configured to receive replication data related to MICROSOFT EXCHANGE while replication modules are configured to receive replication data relating to respectively SHAREPOINT and SQL data. In yet other embodiments each replication module may handle different types of data from the same application.

In other embodiments the replication data may be sent to each of the replication modules based at least in part on the working load of the replication modules . In such embodiments the replication system may further comprise a storage manager or other system component not shown that may monitor the state of each of the replication modules . In yet other embodiments the replication modules may communicate a status report to the data agent s of the source system . When one of the replication modules has an unbalanced load compared to the other replication modules the replication data from the source system may be re routed or directed to one or more of the other replication modules.

It will also be understood that in certain embodiments the replication modules may also communicate with each other either directly or through the network . For example the replication modules may share with each other information regarding storage policies storage location information processing loads storage capacities combinations of the same or the like.

As further depicted in each of the replication modules is associated with a destination storage device the functionality and structure of which is described in more detail above. In certain embodiments each of the destination storage devices is a separate device. For instance one or more of the destination storage devices may be remotely located with respect to the other destination storage devices. In yet other embodiments one or more of the destination storage devices may reside on the same physical medium such as separate volumes on the same physical media.

In certain embodiments the replication system further comprises a second tier having one or more destination storage devices that are used to store copy data generated from storage operations performed on the replication data stored in the destination storage devices . For instance as shown in communication with the destination storage devices is a media agent . In certain embodiments the media agent comprises a module software and or hardware that conducts copies or otherwise moves data between the destination storage devices and the destination storage device . The media agent may generally communicate with the destination storage devices via a local bus such as a SCSI adaptor. In other embodiments the destination storage devices may be communicatively coupled to the media agent via a SAN and or a NAS.

In certain embodiments the media agent may maintain an index cache that stores data generated by the replication system during storage operations. Such data may include for example information regarding the location of the stored data on the destination storage devices information regarding the content of the data stored such as file names sizes creation dates formats application types and other file related criteria information regarding the client system and or the replication modules 

In certain embodiments the media agent and or a storage manager may store information regarding one or more storage policies storage criteria or storage preferences associated with the stored replicated data. For example in certain embodiments the media agent is configured to obtain partial snapshots of data from multiple destination storage devices and merge the partial snapshots into a single snapshot of data for storage on the destination storage device

For instance in certain embodiments the media agent may obtain partial snapshots that are logically associated with the same time stamp. That is partial snapshots of replication data that are generated with respect to the same consistency point may be forwarded to the media agent and constructed into a single snapshot of the data. Because the time stamp logically associated with the merged snapshot refers to a known good state of the application at the time a consistency point was generated at the client system it does not matter the time s at which the partial snapshots of the replicated data were actually taken and or merged. Thus the media agent may be capable of merging snapshots that were taken at different times but that refer to the same known good state of the application.

Furthermore in certain embodiments the media agent or other system component may logically associate the newly merged snapshot with the timestamp of the relevant consistency point. Thus multiple storage operations may be repeatedly performed on the replication data without losing referential integrity with the production volume data stored on the client system.

While performing data copies computer networks are sometimes faced with network errors and or rebooting. For instance a network connection may be lost between two sites and or transmitted data may be corrupted. In view of the foregoing it would be advantageous to have a replication system that further includes a process for verifying that log entries are received and or properly processed or replayed once on the destination machine s .

In certain embodiments the rolling cache configuration maintains a rolling cache of log entries on a source system and keeps track of the last successfully applied sequence number on each of the destinations. In general the rolling cache stores log entries generated by the source system which log entries are subsequently retrieved by one or more replication threads or processes . In a UNIX based environment the log entries in the rolling cache have preferably been assigned a destination file pathname such as through pathname translation which is described in more detail below with respect to .

In certain embodiments the rolling cache comprises a permanent disk cache whose storage that is not affected by system reboots or the like. In certain embodiments the rolling cache is configured to maintain its storage space by performing in a first in first out fashion. In particular when new log entries are added to the rolling cache the oldest log entries are deleted. Such embodiments advantageously reduce fragmentation and the need for disk cleanups of the rolling cache .

In certain embodiments the size of the rolling cache is selectable by a user. For instance the user may input through a user interface the amount of memory on a disk to be allocated for the rolling cache . In other embodiments the size of the rolling cache is fixed and or predetermined. For example in certain embodiments the rolling cache maintains a log entry history of between approximately five gigabytes and approximately twenty gigabytes. In other embodiments the rolling cache has a smaller size or a larger size to meet the needs of the particular replication system e.g. depending on the size of individual log entries .

As depicted in the rolling cache further communicates with a plurality of replication threads a first replication thread a second replication thread a third replication thread and a fourth replication thread . In certain embodiments each of the replication threads communicates through a network with one or more destination systems. For example the first replication thread may communicate with a first destination system the second replication thread may communicate with a second destination system the third replication thread may communicate with a third destination system and the fourth replication thread may communicate with a fourth destination system . In other embodiments multiple replication threads may communicate with a single destination system and or a single replication thread may communicate with multiple destination systems.

In certain embodiments the replication threads obtain log entries from the rolling cache and forward the entries to the appropriate destination system s for data replication. Because a history of the log entries is preferably stored in the rolling cache the replication threads do not need to store copies of the transmitted log entries even if the particular destination system happens to be offline. In other embodiments the replication threads monitor which log entries have been successfully replayed on the destination system s .

For instance in certain embodiments each of the log entries comprises a unique identification such as for example a sequence number. When a particular log entry is replayed on a destination system the subject destination system records the sequence number of the most recently stored log entry. In certain embodiments when one of the replication threads establishes or reestablishes a network connection with one of the destination systems e.g. after a network glitch or reboot the replication thread requests from the destination system the sequence number of the most recently and successfully replayed log entry. The replication thread then uses the sequence number to retrieve the next log entry from the rolling cache and the replication process resumes from where the process was previously suspended or aborted. In circumstances when too much time has passed since the network failure and the rolling cache has deleted the log entries at issue e.g. rolled over then the replication system may fall back to an initial mirroring in order to resume normal operation.

In other embodiments the destination system s send the sequence number of the most recently replayed log entry to the particular destination thread that transmitted the log entry. In other embodiments the replication system may store the sequence number of the most recently played log entry. In yet other embodiments the replication system may notify all the replication threads and or other destination systems that a particular log entry has been successfully replayed.

In certain embodiments because the rolling cache provides a substantial history of all logged changes the replication system is capable of recovering from network glitches or destination machine reboots without having to resynchronize the entire system. That is the replication thread s are able to find the appropriate log entry e.g. the log entry following the latest successfully replayed log entry in the rolling cache and resume replication traffic to the destination system.

Embodiments of the rolling cache configuration of may provide several advantages. For example in certain embodiments the replication systems do not need to send acknowledgments to the replication thread s that a particular log entry has been received by the replication system s . In such embodiments the replication threads are configured to stream or transmit log entries to the destination systems without waiting for a response thus resulting in a more rapid transmission of data. Furthermore components of the source system need not store information regarding which log entries have been replayed.

Although the rolling cache configuration of illustrates a plurality of replication threads and destination systems in other embodiments the rolling cache configuration may comprise more or fewer replication threads and or destination systems. Furthermore in certain embodiments of the invention the rolling cache configuration may comprise more replication threads than destination systems or more destination systems than replication threads.

As discussed above one of the advantages of the embodiments of the data replication systems disclosed herein is that such systems are capable of translating information intercepted by a filter driver on a first source system into information that is suitable for replay e.g. replication on a second destination system. In certain embodiments however the identification of files or directories in the source system may not be suitable for use with the directory structure of the destination system.

For example in UNIX based systems such as SOLARIS and LINUX file system operations are generally identified as operations on inodes or vnodes such that files are referenced by a unique inode number and or by a combination of one or more directory inode numbers and a short name. Such systems often utilize pathname translation algorithms to implement a user level hierarchical view of the file system.

Such usage of inodes and short names however is not conducive for replaying data modification operations on a second system such as occurs in the data replication systems disclosed herein. That is a path having one or more inodes and or short names does not provide a destination system with the appropriate information for performing the replicated data modification operation.

Certain operating systems e.g. SOLARIS 10 LINUX 2.6 perform pathname translation within the operating system kernel by generally traversing backwards a directory name lookup cache DNLC . Using such translation systems in the data replication environment however may yield concurrency issues if certain locking processes are not performed. For instance in order to ensure that other threads or processes do not rename one of the components of a file s absolute path between the time that the thread computes the absolute path and the time that a relevant log entry is emitted the DNLC would need to be locked against updates from other threads during that period of time. Having this central lock on the DNLC however may impose severe performance penalties on the entire operating system.

As shown the pathname translation system comprises a filter driver . In certain embodiments the filter driver is configured to monitor data management operations such as data write operations or file attribute modification operations associated with a computer application executing on a source computer. For instance such operations may comprise changes to data in a production level memory. Examples of embodiments of filter drivers usable with the pathname translation system are described in more detail herein.

The filter driver is further configured to populate a queue with log entries or raw journal entries related to detected data modification operations from the application. In certain embodiments the log entries generated by the filter driver are each associated with an inode that identifies to which directory and or file on the source storage device the associated data modification was directed. The queue is configured to store the log entries until they are processed by a driver thread or process . In certain embodiments the queue is implemented in volatile memory on the source system.

The queue forwards the log entries to the driver thread . In certain embodiments the driver thread polls the queue for newly generated log entries by the filter . The driver thread subsequently stores the log entries in a buffer . In certain embodiments the buffer may be labeled a raw buffer in that it is configured to store raw log entries which were generated by the filter driver and or which do not yet have an absolute file pathname.

In certain embodiments the buffer is a memory based queue for storing the log entries until processed by a database thread or process . In certain embodiments the buffer advantageously facilitates and or expedites the unloading of raw records from expensive driver memory to swappable application memory. For instance the buffer may comprise an application level buffer of a size between approximately 40 megabytes and approximately 60 megabytes. In certain embodiments the buffer is advantageously implemented as a first in first out buffer.

In certain embodiments the database thread is advantageously capable of performing inode to pathname translation for each of the log entries in the buffer . After performing the translation the database thread may send the log entry with the absolute file pathname instead of the inode entry to a desired destination such as a replication system for further processing. In yet other embodiments the database thread sends the log entry to a cache such as the rolling cache of on the source system prior to the log entry being sent to a replication system.

In certain embodiments the database thread is configured to access a pathname database to enable the thread to perform pathname translation. The pathname database advantageously stores information that associates one or more inodes or short names with an absolute file pathname. In yet other embodiments the pathname database may comprise other means or data for performing pathname translation including but not limited to a flat table customized code combinations of the same or the like.

In certain embodiments the database thread is configured to sequentially process and perform pathname translation for each of the log entries and or perform database updates. In certain embodiments because the entries are processed in the same order as recorded by the source storage system the consistency of storage file names in preserved in the pathname database each time a new record arrives. In certain further embodiments a particular file system is associated with only a single database thread . In certain embodiments use of a single thread to perform pathname translation also facilitates synchronization between the pathname database and the source system e.g. source file system . Because all the database lookups and changes to the pathname database are being performed by or through the database thread in a serialized manner unwanted modifications to pathnames prior to accessing the pathname database are avoided.

In certain embodiments of the invention accessing the pathname database introduces delay into the translation system . For example at certain points in the replication process the filter driver may generate log entries at a quicker pace than the pathname translations being performed by the database thread . For instance high activity disk lookups in the database for each log entry may require more time than the generation of the log entries by the filter driver .

In such embodiments the buffer is advantageously capable of adapting itself to the speed of the database thread . For example when the lookups by the database thread are relatively fast the buffer does not introduce significant delay into the data flow e.g. relatively no performance degradation due to the buffer . Thus the buffer may be advantageously sized to be relatively transparent to the data stream e.g. has a small footprint . However when the database lookups begin to slow down the buffer is able to store multiple log entries until the database thread is able to catch up.

In certain embodiments the database lookups by the database thread may become so time intensive that the maximum storage capacity of the buffer is reached. In such embodiments the buffer is configured to provide disk swapping functionality to avoid overflow of the buffer which may result in memory problems and or aborting replication. For instance as shown in the buffer may store excess log entries in a folder in memory . In certain embodiments the memory may comprise a disk and or may be located on the storage device of the source machine.

In performing the swapping the buffer forwards the most recent log entries to the memory . As the database thread retrieves and processes the least recent log entries in the buffer the buffer retrieves the log entries stored in the memory .

Disk swapping as discussed above may also introduce delay or performance degradation into the translation system . Thus in certain embodiments the translation system is configured to monitor the status of the queue and or the buffer to determine whether the accumulation of non processed log entries exceeds a particular threshold. For instance such monitoring of the queue may be performed by the filter driver and or other management component external to the translation system . In certain embodiments the filter driver utilizes a counter to track the number of log entries stored in the queue .

In certain embodiments if the accumulation of non processed log entries becomes too high the translation system is configured to throttle the application and or other applications running on the system. For example the filter driver may monitor the memory required by the stored log entries and when the memory exceeds a given threshold throttling of the application takes place. For instance a threshold between approximately 30 megabytes and approximately 60 megabytes may be established such that application throttling occurs when the threshold is exceeded.

In certain embodiments application throttling includes the filter driver introducing delays into the input output path of the application such as by holding on to data modification operations rather than letting them pass through to the file system. For instance the filter driver may cache the data modification operations until the database thread has processed the excess log entries. In yet other embodiments the filter driver may issue a command to suspend the user application e.g. for a period of a few milliseconds such as for example a particular operation of the application.

In certain embodiments the driver throttling of the application is independent of the disk swapping processes of the buffer . In yet other embodiments communication may exist between components of the translation system such that the driver throttling process is used in conjunction with disk swapping.

Although the translation system has been described with reference to particular embodiments other embodiments of the translation system may take on other configurations and or functions. For example in certain embodiments the translation system may function without the queue and or the driver thread . In such embodiments the buffer may be implemented in the filter driver to advantageously reduce the number of copies made of the log entries. Such a configuration however may utilize additional memory of the operating system kernel.

The illustrated pathname database is configured for inode to pathname translation such as for a UNIX based system. In particular the pathname database includes three columns a directory inode or parent inode column a short name column and an entry inode column .

In certain embodiments each inode in a UNIX based system is recorded as an entry in the pathname database . For instance illustrates a system having four inodes each having a single entry in the entry inode column and having a value of 1 through 4. The corresponding short name column identifies the short name of the file or folder associated with the particular inode. For instance entry inode 4 identifies a folder or file with the short name of user while entry inode 1 identifies a root directory. The directory inode column or parent inode column identifies the inode of the parent directory to the particular entry inode. For instance entry inode 3 which has a short name of file is a child of the folder with an inode of 2. 

As can be seen from the illustrated pathname database when the database thread receives a log entry with a particular inode the database thread is able to access the pathname database and construct an absolute file pathname using the information stored therein.

As shown the translation process begins at Block wherein the database thread receives a log entry to be processed. For example with reference to the database thread may retrieve the log entry from a buffer . In certain embodiments the log entry preferably represents a data modification operation associated with a particular application on the source system.

At Block the database thread identifies the inode associated with the particular operation represented by the log entry. For instance the inode may represent a file or folder to which data is to be written. In other embodiments the inode in the log entry may identify a file name to be modified or other data or file modification operation.

At Block the database thread accesses the pathname database to acquire information for translating the inode to an absolute file pathname. In particular the database thread searches the entry inode column for an entry that corresponds to the value of the log entry inode. Once the corresponding inode entry is found the database thread determines and stores the associated short name from the short name column Block .

The translation process then proceeds with Block . If the subject inode does not correspond to the root directory the database thread identifies from the directory inode the inode of the parent directory Block . The database thread then searches the entry inode column for the parent directory inode Block and adds the short name associated with the parent directory inode to the absolute file pathname Block .

The translation process then returns to Block to repeat the lookups and construction of the absolute file pathname until the database thread reaches the root directory. Once the database thread reaches the root directory the database thread stores the fully translated file pathname with the associated log entry Block and the translation process terminates.

For exemplary purposes the translation process will be now be described with reference to a data write command vop write 4 DATA and the values illustrated in the pathname database of . To begin the translation process the database thread receives the log entry representing the command vop write 4 DATA Block which corresponds to writing DATA to inode 4 on the source system Block .

The database thread then accesses the pathname database and searches the entry inode column for a value of 4 Block . Upon finding 4 in the entry inode column the database thread determines from the short name column that the short name corresponding to inode 4 is user Block .

Because inode 4 does not correspond to the root directory Block the database thread identifies from the directory inode column that the parent directory inode of inode 4 is inode 2 Block . The database thread then returns to search the inode entry column for the inode value of 2 Block determines that the short name for inode 2 is dir and adds dir to the file pathname Block .

Because inode 2 does not correspond to the root directory Block the database thread identifies from the directory inode column that the parent directory inode of inode 2 is inode 1 Block . The database thread then searches the inode entry column for the inode value of 1 Block and determines that the inode 1 corresponds to the root directory Block .

Now that the database thread has encountered the root directory Block the database thread stores the translated file pathname i.e. dir user with the subject log entry and the translation process terminates.

It will be understood that the translation process may differ in other embodiments of the invention in order to suit the needs of the particular system s involved. For instance the translation process may be used to translate particular modes into file pathnames shorter than an absolute file pathname such as for example a relative pathname.

In certain embodiments the three column database provides significant advantages over a flat two column table e.g. with an inode column and an absolute file pathname column . For instance the three column database structure of the pathname database may use less memory than the two column table and or expedite folder rename operations. As an example when a name of a folder is modified the three column database structure allows for a single lookup and modification e.g. modifying the short name column entry associated with the entry inode column entry of the subject inode while the two column table would require multiple lookups and modifications corresponding to each entry having an absolute file pathname that includes the folder to be renamed.

As discussed above in certain embodiments the pathname database is maintained in userland e.g. an application space external to the kernel space . In such embodiments the pathname database may be advantageously managed and or accessed by userland code without impacting the resources of the operating system kernel or other applications.

In certain embodiments the pathname database may be initially populated during an initialization period. For instance a snapshot may be taken to produce a static image of the file system of the source system. The pathname database may then be populated based on the snapshot. As subsequent changes are made to file names of the source system corresponding changes are made in the pathname database in order to maintain synchronization.

In yet other embodiments the pathname database may be specific to the files and or folders of one or more particular applications. For example the pathname database may include inodes short names and related information only for those inodes affected by a single application e.g. MICROSOFT EXCHANGE . In yet other embodiments multiple pathname databases may be used.

As discussed above certain embodiments of the invention allow one or more users to customize the replication processes performed by one or more replication systems described herein. illustrate block diagrams of exemplary embodiments of user interface portions of a replication system.

In particular illustrates a user interface portion having an interface associated with a client computer . In certain embodiments the interface comprises a graphical user interface. For instance the graphical user interface may comprise a one or more windows drop down lists text boxes combinations of the same or the like displayed on a computer monitor.

The client computer further includes a filter driver that communicates with a plurality of source logs . In certain embodiments each of the source logs corresponds to a particular application .

In certain embodiments the interface provides the user with an option of specifying one or more policies that apply to each of the applications . For instance the user may be capable of determining which of the applications is to be monitored and or how frequently data associated with a particular application is to be copied. In yet other embodiments the interface may allow a user to associate particular applications with particular source logs. For example the user may specify that all data files related to an SQL related application be filtered and logged in source log . The user may also specify that all data files related to a MICROSOFT EXCHANGE application is to be filtered and logged in source log .

In yet other embodiments the user through the interface may designate different types of data management operations e.g. storage operations to be performed on the application data by the filter driver . Storage operations may include for example creating snapshot copies application specific backup data system related backup data and or other copy operations associated with data in a storage operation system.

In other embodiments filter preferences may be set forth in a template or default storage policy. For example in the user interface portion illustrated in a single source log may be used to log files associated with different applications specified by the user. For example within the source log the log entries may comprise one or more application identifier fields such as for example the application type field see for distinguishing between logged entries associated with different user selectable applications.

In certain embodiments the interface is included within a source system such as with the client computer . In yet other embodiments the interface may be part of a system management component such as the storage manager of .

In certain embodiments of the invention data replication systems and methods may be used in a modular storage management system embodiments of which are described in more detail in U.S. Pat. No. 7 035 880 issued Apr. 5 2006 which is hereby incorporated herein by reference in its entirety. For example the data replication system may be part of a storage operation cell that includes combinations of hardware and software components directed to performing storage operations on electronic data. Exemplary storage operation cells usable with embodiments of the invention include CommCells as embodied in the QNet storage management system and the QiNetix storage management system by CommVault Systems Inc. Oceanport N.J. and as further described in U.S. patent application Ser. No. 10 877 831 filed Jun. 25 2004 now published as U.S. Patent Application Publication No. 2005 0033800 A1 which is hereby incorporated herein by reference in its entirety.

Systems and modules described herein may comprise software firmware hardware or any combination s of software firmware or hardware suitable for the purposes described herein. Software and other modules may reside on servers workstations personal computers computerized tablets PDAs and other devices suitable for the purposes described herein. Software and other modules may be accessible via local memory via a network via a browser or via other means suitable for the purposes described herein. Data structures described herein may comprise computer files variables programming arrays programming structures or any electronic information storage schemes or methods or any combinations thereof suitable for the purposes described herein. User interface elements described herein may comprise elements from graphical user interfaces command line interfaces and other interfaces suitable for the purposes described herein.

Embodiments of the invention are also described above with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams may be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable memory that can direct a computer or other programmable data processing apparatus to operate in a particular manner such that the instructions stored in the computer readable memory produce an article of manufacture including instruction means which implement the acts specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operations to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide steps for implementing the acts specified in the flowchart and or block diagram block or blocks.

In addition methods and functions described herein are not limited to any particular sequence and the acts or blocks relating thereto can be performed in other sequences that are appropriate. For example described acts or blocks may be performed in an order other than that specifically disclosed or multiple acts or blocks may be combined in a single act or block.

While certain embodiments of the inventions have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the disclosure. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the disclosure. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the disclosure.

