---

title: System and method for operating a big-data platform
abstract: A system and method for operating a big-data platform that includes at a data analysis platform, receiving discrete client data; storing the client data in a network accessible distributed storage system that includes: storing the client data in a real-time storage system; and merging the client data into a columnar-based distributed archive storage system; receiving a data query request through a query interface; and selectively interfacing with the client data from the real-time storage system and archive storage system according to the query.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582528&OS=09582528&RS=09582528
owner: Treasure Data, Inc.
number: 09582528
owner_city: Mountain View
owner_country: US
publication_date: 20160505
---
This application is a continuation of U.S. patent application Ser. No. 13 671 896 filed 8 Nov. 2012 which claims the benefit of U.S. Provisional Application No. 61 558 395 filed 10 Nov. 2011 both of which are incorporated in their entirety by this reference.

This invention relates generally to the data intensive distributed systems field and more specifically to a new and useful system and method for operating a big data platform in the distributed systems field.

Developments in technology have led to an increasing amount of available data. Such data has numerous useful applications and many companies could benefit from being able to analyze that data. Specialized systems have been developed to work with large datasets. The data platform commonly known as HADOOP is one exemplary platform designed for large datasets that leverages MapReduce. However many websites and applications are built on system infrastructures that differ greatly from those that can take advantage of large quantities of data. To take advantage of platforms such as the data platform commonly known as HADOOP a system would have to be re engineered for the new platform. Such infrastructure changes are time and cost prohibitive. Additionally a platform designed for large datasets requires highly specialized and knowledgeable personnel. Many of the people working on existing websites and applications do not have such skills. Thus there is a need in the data intensive distributed systems field to create a new and useful system and method for integrating with a data intensive infrastructure. This invention provides such a new and useful system and method.

The following description of the preferred embodiments of the invention is not intended to limit the invention to these preferred embodiments but rather to enable any person skilled in the art to make and use this invention.

As shown in a system for operating a big data platform of a preferred embodiment of the invention includes a client data agent integrated with a discrete event channel a remote distributed storage system a query processing cluster and a query interface . The remote distributed storage system may additionally include a real time storage system and an archive storage system . The system functions to enable applications built on any infrastructure to benefit from the data processing capabilities of a specialized and dedicated infrastructure. As the system functions independently from the application of a client the big data processing requirements are abstracted to a simple interface for the client. The system preferably uses a columnar based archive storage system and a data intensive query processing cluster deployed on a distributed computing environment to enable fast data interactions on scalable system. The system may additionally be adapted to function as a multitenant service so that numerous clients can utilize the system simultaneously. Data of various users is preferably stored securely and in a format isolated from other user data. The query processing cluster is preferably shared by users of the multitenant infrastructure. The system is suitable for use with big datasets such as datasets measured in the terabytes petabytes or even greater data size. The system has the benefits of using generally discrete and unorganized data encountered periodically during the operation of an application and converting such unstructured data into a structured data from large dataset. As a related benefit the system enables existing applications and data sources to easily and organically benefit from the big data platform. This may be true even for existing application and data sources that were designed without any considerations for big data processing. Put another way the system enables traditional applications to benefit from analysis of data that requires non traditional data management tools. As shown in the system can preferably be built utilizing the data platform commonly known as HADOOP and cloud hosting services for some base components. The system may alternatively include any suitable components or be built on any suitable infrastructure.

The client data agent of a preferred embodiment functions to integrate with client systems to communicate small portions of data to the remote distributed storage system. An event channel or interface preferably enables a website application service or any suitable networked program to supply event data. The client data e.g. event data is preferably discrete event data. Discrete event data preferably has an association to the time of an event the kind of event and or any suitable additional event attributes. An event channel may be a database log app log syslog sensor data stream web content or any suitable source of data. Typically an application will generate client data in response to actions of a user of the application and the client data will include information of the action. Preferably the client data is formatted as a plurality of key value pairs. A sequence of client data is preferably organized in row format. Alternatively the client data may have any suitable format. The client data agent is preferably a daemon providing a gateway to the data intensive infrastructure. The client data agent is preferably a plugin. For example the client data agent may be a ruby gem but may alternatively be a web service a library a script an application programming interface API or any suitable agent with an interface to client systems and a communication channel to the storage system . Additionally the data agent may be integrated into multiple portions of an application as shown in . The client data agent preferably includes networking interface to periodically communicate with the storage system . The client data agent may additionally include a data compressor that functions to compress event data before communicating to the storage system . The data compressor preferably is a data binary serializer and preferably converts data to MessagePack or any suitable data interchange format. MessagePack is a small and compact binary serialization format with a JSON like syntax. Additionally the client data agent may include configuration to identify the client user account or application when communicating with the remote storage system. A private key may be used to uniquely identify event data and distinguish the event data from other clients using the multitenant system.

The client data agent may additionally include a pluggable architecture to enable arbitrary data sources to be used in the client data agent. The pluggable architecture preferably includes an input plugin interface and an output plugin interface. The input plugin interface is preferably configured to allow plugins from third party developers or any suitable entity to be developed to pull data from any suitable source. For example an input plugin may be written to pull data in from a particular site. The output plugin interface is preferably configured to allow plugins from third party developers or any suitable entity to be developed to customize the output of data from the client data agent. The output plugin may translate data customize the transmission of data or provide any customized functionality for the output of the client data agent . In one example the output plugin may convert the data units or language. In another example the output plugin may be used to transmit the data to multiple destinations.

The remote distributed storage system of a preferred embodiment functions to store the event data. The event data coming into the distributed storage system is preferably discrete e.g. organized by time based events and substantially unorganized. The storage system in combination with the query processing cluster enables organized big data analysis to be output from the system. Big data may be characterized as greater than a terabyte of data a petabyte of data or any suitable size. In one exemplary application big data is characterized by greater than 1 terabyte of data storing a billion web clicks but may alternatively be characterized by any suitable sized data. The storage system is preferably hosted on a distributed cloud storage system such as Amazon s S3 EC2 but may alternatively be any suitable infrastructure. The data is preferably kept in a compressed format e.g. MessagePack format . The remote distributed storage system preferably functions to store data for both real time querying and for large batch data querying but may alternatively partition and group data in any suitable way.

The remote distributed storage system preferably includes a real time storage system and an archive storage system . The remote distributed storage system is preferably structured to initially store discrete data sent from the client data agent in the real time storage system and after a temporal delay a data converter merges or otherwise prepares the data for storage in the archive storage system . As shown in the real time storage system preferably stores the client data as row data and the archive storage system preferably stores the data in a columnar data format. The data is preferable merged and converted to columnar style using a MapReduce job. Row file format has the benefit of being easy to append new rows add new keys and is suitable for writing data. Columnar file format has the benefit of enabling unnecessary keys and values to be skipped can be highly compressed due to coherency and is suitable for archiving and reading. The columnar data format additionally can provide optimizations and efficiencies when querying the large data sets of the archive storage system. If the system is implemented for multitenant application the data may additionally be stored according to an account identifier. The data may additionally be encrypted or suitable secured. Data for different users is preferably kept isolated from access from other unauthorized user accounts.

The query processing duster of a preferred embodiment functions to convert a query instruction communicated through a query interface to a data processing method. Depending on the query instruction real time data and or archived data may be processed. The query processing cluster is preferably configured on a remote distributed computing infrastructure such as Amazon s Elastic Compute Cloud. The query processing cluster may be any suitable collection of a plurality of processing units virtual or physical that are preferably distributed but may be co located or positioned in any suitable configuration. In a multitenant implementation of the system queries from a plurality of different user accounts are preferably distributed amongst the shared resources of the query processing cluster . The query processing cluster is preferably a data intensive processing cluster such as the data platform commonly known as HADOOP . The data intensive processing duster is preferably configured to execute data mapping and reduction processes in querying data. More preferably the query processing cluster is a data platform commonly known as HADOOP cluster implementing MapReduce processes. A single query processing module preferably queries the real time storage system and the archive storage system . More preferably a query processing module of the query processing duster preferably queries the real time storage system and the archive storage system in a cooperative and coordinated manner to return a unified cohesive query result that incorporates real time and archived data.

In an alternative embodiment the query processing cluster may include a real time query module and a batch query module. A real time query module preferably performs queries on the real time storage system to provide time relevant time analysis. The real time query module may use a real time optimized data platform commonly known as HADOOP base for performing queries. A batch query module functions to perform big data queries on the archived storage system . The batch query module preferably utilizes a data platform commonly known as HADOOP infrastructure to facilitate working with the big data. The archived storage system is optimized to provide large volumes of data in a format efficient for query processing.

The query interface of a preferred embodiment functions to provide a user access to the stored data. A query interface is preferably used by a user to obtain information about data captured by the client data agent . The query interface may be used for requesting information but may also be used for interacting with the data such as creating a database or table importing data scheduling a query setting a schema for the data or any suitable action. The query interface takes user actions and preferably communicates them to the query processing cluster . In one variation the query interface is a command line interface. A command line interface preferably enables users to issue relational database like query commands to interact with data. The command line interface is preferably modeled to be similar to that of working with a relational database such as the database commonly known as MYSQL even though the backend infrastructure is drastically different. This functions to allow users without in depth knowledge of the data intensive infrastructure e.g. the data platform commonly known as HADOOP to issue to control and interact with the data using familiar commands as shown in . In a second variation the query interface is a web console interface. The web console is preferably website or application providing a graphical user interface. Data visualizations analytics reports and any suitable form of data representation may be part of such an interface. In a third variation the query interface may be an API used by third party applications. In a fourth variation the query interface may be business intelligence tool driver. For example a Java Database Connectivity JDBC or Open Database Connectivity ODBC interface can enable access from various business intelligence tools such as Jasper Soft Tableau Excel Pentaho or any suitable tool. Any of the above variations or other suitable query interfaces may be used in any suitable combination.

As shown in a method for integrating with a data intensive infrastructure of a preferred embodiment includes retrieving discrete client data S storing discrete client data in a remote storage system S wherein storing the data includes storing client data in a real time storage system S and storing client data in a columnar format in an archive storage system S receiving a data request through a user interface S and selectively interacting with the data from the real time storage system and the archive storage system S. The method functions to enable applications built on any infrastructure to benefit from the data processing capabilities of a big data specialized infrastructure. The method preferably leverages the columnar based distributed archive storage system and a data intensive processing cluster e.g. the data platform commonly known as HADOOP to provide fast and scalable data analytics and services. The method takes unorganized discrete event recordings of a wide range of client types and transforms the data into queryable organized data. The method is preferably used in big data applications e.g. datasets larger than terabyte or in some cases a petabyte . The method is preferably utilized by a data processing service provider. The service provider preferably enables a plurality of clients to utilize the infrastructure in a multi tenant environment. The method is preferably implemented on a system as described above but any suitable system may alternatively implement the method.

Step S which includes retrieving discrete client data functions to receive small pieces of client data from a client application. The discrete client data is preferably collected by a plug in library or application programming interface integrated with a client application. The client data is preferably event data including a timestamp associated with the parameters of the client data. Client data is preferably received at a distributed storage system. More preferably a data import module receives the data from a client data agent through an application programming interface API and appropriately adds the client data to the storage system. A client agent integrated with the client application preferably retrieves events from a data source and more preferably the events are retrieved as the events occur. The client data may alternatively be any suitable form of data. An application of a client can preferably easily integrate with such a client agent without making system level changes to the application. In the case of a Ruby on Rails application this may include adding a data agent gem and performing some initial setup such as specifying a schema. The data agent can then relay appropriate information. Retrieving discrete client data may include at a client data agent retrieving logs converting relational databases of an application receiving event notifications retrieving online data retrieving sensor data and or performing any suitable action to obtain data. The client agent may additionally interface with an input plugin. The input plugin may be configured to retrieve any suitable form of data. In some situations a plurality of sources are used to retrieve discrete client event data. Client event data is preferable key value pair data characterizing an event or an object to be stored.

Additionally retrieving client event data may include compressing client event data. Compressing preferably includes serializing the event data which may comprise of transforming the data from a human readable format to a retrievable binary serialization in a JSON like format. MessagePack library is one suitable data interchange format. As the data agent communicates event data to a storage system the data agent may additionally buffer communication to avoid overloading a data importing system of the storage system. The method may additionally enable dynamic schema. Dynamic schema functions to enable flexibly setting data schema. One possible benefit is that schema information for client data is not required prior to adding data. A generic schema may be applied to data by default. The schema of data may later be updated through a data query request and interacting with the data which can result in enhanced performances for subsequent queries.

Step S which includes storing discrete client data in a remote storage system functions to transfer data from an application to storage infrastructure of a big data specialized system. A client agent preferably periodically communicates client event data to the storage system. Alternatively the client agent can continuously communicate client event data to the storage system. As discussed above the event data is preferably in a compressed format for communication. Storing the event data in the storage system preferably enables the event data to be organized for optimized analytics. The client data is preferably stored as time series data. The data may include a timestamp for the collection of the data for the storing in real time storage system for the storing in archive storage and or any suitable timestamp.

As two preferred sub steps storing client event data additionally includes storing client event data in a real time storage system S and storing client data in a columnar format in an archive storage system S which function to store data in formats suitable for real time applications and big data applications respectively. The storage system preferably initially stores event data in a real time storage system. Preferably after some temporal delay data is moved from the real time data storage system to the archived storage system though event data may alternatively be replicated within sub storage systems. Storing in these two sub storage systems enables data to be formatted for different applications. Real time data may have more queries to look at events as a whole. The event data is preferably stored in row format in the real time storage system. Row format preferably simplifies appending a new row to the data adding new keys and is preferably suitable for writing. Initially storing in the real time data storage additionally acts as a buffer as data is merged or otherwise converted into a structure suitable for archived storage. The event data is preferably converted to a MapReduce format for storing in the archive storage system which functions to make the archived event data to be more accessible for batch queries. In converting to a MapReduce format the event data is preferably stored in a columnar format. Columnar format preferably enables unnecessary keys and values to be skipped when reading. Columnar format additionally has improved compression rate and is suitable for archiving and reading. Conversion to the MapReduce format preferably occurs within the storage system to alleviate clients from performing the difficult conversion process.

Step S which includes receiving a data request through a user interface functions to obtain instructions indicating the intensions of a user or system. The data request is preferably obtained through an interface and then subsequently communicated to a query processing cluster such as a data platform commonly known as HADOOP enabled cluster. The data request is preferably communicated through a user interfaces such as a command line interface a web console an API a business intelligence tool interface or through any suitable mechanism. Preferably a data request is a query request that requires a response of corresponding structured data from the storage system. Alternatively a data request may be an instruction requesting the modification to the stored data such as by creating a database or table setting a schema importing data interactively or performing any suitable action. In one variation data requests are received through a command line interface and the requests may additionally be converted from a relational database like query command format to a format compatible for the data storage and query processing cluster. For example SQL like database commands are preferably converted to a MapReduce format suitable for use with the data platform commonly known as HADOOP . In another variation data requests are received through a business intelligence tool interface. The business intelligence tool interface is preferably a driver for a Java Database Connectivity JDBC or Open Database Connectivity ODBC interface. Such business intelligence tool interfaces preferably enables data access and interactions from various business intelligence tools such as Jasper Soft Tableau Excel Pentaho or any suitable tool.

Step S which includes selectively interacting with the data from the real time storage system and the archive storage system functions to perform the data request. For data queries a query processing cluster preferably selectively queries the real time storage system and or the archived storage system. When the query is for real time information the real time storage system is preferably queried. When the query is a batch query the archived storage system is queried. Data in the archived storage system is preferably processed quicker and more efficiently because the data is preferably stored in a columnar format. A query mechanism preferably allows part of the data to be selectively queried as opposed to querying the whole columnar dataset. For example leveraging the columnar format and the time series configuration of the archive data a range get operation for a query on a single parameter of client event data point can be retrieved substantially faster and efficiently than retrieving such data arranged in a row format. The system may alternatively select all of a columnar dataset. The query processing cluster may additionally analyze the query request and limit the queried data. Such query oversight can preferably provide performance improvements. In the variation where the data request is setting a schema the query processing cluster preferably canonizes the data using the specified schema. Additionally the query processing cluster may decompress the data for delivery to the user. For example if data was compressed using MessagePack the data is preferably converted to the original format. Additionally the results or sub calculations of a data request may be cached for future data requests. The method may include the step of returning structured data which functions to return results of a data query. The data results are preferably communicated through the query interface used to make the data request. The results may be represented in an infographic text based result summary as table styled data a spreadsheet a csv data file or in any suitable data format.

In querying data the selective interaction preferably includes cooperatively querying the real time storage system and the archive storage system which functions to distribute the query over multiple storage systems to retrieve a single cohesive query result. Query processing cluster preferably leverages the timestamps of the various client data in the real time storage system and the archive storage system to query non redundant client data. When client data is being merged or otherwise structured for storage in the archive storage system merge status information preferably indicates what client data has been added to the archive storage system. The merge status information may be a merge log an indicator of latest merge block or any suitable parameter s that can indicate the redundancy of data between the real time storage and the archive storage. Preferably this is a log of time blocks that have completed merging in the progress of merging and or awaiting merging. The query cluster preferably uses the merge status information and client data timestamps to appropriately skip data from either the real time storage system or the archive storage system if the skipped data will be accounted for in the other storage system as shown in .

An alternative embodiment preferably implements the above methods in a computer readable medium storing computer readable instructions. The instructions are preferably executed by computer executable components preferably integrated with a data agent a storage system a query processing cluster and an interface. The computer readable medium may be stored on any suitable computer readable media such as RAMs ROMs flash memory EEPROMs optical devices CD or DVD hard drives floppy drives or any suitable device. The computer executable component is preferably a processor but the instructions may alternatively or additionally be executed by any suitable dedicated hardware device.

As a person skilled in the art will recognize from the previous detailed description and from the figures and claims modifications and changes can be made to the preferred embodiments of the invention without departing from the scope of this invention defined in the following claims.

