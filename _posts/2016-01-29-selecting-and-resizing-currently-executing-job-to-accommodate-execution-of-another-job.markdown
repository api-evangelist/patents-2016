---

title: Selecting and resizing currently executing job to accommodate execution of another job
abstract: A job execution scheduling system and associated methods are provided for accommodating a request for additional computing resources to execute a job that is currently being executed or a request for computing resources to execute a new job. The job execution scheduling system may utilize a decision function to determine one or more currently executing jobs to select for resizing. Resizing a currently executing job may include de-allocating one or more computing resources from the currently executing job and allocating the de-allocated resources to the job for which the request was received. In this manner, the request for additional computing resources is accommodated, while at the same time, the one or more jobs from which computing resources were de-allocated continue to be executed using a reduced set of computing resources.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09448842&OS=09448842&RS=09448842
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09448842
owner_city: Armonk
owner_country: US
publication_date: 20160129
---
Certain machine computer executable applications are capable of being executed in parallel by different processing resources. For example a first portion of an application may execute on a first processing resource at least partially concurrently with execution of a second portion of the application on a second processing resource. A resource manager may be configured to allocate a set of processing resources to collectively execute an application. Conventional resource managers suffer from a number of drawbacks. Technical solutions that address at least some of the drawbacks associated with conventional resource managers are described herein.

In one or more example embodiments of the disclosure a method for scheduling execution of parallel or distributed applications is disclosed that includes receiving by a scheduling system comprising one or more computer processors a request to allocate a set of one or more computing resources for execution of a first executable application selecting by the scheduling system a second executable application to resize to accommodate the request to allocate the set of one or more computing resources wherein at least a portion of the second executable application is currently executing on a first computing resource of the set of one or more computing resources causing by the scheduling system the second executable application to be resized at least in part by sending a first signal to the first computing resource to cease execution of the at least a portion of the second executable application on the first computing resource and sending by the scheduling system a second signal to the first computing resource to initiate execution of at least a portion of the first executable application on the first computing resource.

In one or more other example embodiments of the disclosure a system for scheduling execution of parallel or distributed application is disclosed that includes at least one memory storing computer executable instructions and at least one processor configured to access the at least one memory and execute the computer executable instructions to receive a request to allocate a set of one or more computing resources for execution of a first executable application select a second executable application to resize to accommodate the request to allocate the set of one or more computing resources wherein at least a portion of the second executable application is currently executing on a first computing resource of the set of one or more computing resources cause the second executable application to be resized at least in part by sending a first signal to the first computing resource to cease execution of the at least a portion of the second executable application on the first computing resource and send a second signal to the first computing resource to initiate execution of at least a portion of the first executable application on the first computing resource.

In one or more other example embodiments of the disclosure a computer program product for scheduling execution of parallel or distributed applications is disclosed that comprises a non transitory storage medium readable by a processing circuit the storage medium storing instructions executable by the processing circuit to cause a method to be performed the method comprising receiving a request to allocate a set of one or more computing resources for execution of a first executable application selecting a second executable application to resize to accommodate the request to allocate the set of one or more computing resources wherein at least a portion of the second executable application is currently executing on a first computing resource of the set of one or more computing resources causing the second executable application to be resized at least in part by sending a first signal to the first computing resource to cease execution of the at least a portion of the second executable application on the first computing resource and sending a second signal to the first computing resource to initiate execution of at least a portion of the first executable application on the first computing resource.

Example embodiments of the disclosure include among other things systems methods computer readable media techniques and methodologies for accommodating a request for additional computing resources to execute a job that is currently being executed or a request for computing resources to execute a new job by de allocating one or more computing resources from one or more jobs that are currently executing and re allocating the de allocated computing resources to the job associated with the received request while at the same time continuing to execute the one or more jobs from which computing resource s were de allocated using a set of reduced computing resources. As used herein a job may include without limitation a machine computer executable application a machine computer executable process a thread or any collection of machine computer executable instructions. A processing node may be any suitable processing unit including without limitation a processing circuit with multiple processors processing cores a single core processor or the like. Further as used herein a computing resource may refer to a processing node or additional execution time on a processing node.

More specifically in certain example embodiments of the disclosure a client device may request additional computing resources for a job that is currently being executed on one or more processing nodes. A job execution scheduling system in accordance with example embodiments of the disclosure may utilize a decision function to determine one or more currently executing jobs to select for resizing. Resizing a currently executing job may include for example de allocating one or more computing resources from the currently executing job. The de allocated resources may then be allocated to the job for which the request was received. For example for a first job that is executing concurrently on first and second processing nodes the first job may be resized by ceasing execution of a portion of the first job that is executing on the first processing node such that execution of at least a portion of a second job for which additional computing resources were requested can be initiated on the first processing node. In this manner the request for computing resources for the second job can be accommodated while at the same time execution of the first job can continue on the second processing node. The resizing of a job may result in a longer execution time for the job. Thus in certain example embodiments jobs selected for resizing may be those for which the difference between remaining execution time prior to resizing and the remaining execution time after resizing is minimized.

In addition in certain example embodiments a client device may request computing resources for a new job that is not currently being executed. The new job may be for example a high priority job having an assigned priority that is greater than the respective priorities of one or more other jobs that are currently executing. Using a decision function a currently executing job having a lower priority than the new job may be selected for resizing to accommodate execution of the new job.

A job execution scheduling system in accordance with example embodiments of the disclosure eliminates a number of drawbacks associated with conventional computing resource managers. Conventional resource managers manage for example execution of tightly coupled applications that have a high level of interdependence between constituent processes e.g. message passing interface MPI applications and or distributed applications e.g. MapReduce which is a programming model and associated implementation for processing and generating large datasets with a parallel distributed algorithm on a cluster by allocating for each application in an application queue a respective set of computing resources for execution of the application. With conventional resource managers once a set of resources has been allocated to an application and execution of the application has been initiated using the set of resources modifications to the set of resources are not permitted during execution of the application.

As such in conventional resource management two types of approaches may be taken if computing resources that have already been allocated are requested e.g. by a currently executing job or a new job . Under one approach the requested resources are not allocated to the requesting job until execution of an application currently using the requested resources is complete. Under the other approach the application that is currently using the requested resources is preempted e.g. execution of the application is completely halted on the requested resources and the requested resources are instead allocated to the requesting job.

In contrast in accordance with example embodiments of the disclosure a request for additional computing resources for a currently executing job or an urgent request to execute a new job e.g. a high priority job can be accommodated without requiring the requesting job to wait until computing resources are available and without halting execution of any currently executing jobs. This is accomplished by selecting a currently executing job for resizing resizing the selected job by de allocating one or more computing resources from the selected job and allocating the de allocated computing resource s to the requesting job while at the same time continuing execution of the selected job on a reduced set of computing resources.

As previously noted tightly coupled applications have a high level of interdependence between constituent processes. Thus if execution of a particular process is halted or slows down other processes are impacted. As a result the processes of a tightly coupled application are managed as a unit. In contrast loosely coupled applications involve a limited number of communications between constituent processes. Consequently failure of a particular process of a loosely coupled application has a minimal effect on other processes and on execution of the application as a whole. Therefore constituent processes of a loosely coupled application can be managed independently of each other. A job execution scheduling system in accordance with example embodiments of the disclosure provides the capability to manage the processes of a tightly coupled application while still allowing those processes to be resized to accommodate requests for additional computing resources from other processes and or requests to execute high priority jobs. Conventional resource managers on the other hand do not permit modification to a set of computing resources allocated for example for execution of processes of a tightly coupled application and thus do not provide such a capability.

Additional technical advantages of a job execution scheduling system in accordance with example embodiments of the disclosure over conventional resource managers scheduling systems include the capability to schedule execution of applications associated with different frameworks e.g. tightly coupled applications distributed analytic applications etc. concurrently the capability to resize an application to an arbitrary number of processors the capability to resize an application while taking into account whether the application prioritizes data locality or processor locality and so forth. In addition a job execution scheduling system in accordance with example embodiments of the disclosure is configured to manage job execution for a distributed system and thus is configured handle issues that an operating system OS scheduler for a single system is not such as for example node failure synchronization of checkpoint intervals staleness of load information about nodes distributed cache effects rack aware scheduling or the like.

The client device may be communicatively coupled to a processing node cluster . The processing node cluster may include processing nodes . A respective set of one or more of the processing nodes may be allocated for execution of each of the executing jobs . Further a respective set of one or more of the processing nodes may be scheduled for future execution of each of one or more of the queued jobs . For example the executing job may be currently executing on processing nodes of the set of processing nodes .

At some point in time the client device may send a request to the processing node cluster for additional resources to execute job . The request may include a request for one or more additional processing nodes and or a request for additional execution time. A job execution scheduling system in accordance with example embodiments of the disclosure may accommodate the request by allocating one or more additional processing nodes for execution of job . The request may be accommodated by resizing one or more other executing jobs of the set of executing jobs . For example two additional processing nodes may be de allocated from one or more other executing jobs and re allocated for execution of the executing job resulting in an expanded set of processing nodes now allocated for execution of the job .

The scheduler module may for example include computer executable instructions code or the like that responsive to execution by a processing circuit cause operations to be performed for accepting incoming jobs and a resize request utilizing a decision function to select one or more executing jobs for resizing and transmitting one or more resize commands e.g. signals to the dispatcher module .

The incoming jobs may be placed for example in a job queue . The resize request may be a request for additional computing resources for execution of a job that is currently being executed or may be a request to execute a new high priority job e.g. one of the incoming jobs . The dispatcher module may be communicatively coupled to a set of processing nodes N N may be any integer greater than or equal to 1 . Any of the processing node s N may be referred to herein generically as processing node . Upon receiving a resize command from the scheduler module computer executable instructions code or the like of the dispatcher module may be executed to communicate the resize command to the appropriate processing node . For example processing nodes and may be currently executing job X. If the resize command indicates that processing node should be made available to execute at least a portion of job Y which may be another currently executing job or a new high priority job the dispatcher module may send a signal to processing node to cease execution of job X at a particular point in time and initiate execution of the at least a portion of job Y. Also more generally the dispatcher module may be configured to relay commands received from the scheduler module to appropriate processing nodes to initiate execution of new jobs from the job queue checkpoint currently executing jobs e.g. completely halt execution of a job save an execution state of the job and reinitiate execution at a later point in time from the saved execution state potentially on one or more different processing nodes and so forth.

Referring now to other illustrative components of the job execution scheduling system each of the job monitoring module and the resource management module may also be communicatively coupled to the set of processing nodes N . The job monitoring module may include computer executable instructions code or the like that responsive to execution by a processing circuit may cause operations to be performed for monitoring the progress of jobs that are currently being executed on the set of processing nodes N . For example the job monitoring module may be configured to determine the remaining execution time for a job executing on one or more of the processing nodes N . The resource management module may include computer executable instructions code or the like that responsive to execution by a processing circuit may cause operations to be performed for tracking the health and utilization of the processing nodes N . For example the resource management module may track which processing node s are being used to execute a particular job.

Referring to at block the job execution scheduling system or more specifically for example the scheduler module may determine a set of jobs and corresponding job states. For example the job execution scheduling system may determine for each job whether the job is in a job queue and awaiting execution whether the job is currently being executed and if so which processing node s the job is executing on whether the job has been checkpointed and so forth.

At block the job execution scheduling system may determine resource requirements for executing the set of jobs. For example the job scheduling system may determine at block the number of processing nodes and the execution time required to complete execution of each job including for instance jobs that currently being executed as well as jobs that in the job queue awaiting execution. At block the job execution scheduling system may sort currently executing jobs based on their respective execution priorities. For example the job execution scheduling system may order the currently executing jobs based on priority such that a least priority job that is eligible for resizing or checkpointing is selected for resizing or checkpointing first. In other example embodiments the currently executing jobs may be ordered based differences between a current remaining execution time and a remaining execution time if the jobs are resized or checkpointed. In such example embodiments a job for which the difference between the current remaining execution time and a remaining execution time if the job is resized or checkpointed is minimized may be selected first for resizing or checkpointing. In yet other example embodiments both priority of a job and an execution time difference may be considered when sorting jobs.

Blocks and represent an iterative process that the job execution scheduling system may perform to construct a set of candidate jobs for resizing or checkpointing. At block the job execution scheduling system may select a job from the set of jobs identified at block . At block the job execution scheduling system may determine whether the selected job is data parallel or can be checkpointed. Determining whether a job is data parallel may include determining whether the job is currently being executed on multiple processing nodes and thus whether the job is capable of being resized. Determining whether a job is capable of being checkpointed may include determining whether the execution of the job can be completely halted and resumed at a later point in time potentially on one or more different processing nodes.

In response to a positive determination at block the job execution scheduling system may add the selected job to a set of candidate jobs for resizing or checkpointing at block . On the other hand in response to a negative determination to block the method may proceed to block where the job execution scheduling system may determine whether resource requirements have been satisfied for all jobs or whether all jobs have been iterated through to determine candidacy for resizing or checkpointing. In response to a negative determination at block the method may again proceed to block where another job may be selected. A negative determination may be made at block if for example the resource requirements for at least one job have not been satisfied e.g. processing node s need to be allocated for at least one job or even if the resource requirements for all jobs have not been satisfied all jobs have been iterated through to determine their candidacy for resizing or checkpointing. In response to a positive determination at block on the other hand the method may proceed to block of or block of depending on whether a request for computing resources is received from a currently executing job or from a new high priority job.

It should be appreciated that what is obtained after performance of method is a set of jobs that are candidates for resizing or checkpointing. The candidate jobs may be ordered in accordance with their execution priority differences between pre resizing and post resizing execution times differences between pre checkpointing and post checkpointing execution times or any combination thereof.

Referring now to in conjunction with one another at block the scheduler module may receive the request for additional computing resources for a first executing job at time t. In the example depicted in the first executing job is job J and the additional computing resources being requested are an additional processing node and 10 minutes of additional execution time.

At block the scheduler module may determine using a decision function one or more other currently executing jobs to resize. An example decision function may be one that seeks to maximize throughput e.g. the number of jobs executed within a time interval or in other words minimize the execution time of jobs since throughput is the inverse of execution time. In certain example embodiments each job may have a corresponding deadline associated therewith that indicates a period of time in which execution of the job is to be completed. If a job deadline is violated the scheduler module does not consider the job for further resizing and prioritizes the job for completion.

Given a set of jobs J j j . . . j that are currently being executed on processing nodes e.g. the processing nodes under the control of the scheduler module where e r and drepresent the estimated execution horizon the current time and the deadline for job j respectively then the decision function utilized to determine the one or more executing jobs to resize may be as follows. For each jin J j where jis the job for which computing resources are being requested if jis eligible for resizing the estimated execution horizon after resizing e is computed. If e d then the deadline associated with job jis violated by resizing and the decision function selects a next job from the set of candidate jobs. If e d then the remaining execution time for job j t is computed where t e r. The remaining execution time for job jafter resizing t is then computed where t e r. The jobs may then be sorted in increasing order of t tand iterated through to resize one or more jobs until a sufficient amount of computing resources are freed up to satisfy the resource request .

In particular the scheduler module may generate a decision function output and communicate the output to the dispatcher module to cause at block computing resources to be de allocated from the one or more other executing jobs e.g. one or more candidate jobs that are selected in increasing order of t t and allocated at block to the first executing job. In the example shown in in response to the resource request job J may be resized by de allocating processing node from job J at time t e.g. ceasing execution of J on processing node at time t . From time tuntil execution completion job J may be in an execution state B in which job J executed only on processing node . In addition processing node that has now been made available may initiate execution of a portion of job J at time tsuch that from time tuntil execution completion job J is in an execution state B in which job J is executing on both processing node and processing node . Further job J may be executed on processing nodes and for a longer period of time to accommodate the request for additional execution time. Moreover job J which is initially scheduled for execution on processing nodes and may be rescheduled to a future execution state B in which job J is executed only on processing node . Alternatively the scheduler module may allocate processing node for execution of a different job than job J.

It should be noted that a conventional scheduler would not be able to accommodate the request for additional computing resources for job J. Instead in a conventional scheduling scenario a user would be required to kill job J and reschedule it with more processing resources and execution time. The conventional resource manager would then determine when rescheduled job J would be executed which could potentially result in significant delays in execution of job J if higher priority jobs are awaiting execution in the job queue.

The resource request may be received at time t. The resource request may be a request to execute a new high priority job J and may indicate requested resources e.g. a number of processing node s and an amount of execution time requested for job J . In the conventional scenario depicted in the resource request may only be accommodated after free computing resources become available at time t. For instance all currently executing jobs may be permitted to continue execution without preemption and job J may only enter execution state and begin execution on processing node upon completion of execution of J on processing node . In this example job J may be determined to have a higher priority than job J and thus execution of job J may be delayed until a future time. That is job J may become associated with a future execution state B that is later in time than execution state A and that includes an allocation of different processing nodes e.g. processing nodes and instead of processing nodes and .

The resource request may be received at time t. The resource request may be a request to execute a new high priority job J and may indicate requested resources e.g. a number of processing node s and an amount of execution time requested for job J . In the conventional preemption scenario depicted in the resource request may be accommodated by preempting job J at time t. In particular job J may be checkpointed at time t. More specifically job J may be halted at time tand resumed on a different processing node e.g. node at a later point in time e.g. after job J completes execution on processing node as part of future execution state B. Halting execution of job J at time tfrees up processing node to be used to initiate execution of job J as part of execution state . Execution of job J may then be delayed to future execution state B at which point both processing node and processing node are available.

Referring now to in conjunction with one another at block the scheduler module may receive at time t the request to execute a new job. The resource request may specify a number of processing node s being requested and an amount of execution time being requested. In the example depicted in the new job is job J and the resources being requested are a processing node and 15 minutes of execution time.

At block the scheduler module may determine that the new job is a high priority job. For example scheduler module may determine that a priority associated with the new job is higher than a respective priority associated with each of one or more currently executing jobs. At block the scheduler module may determine the resource requirements of the new job. For example in the example depicted in the scheduler module may determine the resource requirements from the resource request .

At block the scheduler module may determine using a decision function one or more other currently executing jobs to resize. The example decision function described earlier or any other suitable decision function may be used. In certain example embodiments the one or more other executing jobs selected for resizing may each have a lower execution priority than the new job e.g. job J .

At block the scheduler module may generate a decision function output and communicate the output to the dispatcher module to cause computing resources to be de allocated from the one or more other executing jobs selected for resizing and allocated at block to the new job. In the example shown in in response to the resource request job J may be resized by de allocating processing node from job J at time t e.g. ceasing execution of J on processing node at time t . As such from time tuntil execution completion job J may be in an execution state B in which job J executed only on processing node . In addition processing node that has now been made available may initiate execution of a portion of job J at time tsuch that from time tuntil execution completion job J is in an execution state in which job J is executing on processing node . In the example shown in job J is accommodated by resizing job J and without having to resize jobs J J or J. The technique for accommodating job J depicted in eliminates the drawback of delayed execution of job J in the non preemptive scenario of and the delayed execution of jobs J and J in the preemptive scenario of .

Example embodiments of the disclosure include or yield various technical features technical effects and or improvements to technology. For instance example embodiments of the disclosure provide the technical effect of accommodating a request for additional computing resources from a currently executing job and or a request to execute a high priority job without having to wait until another currently executing job is completed and without having to preempt execution of another currently executing job. This technical effect is achieved as a result of the technical features of selecting one or more currently executing jobs to resize using a decision function and resizing the selected job s to enable accommodation of the received request while at the same time continuing execution of the selected job s using a reduced set of computing resources and thus ensuring a high degree of utilization of processing nodes. In addition by virtue of the technical features noted above example embodiments of the disclosure also provide the technical effect of being above to resize processes of a tightly coupled application to an arbitrary number of processing nodes which a conventional resource manager is incapable of. Thus example embodiments of the disclosure provide the technical effect of enabling dynamic accommodation of both tightly coupled and loosely coupled applications on the same processing infrastructure. As a result of the aforementioned technical features and technical effects example embodiments of the disclosure constitute an improvement to existing computing resource management technology. It should be appreciated that the above examples of technical features technical effects and improvements to technology of example embodiments of the disclosure are merely illustrative and not exhaustive.

One or more illustrative embodiments of the disclosure have been described above. The above described embodiments are merely illustrative of the scope of this disclosure and are not intended to be limiting in any way. Accordingly variations modifications and equivalents of embodiments disclosed herein are also within the scope of this disclosure.

In an illustrative configuration the job execution scheduling server may include one or more processors processor s one or more memory devices generically referred to herein as memory one or more input output I O interface s one or more network interfaces and data storage . The may further include one or more buses that functionally couple various components of the job execution scheduling server .

The bus es may include at least one of a system bus a memory bus an address bus or a message bus and may permit exchange of information e.g. data including computer executable code signaling etc. between various components of the job execution scheduling server . The bus es may include without limitation a memory bus or a memory controller a peripheral bus an accelerated graphics port and so forth. The bus es may be associated with any suitable bus architecture including without limitation an Industry Standard Architecture ISA a Micro Channel Architecture MCA an Enhanced ISA EISA a Video Electronics Standards Association VESA architecture an Accelerated Graphics Port AGP architecture a Peripheral Component Interconnects PCI architecture a PCI Express architecture a Personal Computer Memory Card International Association PCMCIA architecture a Universal Serial Bus USB architecture and so forth.

The memory of the job execution scheduling server may include volatile memory memory that maintains its state when supplied with power such as random access memory RAM and or non volatile memory memory that maintains its state even when not supplied with power such as read only memory ROM flash memory ferroelectric RAM FRAM and so forth. Persistent data storage as that term is used herein may include non volatile memory. In certain example embodiments volatile memory may enable faster read write access than non volatile memory. However in certain other example embodiments certain types of non volatile memory e.g. FRAM may enable faster read write access than certain types of volatile memory.

In various implementations the memory may include multiple different types of memory such as various types of static random access memory SRAM various types of dynamic random access memory DRAM various types of unalterable ROM and or writeable variants of ROM such as electrically erasable programmable read only memory EEPROM flash memory and so forth. The memory may include main memory as well as various forms of cache memory such as instruction cache s data cache s translation lookaside buffer s TLBs and so forth. Further cache memory such as a data cache may be a multi level cache organized as a hierarchy of one or more cache levels L1 L2 etc. .

The data storage may include removable storage and or non removable storage including but not limited to magnetic storage optical disk storage and or tape storage. The data storage may provide non volatile storage of computer executable instructions and other data. The memory and the data storage removable and or non removable are examples of computer readable storage media CRSM as that term is used herein.

The data storage may store computer executable code instructions or the like that may be loadable into the memory and executable by the processor s to cause the processor s to perform or initiate various operations. The data storage may additionally store data that may be copied to memory for use by the processor s during the execution of the computer executable instructions. Moreover output data generated as a result of execution of the computer executable instructions by the processor s may be stored initially in memory and may ultimately be copied to data storage for non volatile storage.

More specifically the data storage may store one or more operating systems O S one or more database management systems DBMS configured to access the memory and or one or more datastores and one or more program modules applications engines computer executable code scripts or the like such as for example a scheduler module a job monitoring module a dispatcher module and a resource management module . Any of the components depicted as being stored in data storage may include any combination of software firmware and or hardware. The software and or firmware may include computer executable code instructions or the like that may be loaded into the memory for execution by one or more of the processor s to perform any of the operations described earlier in connection with correspondingly named modules.

Although not depicted in the data storage may further store various types of data utilized by components of the job execution scheduling server e.g. any of the data depicted as being stored in the datastore s . Any data stored in the data storage may be loaded into the memory for use by the processor s in executing computer executable code. In addition any data depicted as being stored in the data storage may potentially be stored in one or more of the datastore s and may be accessed via the DBMS and loaded in the memory for use by the processor s in executing computer executable instructions code or the like.

The processor s may be configured to access the memory and execute computer executable instructions loaded therein. For example the processor s may be configured to execute computer executable instructions of the various program modules applications engines or the like of the job execution scheduling server to cause or facilitate various operations to be performed in accordance with one or more embodiments of the disclosure. The processor s may include any suitable processing unit capable of accepting data as input processing the input data in accordance with stored computer executable instructions and generating output data. The processor s may include any type of suitable processing unit including but not limited to a central processing unit a microprocessor a Reduced Instruction Set Computer RISC microprocessor a Complex Instruction Set Computer CISC microprocessor a microcontroller an Application Specific Integrated Circuit ASIC a Field Programmable Gate Array FPGA a System on a Chip SoC a digital signal processor DSP and so forth. Further the processor s may have any suitable microarchitecture design that includes any number of constituent components such as for example registers multiplexers arithmetic logic units cache controllers for controlling read write operations to cache memory branch predictors or the like. The microarchitecture design of the processor s may be capable of supporting any of a variety of instruction sets.

Referring now to other illustrative components depicted as being stored in the data storage the O S may be loaded from the data storage into the memory and may provide an interface between other application software executing on the job execution scheduling server and hardware resources of the job execution scheduling server . More specifically the O S may include a set of computer executable instructions for managing hardware resources of the job execution scheduling server and for providing common services to other application programs e.g. managing memory allocation among various application programs . In certain example embodiments the O S may control execution of one or more of the program modules depicted as being stored in the data storage . The O S may include any operating system now known or which may be developed in the future including but not limited to any server operating system any mainframe operating system or any other proprietary or non proprietary operating system.

The DBMS may be loaded into the memory and may support functionality for accessing retrieving storing and or manipulating data stored in the memory data stored in the data storage and or data stored in the datastore s . The DBMS may use any of a variety of database models e.g. relational model object model etc. and may support any of a variety of query languages. The DBMS may access data represented in one or more data schemas and stored in any suitable data repository.

The datastore s may include but are not limited to databases e.g. relational object oriented etc. file systems flat files distributed datastores in which data is stored on more than one node of a computer network peer to peer network datastores or the like. The datastore s may store various types of data including without limitation job data metadata job execution data and decision function data . The job data metadata may include without limitation data indicative of a set of jobs a corresponding job state for each job e.g. currently executing completed or awaiting execution in a job queue and resource requirements for each job. The job execution data may include without limitation data indicative of the current set of computing resources being used to execute each currently executing job data indicative of the set of computing resources allocated for future execution of queued jobs data indicative of modifications to computing resource allocations based on resizing of one or more executing jobs or the like. The decision function data may include without limitation data indicative of various decision functions that may be used to sort and select jobs for resizing or checkpointing. It should be appreciated that in certain example embodiments any of the datastore s and or any of the data depicted as residing thereon may additionally or alternatively be stored locally in the data storage .

Referring now to other illustrative components of the job execution scheduling server the input output I O interface s may facilitate the receipt of input information by the job execution scheduling server from one or more I O devices as well as the output of information from the job execution scheduling server to the one or more I O devices. The I O devices may include any of a variety of components such as a display or display screen having a touch surface or touchscreen an audio output device for producing sound such as a speaker an audio capture device such as a microphone an image and or video capture device such as a camera a haptic unit and so forth. Any of these components may be integrated into the job execution scheduling server or may be separate. The I O devices may further include for example any number of peripheral devices such as data storage devices printing devices and so forth.

The I O interface s may also include an interface for an external peripheral device connection such as universal serial bus USB FireWire Thunderbolt Ethernet port or other connection protocol that may connect to one or more networks. The I O interface s may also include a connection to one or more antennas to connect to one or more networks via a wireless local area network WLAN such as Wi Fi radio Bluetooth and or a wireless network radio such as a radio capable of communication with a wireless communication network such as a Long Term Evolution LTE network WiMAX network 3G network etc.

The job execution scheduling server may further include one or more network interfaces via which the job execution scheduling server may communicate with any of a variety of other systems platforms networks devices and so forth. The network interface s may enable communication for example with one or more other devices via one or more of the network s . The network s may include but are not limited to any one or more different types of communications networks such as for example cable networks public networks e.g. the Internet private networks e.g. frame relay networks wireless networks cellular networks telephone networks e.g. a public switched telephone network or any other suitable private or public packet switched or circuit switched networks. The network s may have any suitable communication range associated therewith and may include for example global networks e.g. the Internet metropolitan area networks MANs wide area networks WANs local area networks LANs or personal area networks PANs . In addition the network s may include communication links and associated networking devices e.g. link layer switches routers etc. for transmitting network traffic over any suitable type of medium including but not limited to coaxial cable twisted pair wire e.g. twisted pair copper wire optical fiber a hybrid fiber coaxial HFC medium a microwave medium a radio frequency communication medium a satellite communication medium or any combination thereof.

It should be appreciated that the modules depicted in as being stored in the data storage or depicted in more generally as part of the job execution scheduling system are merely illustrative and not exhaustive and that processing described as being supported by any particular engine or module may alternatively be distributed across multiple engines modules or the like or performed by a different engine module or the like. In addition various program module s script s plug in s Application Programming Interface s API s or any other suitable computer executable code hosted locally on the Job execution scheduling server and or hosted on other computing device s accessible via one or more of the network s may be provided to support functionality provided by the modules depicted in and or additional or alternate functionality. Further functionality may be modularized differently such that processing described as being supported collectively by the collection of modules depicted in may be performed by a fewer or greater number of program modules or functionality described as being supported by any particular module may be supported at least in part by another program module. In addition program modules that support the functionality described herein may form part of one or more applications executable across any number of job execution scheduling servers in accordance with any suitable computing model such as for example a client server model a peer to peer model and so forth. In addition any of the functionality described as being supported by any of the modules depicted in may be implemented at least partially in hardware and or firmware across any number of devices.

It should further be appreciated that the job execution scheduling server may include alternate and or additional hardware software or firmware components beyond those described or depicted without departing from the scope of the disclosure. More particularly it should be appreciated that software firmware or hardware components depicted as forming part of the job execution scheduling server are merely illustrative and that some components may not be present or additional components may be provided in various embodiments. While various illustrative modules have been depicted and described as software modules stored in data storage it should be appreciated that functionality described as being supported by the modules may be enabled by any combination of hardware software and or firmware. It should further be appreciated that each of the above mentioned modules may in various embodiments represent a logical partitioning of supported functionality. This logical partitioning is depicted for ease of explanation of the functionality and may not be representative of the structure of software hardware and or firmware for implementing the functionality. Accordingly it should be appreciated that functionality described as being provided by a particular module may in various embodiments be provided at least in part by one or more other modules. Further one or more depicted modules may not be present in certain embodiments while in other embodiments additional modules not depicted may be present and may support at least a portion of the described functionality and or additional functionality. Moreover while certain modules may be depicted or described as sub modules of another module in certain embodiments such modules may be provided as independent modules or as sub modules of other modules.

One or more operations of the methods may be performed by a job execution scheduling system that includes one or more job execution scheduling servers having the illustrative configuration depicted in or more specifically by one or more program modules engines applications or the like executable on such device s . It should be appreciated however that such operations may be implemented in connection with numerous other system configurations.

The operations described and depicted in the illustrative methods of may be carried out or performed in any suitable order as desired in various example embodiments of the disclosure. Additionally in certain example embodiments at least a portion of the operations may be carried out in parallel. Furthermore in certain example embodiments less more or different operations than those depicted in may be performed.

Although specific embodiments of the disclosure have been described one of ordinary skill in the art will recognize that numerous other modifications and alternative embodiments are within the scope of the disclosure. For example any of the functionality and or processing capabilities described with respect to a particular system system component device or device component may be performed by any other system device or component. Further while various illustrative implementations and architectures have been described in accordance with embodiments of the disclosure one of ordinary skill in the art will appreciate that numerous other modifications to the illustrative implementations and architectures described herein are also within the scope of this disclosure. In addition it should be appreciated that any operation element component data or the like described herein as being based on another operation element component data or the like may be additionally based on one or more other operations elements components data or the like. Accordingly the phrase based on or variants thereof should be interpreted as based at least in part on. 

The present disclosure may be a system a method and or a computer program product. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present disclosure.

The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present disclosure may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present disclosure.

Aspects of the present disclosure are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present disclosure. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

