---

title: Memory sharing via a unified memory architecture
abstract: A method and system for sharing memory between a central processing unit (CPU) and a graphics processing unit (GPU) of a computing device are disclosed herein. The method includes allocating a surface within a physical memory and mapping the surface to a plurality of virtual memory addresses within a CPU page table. The method also includes mapping the surface to a plurality of graphics virtual memory addresses within an I/O device page table.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09514559&OS=09514559&RS=09514559
owner: Intel Corporation
number: 09514559
owner_city: Santa Clara
owner_country: US
publication_date: 20160324
---
The present patent application is a continuation application claiming priority from U.S. application Ser. No. 13 588 453 filed Aug. 17 2012 and is currently pending.

The present invention relates generally to the sharing of memory between a central processing unit CPU and an input output I O device within a computing system. More specifically the present invention relates to the sharing of physical memory between a CPU and an I O device.

Modern I O devices may include computer processing capabilities that rival the computer processing capabilities of many central processing units CPUs . As a result a portion of the computational tasks traditionally performed by the CPU may be offloaded to an I O device of the computing device. For example an I O device such as a graphics processing unit GPU of a computing device can perform some of the tasks traditionally performed by the CPU thereby increasing the efficiency of the CPU.

The same numbers are used throughout the disclosure and the figures to reference like components and features. Numbers in the series refer to features originally found in numbers in the series refer to features originally found in and so on.

Current operating systems and graphics interfaces manage GPUs as I O devices rather than managing the GPUs as processors with resources similar to CPUs. By managing GPUs as I O devices CPUs and GPUs have physical memories with separate physical address domains. When offloading computational tasks to the GPUs data is copied from the physical address domain of the CPU to the physical address domain of the GPU. After the GPU has finished processing the data is copied back to the physical address domain of the CPU.

Offloading a portion of the computational tasks traditionally performed by the CPU to the GPU of a computing device may increase the efficiency of the CPU. As discussed above in order to offload tasks to the GPU data may be transferred between the physical memory of the CPU to the physical memory of the GPU. The data transfers that occur when offloading computational tasks to the GPU may reduce any efficiency gained by offloading tasks to the GPU. Accordingly embodiments described herein relate to the sharing of memory between the CPU and the GPU of a computing device. The memory may be shared via a unified memory architecture UMA .

In various embodiments the UMA provides for memory sharing between the CPU and GPU by providing both the CPU and the GPU with the same physical memory. Thus the physical memory and the corresponding physical address space of the CPU and GPU are one and the same. In embodiments the physical memory may be partitioned between the CPU and the GPU. Further the physical memory can be a paged system memory that is allocated by the operating system of the computing device. The virtual memory address space of the CPU may be mapped to the same physical memory pages as the graphics virtual memory address space of the GPU. Additionally in some embodiments the CPU and GPU are physically located on the same die. Thus the CPU and the GPU may share the data contained within the physical memory without copying data from the address space of the GPU to the address space of the CPU or vice versa. This may reduce the cost of offloading computational tasks from the CPU to the GPU by for example decreasing the time and the power consumption for sharing data between the CPU and the GPU.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Rather in particular embodiments connected may be used to indicate that two or more elements are in direct physical or electrical contact with each other. Coupled may mean that two or more elements are in direct physical or electrical contact. However coupled may also mean that two or more elements are not in direct contact with each other but yet still co operate or interact with each other.

Some embodiments may be implemented in one or a combination of hardware firmware and software. Some embodiments may also be implemented as instructions stored on a machine readable medium which may be read and executed by a computing platform to perform the operations described herein. A machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computer. For example a machine readable medium may include read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or electrical optical acoustical or other form of propagated signals e.g. carrier waves infrared signals digital signals or the interfaces that transmit and or receive signals among others.

An embodiment is an implementation or example. Reference in the specification to an embodiment. one embodiment some embodiments various embodiments or other embodiments means that a particular feature structure or characteristic described in connection with the embodiments is included in at least some embodiments but not necessarily all embodiments of the inventions. The various appearances of an embodiment one embodiment or some embodiments are not necessarily all referring to the same embodiments. Elements or aspects from an embodiment can be combined with elements or aspects of another embodiment.

Not all components features structures characteristics etc. described and illustrated herein need be included in a particular embodiment or embodiments. If the specification states a component feature structure or characteristic may might can or could be included for example that particular component feature structure or characteristic is not required to be included. If the specification or claim refers to a or an element that does not mean there is only one of the element. If the specification or claims refer to an additional element that does not preclude there being more than one of the additional element.

It is to be noted that although some embodiments have been described in reference to particular implementations other implementations are possible according to some embodiments. Additionally the arrangement and or order of circuit elements or other features illustrated in the drawings and or described herein need not be arranged in the particular way illustrated and described. Many other arrangements are possible according to some embodiments.

In each system shown in a figure the elements in some cases may each have a same reference number or a different reference number to suggest that the elements represented could be different and or similar. However an element may be flexible enough to have different implementations and work with some or all of the systems shown or described herein. The various elements shown in the figures may be the same or different. Which one is referred to as a first element and which is called a second element is arbitrary.

The computing device may also include a graphics processing unit GPU . The GPU is an input output I O device within the computing device . An I O device is a device that can be used to communicate with a computer using input output or any combination thereof. As shown the CPU may be connected through a bus to the GPU . However in some embodiments the GPU is located on the same die as the CPU within the computing device . In this manner the CPU and the GPU are physically connected in such a manner that the connection between the CPU and the GPU via the bus may be eliminated. Furthermore in embodiments the CPU and the GPU may be included within a unified memory architecture of the computing device as discussed with respect to .

The GPU may be configured to perform any number of graphics operations within the computing device . For example the GPU may be configured to render or manipulate graphics images graphics frames videos or the like to be displayed to a user of the computing device . In some embodiments the GPU includes a number of graphics engines not shown wherein each graphics engine is configured to perform specific graphics tasks or to execute specific types of workloads.

The computing device may also include a memory device . The memory device can include random access memory RAM read only memory ROM flash memory or any other suitable memory systems. For example the memory device may include dynamic random access memory DRAM . The memory may include a device driver that is configured to execute the instructions for implementing the memory sharing procedure. The device driver may be software an application program application code or the like. In some embodiments the device driver is a user mode driver.

The memory also includes a multi level cache that includes a last level cache LLC a level 2 cache and a level 1 cache . Although a multi level cache is used for illustration any cache can be included in the computing device . The multi level cache may be a smaller faster memory that stores a smaller subset of frequently used data for the CPU . A larger data set may be stored in a storage . The storage is a physical memory such as a hard drive an optical drive a thumbdrive an array of drives or any combinations thereof. The storage may also include remote storage drives. The amount of time for the CPU to access data stored in the storage may be slower relative to the amount of time it takes for the CPU to access the multi level cache in the memory .

In some embodiments the LLC is shared between the CPU and the GPU while the level 2 cache and the level 1 cache may be hidden from the GPU such that the GPU cannot directly access data cached in the level 2 cache and the level 1 cache . However the LLC can read and write data stored in the level 2 cache and the level 1 cache . Thereby when the GPU requests data cached in the level 2 cache or the level 1 cache the LLC is able to retrieve data from the level 2 cache and the level 1 cache for processing by the GPU . In this manner the LLC ensures data coherency within the computing device . As used herein coherency refers to the state wherein the data being accessed by the CPU and the GPU is the same. Accordingly the CPU will ensure that data from the storage device is accurately reflected in the LLC the level 2 cache and the level 1 cache by ensuring the data is coherent with the LLC in order to enable the correct data to be shared with the GPU .

Additionally in embodiments the CPU and GPU can access any level of memory. However data from other levels of memory may be stale while the LLC includes the most recent data. Furthermore in embodiments the CPU and GPU can employ any mutually accessible storage location to perform shared virtual memory. Any mutually accessible storage location may include but is not limited to any area of the memory device any area of the storage a networked storage location a thumbdrive or any combination thereof.

The storage includes a surface as well as any number of applications that are configured to run on the computing device . The surface is a designated portion of physical memory that is allocated by the device driver . The surface may be updated based on processing performed on the contents of the physical memory within the surface . In embodiments when an application is executed by CPU the application may request that a surface be allocated by the device driver . Furthermore the applications running on the CPU may configure the surface depending on the memory allocation called for by the applications by specifying the desired size and characteristics of the surface . Additionally surface allocation may be performed for example in response to input from the CPU of the computing device . Furthermore in embodiments the surface is marked as LLC cacheable. By designated the surface as LLC cacheable the data cached from locations within the surface may be cached to the LLC and thereby accessible in the LLC by both the CPU and the GPU .

A memory management unit MMU may be used to manage access to data that is stored within the surface . The MMU can divide the virtual address space of the CPU and the GPU into various pages of address space. The CPU and the GPU each have their own virtual address spaces. The virtual address space allows for protection of the data contained within the surface by isolating the various applications executing within a computing system to a particular subset of virtual addresses. Through the use of virtual address spaces one application will not access the data of another application . Accordingly the MMU includes a CPU page table and a GPU page table . The CPU page table contains the virtual addresses of the CPU mapped to a physical address location within the surface . Similarly the GPU page table contains the virtual addresses of the GPU mapped to a physical address location within the surface . In the memory sharing procedure described herein the CPU page table may include a mapping of the CPU virtual address space to a physical address space. The physical address space corresponds to physical locations within the surface . Likewise the GPU page table may include a mapping of the GPU virtual address space to the same.

In various embodiments the virtual memory addresses from the CPU page table and the graphics virtual memory addresses from the GPU page table are mapped to the physical memory pages of the surface via a translation procedure. The translation procedure may be used to convert any of the virtual memory addresses to a corresponding physical address. For example the translation procedure may be performed via a page table walk which may be performed based on a specific translation table for converting virtual memory addresses within a page table to physical memory addresses within the page table. Additionally in embodiments a translation look aside buffer may be used to translate the virtual addresses of the CPU and the GPU into physical address spaces within their respective page tables.

After a translation procedure has been performed the surface may be pinned. Pinning the surface refers to protecting the surface so that the physical locations and the corresponding physical addresses are unchanged. Thus pinning the surface ensures a hard mapping between virtual address spaces and physical address spaces. The hard mapping between address spaces is a mapping that does not change after the surface has been pinned. If the surface is not pinned a page fault may be generated or the wrong data may be processed as the physical location of the surface may shift.

In embodiments an application may execute on the CPU and requests a surface such as the surface in order to perform the operations such as processing data. The CPU may handoff the operations to the GPU . Since the page tables have been mapped to the surface the GPU can begin immediate execution of the operations that have been offloaded by the CPU by accessing the surface without copying data over to another address space. When the operations are completed by the CPU the GPU may signal to the CPU that the operations are complete. The CPU may then continue processing the data without copying the data back to an original address space.

When the operations that are requested by the application are performed by the GPU modifications to the surface may occur. According to the memory sharing procedure described herein such modifications to the surface are fully visible to the CPU . Thus data may be shared between the GPU and the CPU without copying the data from the GPU to the CPU or vice versa.

The CPU may be connected through the bus to an input output I O device interface adapted to connect the computing device to one or more I O devices . The I O devices may include for example a keyboard and a pointing device wherein the pointing device may include a touchpad or a touchscreen among others. The I O devices may be built in components of the computing device or may be devices that are externally connected to the computing device .

The CPU may also be linked through the bus to a display interface adapted to connect the computing device to a display device . The display device may include a display screen that is a built in component of the computing device . The display device may also include a computer monitor television or projector among others that is externally connected to the computing device .

A network interface controller NIC may be adapted to connect the computing device through the bus to a network . The network may be a wide area network WAN local area network LAN or the Internet among others.

The block diagram of is not intended to indicate that the computing device is to include all of the components shown in . Further the computing device may include any number of additional components not shown in depending on the details of the specific implementation.

The UMA may enable direct memory sharing between the CPU and the GPU without any type of data copying or data transfer between the CPU and the GPU . This may be accomplished by allowing the CPU and the GPU to share the surface . As described above the surface may be a portion of a physical storage device. The surface includes any number of physical memory locations . The physical memory locations may be organized into a paged memory format where a page is a fixed length block of physical memory within the surface .

The CPU page table may include a number of CPU virtual memory addresses and the GPU page table may include a number of graphics virtual memory addresses . The CPU virtual memory addresses form the CPU virtual address space while the graphics virtual memory addresses form the graphics virtual address space. Each address space is mapped to a physical address in each page table. Thus the CPU virtual memory addresses and the graphics virtual memory addresses both map to the same set of physical addresses within the CPU page table and the GPU page table respectively.

The physical addresses enable the CPU and the GPU to process data stored at physical locations within the surface . In various embodiments the surface is allocated based on the specific CPU virtual addresses accessed by an application such as an application . Once the surface has been allocated each physical address is mapped to a corresponding CPU virtual address within the CPU page table as shown in . The graphics virtual memory addresses within the GPU page table may be synchronized with the CPU page table such that the CPU virtual addresses and the GPU virtual memory addresses are mapped to the same set of physical addresses . The physical addresses correspond to physical locations within the surface . Accordingly the surface may be directly shared between the CPU and the GPU . In embodiments if the GPU modifies data located at any of physical locations the modifications are automatically visible to the CPU via the surface without any data copying or data marshaling.

The schematic of is not intended to indicate that the UMA is to include all of the components shown in . Further the UMA may include any number of additional components not shown in depending on the details of the specific implementation.

In some embodiments the method may be executed on a computing device such as the computing device where the CPU and the GPU are connected by a bus . In other embodiments the CPU and the GPU may be included in a UMA such as the UMA discussed above with respect to . Further the method may executed by a driver of the computing device such as the device driver of the computing device .

The method begins at block with the allocation of a surface within a physical memory. In embodiments the surface may be allocated within the physical memory of a computing device in response to input from an application running on the CPU of the computing device. Furthermore in embodiments the surface may be allocated by the device driver. The application or the device driver may access the surface from the CPU using a CPU virtual address. In embodiments the CPU virtual addresses are provided to the application or the device driver by an operating system of the computing device.

At block the physical addresses corresponding to physical locations within the surface are mapped to the CPU virtual addresses. The mapping between the CPU virtual memory addresses and the physical addresses are included within a CPU page table. Mapping the physical memory pages to the virtual memory addresses may include translating the CPU virtual addresses to determine corresponding physical memory pages within the system memory. When the CPU virtual addresses have been translated to physical addresses the associations between the CPU virtual addresses and the physical addresses found during the translation process are locked. By locking the associations the physical locations of the surface that correspond to the physical addresses in the CPU page table may be paged in to the cache. The pages will remain in the cache while the associations are locked as the physical addresses of the surface are prevented from changing by the device driver.

At block the GPU virtual memory addresses are mapped to the physical locations within the surface. In embodiments the surface is designated as LLC cacheable. Such a designation ensures that the physical locations of the surface are cached into the LLC which is shared by the CPU and the GPU. The graphics virtual memory addresses used by the application may be translated to the same physical addresses that are mapped to the virtual addresses of the CPU. In embodiments the device driver may update the mapping of graphics virtual memory addresses to the physical addresses within the GPU page table.

Mapping the surface to the GPU virtual addresses may include pinning the surface. By pinning the surface the mapping between the GPU virtual addresses and the surface are prevented from being changed. Thus the GPU virtual memory will correspond to the same physical memory without the physical memory being changed. For example an operating system may change assigned physical memory locations as a part of its memory management. However once the surface has been pinned the operating system is prevented from changing the physical memory locations of the surface.

At block an operation may be offloaded from the CPU to the GPU. The operation may be offloaded to the GPU as directed by an application such as the application . Additionally any application programming interface API used to control the CPU or the GPU may be used to direct the offloading of an operation from the CPU to the GPU. In embodiments prior to offloading an operation from the CPU to the GPU the data located within the surface that is being processed by the CPU may be made coherent with the LLC.

At block the GPU may begin processing of the offloaded operation. The GPU accesses data within the LLC and the surface in order to perform the operation. In the event that the GPU requests data that is not in the LLC but is in some other cache of the CPU the LLC may retrieve the data from the other cache for processing by the GPU.

At block the GPU signals that the operation is complete. The completion signal may be sent to the host. In embodiments when the operation is complete the device driver synchronizes the operation between the GPU and the CPU. Further in embodiments the completion signal may be for example a mailbox write or an interrupt. The completion signal may indicate that the GPU has performed some computation or graphics operation that has resulted in a modification of the data within the surface. After completion the output of the GPU may be processed by the CPU. In various embodiments when the GPU processes the surface by reading from or writing to any of the physical locations of the surface processing may occur in internal buffers and caches of the GPU. Accordingly the data within the internal buffers and caches of the GPU is made coherent with the LLC after the GPU processing has completed.

The process flow diagram of are not intended to indicate that the blocks of methods and are to be executed in any particular order or that all of the blocks are to be included in every case. Further any number of additional blocks may be included within the methods and depending on the details of the specific implementation. Additionally while the methods described herein include a GPU the memory may be shared between any I O device such as another CPU or a direct memory access DMA controller.

The various software components discussed herein may be stored on the tangible non transitory computer readable media as indicated in . For example a surface allocation module may be configured to allocate or generate a surface including a number of physical memory pages within a memory of the computing device. A mapping module may be configured to map the physical locations within the surface to virtual memory addresses within the CPU address table and GPU address table. Further a pinning module may be configured to pin the surface so that the physical locations within the surface are prevented from changing.

The block diagram of is not intended to indicate that the tangible non transitory computer readable media is to include all of the components shown in . Further the tangible non transitory computer readable media may include any number of additional components not shown in depending on the details of the specific implementation.

In embodiments the CPU does not have to marshal data between the CPU address space and the GPU address space. Furthermore the CPU is not tasked with ensuring that no other processing cores are working on the particular set of data that the CPU wants the GPU to handle thus preventing processing races between processing cores.

In various embodiments the system comprises a platform coupled to a display . The platform may receive content from a content device such as content services device s or content delivery device s or other similar content sources. A navigation controller including one or more navigation features may be used to interact with for example the platform and or the display . Each of these components is described in more detail below.

The platform may include any combination of a chipset a central processing unit CPU a memory device a storage device a graphics subsystem applications and a radio . The chipset may provide intercommunication among the CPU the memory device the storage device the graphics subsystem the applications and the radio . For example the chipset may include a storage adapter not shown capable of providing intercommunication with the storage device .

The CPU may be implemented as Complex Instruction Set Computer CISC or Reduced Instruction Set Computer RISC processors x86 instruction set compatible processors multi core or any other microprocessor or central processing unit CPU . In some embodiments the CPU includes dual core processor s dual core mobile processor s or the like.

The memory device may be implemented as a volatile memory device such as but not limited to a Random Access Memory RAM Dynamic Random Access Memory DRAM or Static RAM SRAM . The storage device may be implemented as a non volatile storage device such as but not limited to a magnetic disk drive optical disk drive tape drive an internal storage device an attached storage device flash memory battery backed up SDRAM synchronous DRAM and or a network accessible storage device. In some embodiments the storage device includes technology to increase the storage performance enhanced protection for valuable digital media when multiple hard drives are included for example.

The graphics subsystem may perform processing of images such as still or video for display. The graphics subsystem may include a graphics processing unit GPU such as the GPU or a visual processing unit VPU for example. An analog or digital interface may be used to communicatively couple the graphics subsystem and the display . For example the interface may be any of a High Definition Multimedia Interface DisplayPort wireless HDMI and or wireless HD compliant techniques. The graphics subsystem may be integrated into the CPU or the chipset . Alternatively the graphics subsystem may be a stand alone card communicatively coupled to the chipset .

The graphics and or video processing techniques described herein may be implemented in various hardware architectures. For example graphics and or video functionality may be integrated within the chipset . Alternatively a discrete graphics and or video processor may be used. As still another embodiment the graphics and or video functions may be implemented by a general purpose processor including a multi core processor. In a further embodiment the functions may be implemented in a consumer electronics device.

The radio may include one or more radios capable of transmitting and receiving signals using various suitable wireless communications techniques. Such techniques may involve communications across one or more wireless networks. Exemplary wireless networks include wireless local area networks WLANs wireless personal area networks WPANs wireless metropolitan area network WMANs cellular networks satellite networks or the like. In communicating across such networks the radio may operate in accordance with one or more applicable standards in any version.

The display may include any television type monitor or display. For example the display may include a computer display screen touch screen display video monitor television or the like. The display may be digital and or analog. In some embodiments the display is a holographic display. Also the display may be a transparent surface that may receive a visual projection. Such projections may convey various forms of information images objects or the like. For example such projections may be a visual overlay for a mobile augmented reality MAR application. Under the control of one or more applications the platform may display a user interface on the display .

The content services device s may be hosted by any national international or independent service and thus may be accessible to the platform via the Internet for example. The content services device s may be coupled to the platform and or to the display . The platform and or the content services device s may be coupled to a network to communicate e.g. send and or receive media information to and from the network . The content delivery device s also may be coupled to the platform and or to the display .

The content services device s may include a cable television box personal computer network telephone or Internet enabled device capable of delivering digital information. In addition the content services device s may include any other similar devices capable of unidirectionally or bidirectionally communicating content between content providers and the platform or the display via the network or directly. It will be appreciated that the content may be communicated unidirectionally and or bidirectionally to and from any one of the components in the system and a content provider via the network . Examples of content may include any media information including for example video music medical and gaming information and so forth.

The content services device s may receive content such as cable television programming including media information digital information or other content. Examples of content providers may include any cable or satellite television or radio or Internet content providers among others.

In some embodiments the platform receives control signals from the navigation controller which includes one or more navigation features. The navigation features of the navigation controller may be used to interact with the user interface for example. The navigation controller may be a pointing device that may be a computer hardware component specifically human interface device that allows a user to input spatial e.g. continuous and multi dimensional data into a computer. Many systems such as graphical user interfaces GUI and televisions and monitors allow the user to control and provide data to the computer or television using physical gestures. Physical gestures include but are not limited to facial expressions facial movements movement of various limbs body movements body language or any combination thereof. Such physical gestures can be recognized and translated into commands or instructions.

Movements of the navigation features of the navigation controller may be echoed on the display by movements of a pointer cursor focus ring or other visual indicators displayed on the display . For example under the control of the applications the navigation features located on the navigation controller may be mapped to virtual navigation features displayed on the user interface . In some embodiments the navigation controller may not be a separate component but rather may be integrated into the platform and or the display .

The system may include drivers not shown that include technology to enable users to instantly turn on and off the platform with the touch of a button after initial boot up when enabled for example. Program logic may allow the platform to stream content to media adaptors or other content services device s or content delivery device s when the platform is turned off. In addition the chipset may include hardware and or software support for 5.1 surround sound audio and or high definition 7.1 surround sound audio for example. The drivers may include a graphics driver for integrated graphics platforms. In some embodiments the graphics driver includes a peripheral component interconnect express PCle graphics card.

In various embodiments any one or more of the components shown in the system may be integrated. For example the platform and the content services device s may be integrated the platform and the content delivery device s may be integrated or the platform the content services device s and the content delivery device s may be integrated. In some embodiments the platform and the display are an integrated unit. The display and the content service device s may be integrated or the display and the content delivery device s may be integrated for example.

The system may be implemented as a wireless system or a wired system. When implemented as a wireless system the system may include components and interfaces suitable for communicating over a wireless shared media such as one or more antennas transmitters receivers transceivers amplifiers filters control logic and so forth. An example of wireless shared media may include portions of a wireless spectrum such as the RF spectrum. When implemented as a wired system the system may include components and interfaces suitable for communicating over wired communications media such as input output I O adapters physical connectors to connect the I O adapter with a corresponding wired communications medium a network interface card NIC disc controller video controller audio controller or the like. Examples of wired communications media may include a wire cable metal leads printed circuit board PCB backplane switch fabric semiconductor material twisted pair wire co axial cable fiber optics or the like.

The platform may establish one or more logical or physical channels to communicate information. The information may include media information and control information. Media information may refer to any data representing content meant for a user. Examples of content may include for example data from a voice conversation videoconference streaming video electronic mail email message voice mail message alphanumeric symbols graphics image video text and the like. Data from a voice conversation may be for example speech information silence periods background noise comfort noise tones and the like. Control information may refer to any data representing commands instructions or control words meant for an automated system. For example control information may be used to route media information through a system or instruct a node to process the media information in a predetermined manner. The embodiments however are not limited to the elements or the context shown or described in .

As described above examples of a mobile computing device may include a personal computer PC laptop computer ultra laptop computer tablet touch pad portable computer handheld computer palmtop computer personal digital assistant PDA cellular telephone combination cellular telephone PDA television smart device e.g. smart phone smart tablet or smart television mobile internet device MID messaging device data communication device and the like.

An example of a mobile computing device may also include a computer that is arranged to be worn by a person such as a wrist computer finger computer ring computer eyeglass computer belt clip computer arm band computer shoe computer clothing computer or any other suitable type of wearable computer. For example the mobile computing device may be implemented as a smart phone capable of executing computer applications as well as voice communications and or data communications. Although some embodiments may be described with a mobile computing device implemented as a smart phone by way of example it may be appreciated that other embodiments may be implemented using other wireless mobile computing devices as well.

As shown in the device may include a housing a display an input output I O device and an antenna . The device may also include navigation features . The display may include any suitable display unit for displaying information appropriate for a mobile computing device. The I O device may include any suitable I O device for entering information into a mobile computing device. For example the I O device may include an alphanumeric keyboard a numeric keypad a touch pad input keys buttons switches rocker switches microphones speakers a voice recognition device and software or the like. Information may also be entered into the device by way of microphone. Such information may be digitized by a voice recognition device.

A method for sharing memory between a central processing unit CPU and an input output I O device of a computing device is described herein. The method includes allocating a surface within a physical memory. The method includes mapping the surface to a plurality of virtual memory addresses within a CPU page table. The method also includes mapping the surface to a plurality of graphics virtual memory addresses within an I O device page table based on the surface. The method further includes pinning the surface.

Memory may be shared between the CPU and the I O device via the surface without copying data from a CPU memory to an I O device memory. The surface may be allocated in response to input from an application running on the CPU of the computing device. In addition the method may be executed by a driver of the computing device.

Data from the cache of the CPU and the I O device may be coherent with a last level cache LLC that is shared between the CPU and the I O device. An operation may be offloaded from the CPU to the I O device and the operation may be performed within the I O device. A completion signal may be sent to the CPU wherein the completion signal includes an indication that the I O device has performed some computation that has resulted in a modification of data within the surface. Additionally a device driver may synchronize the processing of data between the CPU and the I O device.

A computing device is described herein. The computing device includes a central processing unit CPU that is configured to execute stored instructions and a storage device that stores instructions. The storage device includes processor executable code that when executed by the CPU is configured to allocate a surface within a physical memory. The computing device also includes a graphics processing unit GPU and GPU page table. The surface may be mapped to a plurality of virtual memory addresses within a CPU page table. The surface may also be mapped to a plurality of virtual memory addresses within the GPU page table. The computing device may pin the surface.

The physical memory may be shared between the CPU and the GPU without copying data from a CPU memory to a GPU memory. Further the CPU and the GPU are located on a same die within the computing device. The CPU and the GPU may share a last level cache LLC wherein the LLC can retrieve data from any cache of the CPU or GPU. The CPU and the GPU may include a unified memory architecture UMA .

The processor executable code may be configured to allocate the surface in response to input from an application running on the CPU of the computing device. The virtual memory addresses in the CPU page table and the GPU page table may be mapped to physical locations within the surface by translating the virtual addresses to physical addresses. A driver may be configured to initiate execution of the processor executable code. Additionally the computing device may include a radio and a display and the radio and display may be communicatively coupled at least to the central processing unit.

At least one non transitory machine readable medium having instructions stored therein is described herein. In response to being executed on a computing device the instructions cause the computing device to generate a surface within a physical memory. The instructions also cause the computing device to map the surface to a number of CPU virtual memory addresses and map the surface to a number of GPU virtual memory addresses. The surface may also be pinned.

The physical memory may be shared between the CPU and the GPU without copying data from a CPU memory to a GPU memory. Further the instructions may cause the data from the cache of the CPU and the GPU to be coherent with a last level cache LLC . In addition the instructions may also cause the computing device to allocate the surface in response to input from an application running on a CPU of the computing device.

It is to be understood that specifics in the aforementioned examples may be used anywhere in one or more embodiments. For instance all optional features of the computing device described above may also be implemented with respect to either of the methods or the computer readable medium described herein. Furthermore although flow diagrams and or state diagrams may have been used herein to describe embodiments the inventions are not limited to those diagrams or to corresponding descriptions herein. For example flow need not move through each illustrated box or state or in exactly the same order as illustrated and described herein

The inventions are not restricted to the particular details listed herein. Indeed those skilled in the art having the benefit of this disclosure will appreciate that many other variations from the foregoing description and drawings may be made within the scope of the present inventions. Accordingly it is the following claims including any amendments thereto that define the scope of the inventions.

