---

title: Selecting data of a server system for transmission
abstract: Methods, systems and apparatuses for selecting graphics data of a server system for transmission are disclosed. One method includes reading data from memory of the server system, checking if the data is being read for the first time, checking if the data was written by a processor of the server system during processing, comprising checking if the data is available on a client system or present in a transmit buffer, placing the data in the transmit buffer if the data is being read for the first time and was not written by the processor during the processing as determined by the checking if the data was written by the processor of the server system during processing, wherein if the data is being read for the first time and was written by the processor of the server system during processing the data is not placed in the transmit buffer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09640150&OS=09640150&RS=09640150
owner: ThinCI, Inc.
number: 09640150
owner_city: El Dorado Hills
owner_country: US
publication_date: 20160519
---
This patent application is a continuation of U.S. patent application Ser. No. 14 287 036 filed May 25 2014 which is continuation in part CIP of U.S. patent application Ser. No. 13 161 547 filed on Jun. 16 2011 which claims priority to U.S. provisional patent application Ser. No. 61 355 768 filed Jun. 17 2010 which are all herein incorporated by reference.

The described embodiments relate generally to transmission of graphics data. More particularly the described embodiments relate to methods apparatuses and systems for selecting data of a server system for transmission.

The onset of cloud computing is causing a paradigm shift from distributed computing to centralized computing. Centralized computer includes most of the resources of a system being centralized . These resources generally include a centralized server that includes central processing unit CPU memory storage and support for networking. Applications run on the centralized server and the results are transferred to one or more clients.

Centralized computing works well in many applications but falls short in the execution of graphics rich applications which are increasingly popular with consumers. Proprietary techniques are currently used for remote processing of graphics for thin client applications. Proprietary techniques include Microsoft RDP Remote Desktop Protocol Personal Computer over Internet Protocol PCoIP VMware View and Citrix Independent Computing Architecture ICA and may apply a compression technique to a frame display buffer.

Video compression scheme is most suited for remote processing of graphics for thin client applications as the content of the frame buffer changes incrementally. Video compression scheme is an adaptive compression technique based on instantaneous network bandwidth availability computationally intensive and places additional burden on the server resources. In video compression scheme the image quality is compromised and additional latency is introduced due to the compression phase.

It is desirable to have a method apparatus and system for transmission of graphics data that reduces computation demands enables lossless compression and improves latency.

One embodiment includes a method of selecting data of a server system for transmission. The method includes reading data from memory of the server system checking if the data is being read for the first time checking if the data was written by a processor of the server system during processing comprising checking if the data is available on a client system or present in a transmit buffer placing the data in the transmit buffer if the data is being read for the first time as determined by the checking if the data is being read for the first time and was not written by the processor of the server system during the processing as determined by the checking if the data was written by a processor of the server system during processing wherein if the data is being read for the first time and was written by the processor of the server system during processing the data is not placed in the transmit buffer and transmitting the data of the transmit buffer to the client system.

Another embodiment includes a system for selecting data for transmission. The system includes a server system. The server system includes a processor and memory wherein the memory comprises a frame buffer and a transmit buffer the server system operable to read data from the memory check if the data is being read for the first time check if the data was written by the processor of the server system during processing comprising checking if the data is available on a client system or present in the transmit buffer place the data in the transmit buffer if the data is being read for the first time as determined by the checking if the data is being read for the first time and was not written by the processor of the server system during the processing as determined by the checking if the data was written by a processor of the server system during the processing wherein if the data is being read for the first time and was written by the processor of the server system during the processing the data is not placed in the transmit buffer.

Other aspects and advantages of the described embodiments will become apparent from the following detailed description taken in conjunction with the accompanying drawings illustrating by way of example the principles of the described embodiments.

The described embodiments are embodied in methods apparatuses and systems for selecting graphics data for transmission. These embodiments provide for lossless or near lossless transmission of graphics data between a server system and a client system while maintaining low latency. For the described embodiments lossless and near lossless may be used interchangeably and may mean lossless or near lossless compression and transmission methods. For the described embodiments processor refers to a device that processes graphics which includes and not limited to any one of or all of graphics processing unit GPU central processing unit CPU Accelerated Processing Unit APU and Digital Signal Processor DSP . Depending upon a link bandwidth and or capabilities of the client system the described embodiments also include the transmission of video stream. For the described embodiments graphics stream refers to uncompressed data which is a subset of graphics and command data. For the described embodiments video stream refers to compressed frame buffer data.

As shown in for the described embodiments graphics memory includes command and graphics data frame buffer transmit buffer s while shown as a single transmit buffer for the embodiments that include multiple graphic render passes the transmit buffer actually includes a transmit buffer for each of the graphic render passes and compressed frame buffer . For the described embodiments graphics memory resides in server system . In another embodiment graphics memory may not reside in server system . The server system processes graphics data and manages data for transmission to the client system. Graphics memory may be any one of or all of Dynamic Random Access memory DRAM Static Random Access Memory SRAM flash memory content addressable memory or any other type of memory. For the described embodiments graphics memory is a DRAM storing graphics data. For the described embodiments a block of data that is read or written to memory is referred to as a cache line. For the described embodiments the status of the cache line of command and graphics data is stored in graphics memory . In another embodiment the status can be stored in a separate memory. In this embodiment status bits refer to a set of one or more status bits of memory used to store the status of a cache line or a subset of the cache line. A cache line can have one or more sets of status bits.

For the described embodiments graphics memory is located in the system memory not shown in . In another embodiment graphics memory may be in a separate dedicated video memory. Graphics application running on the CPU loads graphics data into system memory. For the described embodiments graphics data includes at least index buffers vertex buffers and textures. The graphics driver of GPU translates graphics Application Programming Interface API calls made by for example a graphics application into command data. For the described embodiments graphics API refers to an industry standard API such as OpenGL or DirectX. For the described embodiments the graphics and command data is placed in graphics memory either by copying or remapping. Typically the graphics data is large and generally not practical to transmit to client systems as is.

GPU processes command and data in command and graphics data and selectively places data either in frame buffer at the end of graphics rendering or in transmit buffer s during graphics rendering. GPU is a specialized processor for manipulating and displaying graphics. For the described embodiments GPU supports 2D 3D graphics and or video. As will be described GPU manages generation of compressed data for placement in the compressed frame buffer and a subset of uncompressed graphics and command data is placed in transmit buffer s . The data from transmit buffer s contains graphics data and is referred to as graphics stream .

Transmit buffer s is populated with a selected subset of command and graphics data during graphics rendering. The selected subset of data from command and graphics data is such that the results obtained by the client system by processing the subset of data can be identical or almost identical to processing the entire contents of command and graphics data . The process of selecting a subset of data from command and graphics data to fill transmit buffer s is discussed further in conjunction with . During the process of graphics rendering GPU fills transmit buffer s . For the described embodiments the contents of transmit buffer s includes at least command data or graphics API command calls along with graphics data. For an embodiment the allocated size of transmit buffer s is adaptively determined by the maximum available bandwidth on the link. For example the size of the frame buffer can dynamically change over time as the bandwidth of the link between the server system and the client system varies.

In this embodiment GPU is responsible for graphics rendering frame buffer and generating compressed frame buffer . In this embodiment compressed frame buffer is generated if the client does not have capabilities or the bandwidth is not sufficient to transmit graphics stream. The compressed frame buffer is generated by encoding the contents of frame buffer using industry standard compression techniques for example MPEG2 and MPEG4.

Graphics stream includes at least uncompressed graphics data and header with at least data type information. Graphics stream is generated during graphics rendering and may be available while the transmit buffer s has data.

Video stream includes at least a compressed video data and header conveying the information required for interpreting the data type for decompression. Video stream can be available as and when compressed frame buffer is generated.

Mux illustrates a selection between graphics stream generated by data from the transmit buffer s and video stream generated by data from compressed frame buffer . The selection by mux is done on a frame by frame basis and is controlled by control which at least in some embodiments is generated by the GPU . A frame is the interval of processing time for generating a frame buffer for display. For other embodiments control is generated by CPU and or GPU. For the described embodiments control dependents on at least in part upon either bandwidth of link between the server system and the client system and the processing capabilities of client system .

Mux selects between the graphics stream and the video stream the selection can occur once per clock cycle which is typically less than a frame. In this embodiment the data transmitted on link consists of data from compressed frame buffer and or transmit buffer s . For some embodiments link is a dedicated Wide Area Graphics Network WAGN Local Area Graphics Network LAGN to transmit graphics video stream from server system to client system . In an embodiment a hybrid Transmission Control Protocol TCP User Datagram Protocol UDP may be implemented to provide an optimal combination of speed and reliability. For example the TCP protocol is used to transmit the command control packets and the UDP protocol is used to transfer the data packets. For example command control packet can be the previously described command data the data packets can be the graphics data.

The client system receives data from the server system and manages the received data for user display. For the described embodiments client system includes at least client graphics memory CPU and GPU . Client graphics memory which includes at least a frame buffer may be a Dynamic Random Access memory DRAM Static Random Access Memory SRAM flash memory content addressable memory or any other type of memory. In this embodiment client graphics memory is a DRAM storing command and graphics data.

In an embodiment graphics video stream received from server system via link is a frame of data and processed using standard graphics rendering or video processing techniques to generate the frame buffer for display. The received frame includes at least a header and data. For the described embodiments the GPU reads the header to detect the data type which can include at least uncompressed graphics stream or compressed video stream to process the data. The method of handling the received data is discussed in conjunction with .

In step command and graphics data buffer is allocated. In this step a portion of free or unused graphics memory is defined as command and graphics data based on the requirement and the command and graphics data in system memory is copied to graphics memory if the graphics memory is a dedicated video memory or remapped copied to graphics memory if the graphics memory is part of system memory.

In step graphics data is rendered on server system . Graphics data in server system read from command and graphics data is rendered by GPU . For the described embodiments graphics rendering or 3D rendering is the process of producing a two dimensional image based on three dimensional scene data. Graphics rendering involves processing of polygons and generating the contents of frame buffer for display. Polygons such as triangles lines points have attributes associated with the vertices which are stored in vertex buffer s and determine how the polygons are processed. The position coordinates undergo linear scaling rotation translation etc. and viewing world and view space transformation. The polygons are rasterized to determine the pixels enclosed within. Texturing is a technique to apply paste texture images onto these pixels. The pixel color values are written to frame buffer .

Step involves checking the client system capabilities to decide the compression technique. In the described embodiments the size and bandwidth of client graphics memory graphics API support in the client system the performance of GPU and decompression capabilities of client system constitutes client system capabilities.

When the client system has capabilities transmit buffer s is generated. In step the contents of transmit buffer s is generated during graphics rendering. Data is written into transmit buffer s as and when data is rendered. A subset of graphics and command data is identified and unique instances of data are selected for placing data in transmit buffer s which is discussed in conjunction with . The data from transmit buffer s is referred to as graphics stream .

In step method checks for at least the bandwidth of link connecting server system and client system . If sufficient bandwidth is available graphics stream is transmitted in step .

If the bandwidth available is not sufficient or if the client system does not have capabilities compressed frame buffer is generated. In step compressed frame buffer is generated by encoding the contents of frame buffer using MPEG2 MPEG4 or any other compression techniques. The selection of compression technique is determined by the client capabilities. After graphics rendering is complete the compressed frame buffer is filled during compression of frame buffer . In step compressed frame buffer is transmitted.

In step the cache line is checked for being read for the first time to determine if the data in the cache line is new. If the data has been read earlier the data is available on client system or present in transmit buffer s the cache line is not processed further and method returns to step . If the cache line is being read for the first time the client system does not have the data and not present in the transmit buffer s method proceeds to step .

In step the cache line of command and graphics data or frame buffer is checked if the data in the cache line was written during graphics rendering by a processor. If the data in the cache line was written by a processor the data in cache line is not processed and method returns to step . If the cache line is not written by the processor then method proceeds to step . In step the cache line is placed in transmit buffer s .

Note that for at least some embodiments steps and are performed for each of the described graphic render passes.

In this embodiment the server system includes a central processing unit CPU and a graphics processing unit GPU . The GPU controls compression and placement of data of a frame buffer into a compressed frame buffer. The GPU controls selection of either compressed data of the compressed frame buffer or uncompressed data of the transmit buffer s for transmission to the client system.

Checking a first status bit determines whether the data is being read for the first time. The first status bit is set when the data is placed in the transmit buffer s and not yet transmitted.

The data being read can be a cache line which is a block of data. One or more status bits define the status of the cache line. In another embodiment each sub block of the cache line can have one or more status bits. For an embodiment the data comprises a plurality of blocks and wherein determining if the data is being read for the first time comprises checking at least one status bit corresponding to at least one block

The second status bit determines whether the data was not written by the processor. The second status bit is set when the processor writes to the graphics memory. The first status bit is reset upon detecting a direct memory access DMA of the graphics memory or reallocation of the graphics memory. The second status bit is reset upon detecting a direct memory access DMA of the graphics memory or reallocation of the graphics memory. For the described embodiments DMA refers to the process of copying data from the system memory to graphics memory.

The method of selecting graphics data of a server system for transmission further comprises compressing data of a frame buffer of the graphics memory.

The method of selecting graphics data of a server system for transmission further comprises checking at least one of a bandwidth of a link between the server system and a client system and capabilities of the client system and the server system transmitting at least one of the compressed frame buffer data or the transmit buffer s based at least in part on the at least one of the bandwidth of the links and the capabilities of the client system.

The bandwidth and the client capabilities are checked on a frame by frame basis to determine whether to compress data of the frame buffer on a frame by frame basis and place a percentage of the data in the transmit buffer s for every frame. For an embodiment checking on a frame by frame basis includes checking the client capabilities and the bandwidth at the start of each frame and placing the compresses or uncompressed data in the frame buffer or transmit buffer s accordingly for the frame.

If adequate bandwidth is available and the client is capable of processing graphics stream the transmit buffer s is transmitted to the client system. If the bandwidth and the client capabilities determine that graphics stream cannot be transmitted then compressed frame buffer data and optionally partial uncompressed transmit buffer data is transmitted to the client system. If the client system does not have the capabilities to handle uncompressed data then compressed frame buffer data is transmitted to the client system. If the transmit buffer s is capable of being transmitted to the client system the compression phase is dropped and no compressed video stream is generated.

The server system maintains reference frame s for subsequent compression of data of the frame buffer. For each frame a decision is made to send either lossless graphics data or lossy video compression data. When implementing video compression for a particular frame on the server previous frames are used as reference frames. The reference frames correspond to lossless frame or lossy frame transmitted to the client.

For at least some of the described embodiment graphics rendering consists of a series of steps passes connected in a hierarchical tree topology with each step pass generating outputs which are provided as inputs to downstream steps passes . Each of these steps is defined as a graphic render pass.

For at least some embodiments a set images of at least one of the graphic render passes is used as graphic data of a subsequent graphic render pass. For at least some embodiments a final graphic render pass generates a final set of images.

At least some embodiments further include determining a size of each transmit buffer of each of multiple graphic render passes summing a plurality of combinations of sizes of combinations of the plurality of transmit buffers and selecting a combination of the plurality of combinations that provides within a margin a minimal summed size. For an embodiment the margin is zero and the selected combination provides the minimum summed size. For an embodiment the margin is greater than zero. An embodiment includes the server system transmitting the transmit buffers of the selected combination of transmit buffers.

For at least some embodiments the processor includes at least one of a central processing unit CPU and a graphics processing unit GPU the method further comprising the GPU controlling compression and placement of data of a frame buffer into a compressed frame buffer and the GPU controlling a selection of either compressed graphics data of the compressed frame buffer or the plurality of data of the plurality of transmit buffers for transmission to the client system.

At least some embodiments further include compressing data of a frame buffer of the graphics memory. At least some embodiments further include checking at least one of a bandwidth of a link between the server system and the client system and capabilities of the client system and the server system transmitting at least one of the compressed frame buffer data or the data of the transmit buffer based at least in part on the at least one of the bandwidth of the links and the capabilities of the client system. For at least some embodiments checking the bandwidth and the capabilities is performed on a frame by frame basis.

At least some embodiments further include the server system providing a reference frame to the client system for allowing the client system to decompress compressed video received from the server system and maintaining the reference frame for subsequent compression of data of the frame buffer even when the reference frame is lossless.

As part of the network graphics mechanism each of these render passes goes through the identification of the data to be placed in the transmit buffer. After the completion of rendering of all the render passes the partitioning of the tree graph is determined based on the minimal bandwidth needed between server and client. The minimal bandwidth determination is made based at least one of several conditions. For every combination of render pass execution on the client side the sizes of the transmit buffers feeding into those render passes are added up. The combination providing the minimum summed size corresponds to the minimum bandwidth between server and client. As previously stated the minimum may not actually be selected. That is a sub minimum combination or a combination within a margin of the minimum combination may be selected.

From 00 State When a cache line of server graphics data is read or written by the processors for the first time from command and graphics data and or frame buffer step the status bits of each cache line has a value 00 also referred to as state 00 . The cache line can be either read by the processors or written by the processor to change state. When the processor reads the cache line the status bits are updated to 01 state. If the cache line is written by the processor the status bits of the cache line are updated to 10 state.

From 01 State The status bits of the cache line read by the processor is updated to state 11 when the cache line is transmitted to client system . The status bits are reset to 00 state if the cache line was not transmitted due to bandwidth limitations.

From 11 State The status bits can have the value 11 when the cache line is transmitted to client system via transmit buffer s . The status bits are reset when the cache line is cleared due to memory reallocation or Direct Memory Access DMA operation.

From 10 State Once a cache line is written by processor the cache line cannot be transmitted via transmit buffer s and assumes a 10 state. The status bits of the cache line are reset due to memory reallocation or Direct Memory Access DMA operation.

In step method reads the data header to detect the data type. If method detects uncompressed data method proceeds to step . If method detects compressed data method proceeds to step . Graphics rendering of received data takes place in step . In step method decompresses the received data. In step data is placed in the frame buffer of client graphics memory for display.

Different proprietary techniques are currently used for remoting of graphics for thin client applications. These include Microsoft RDP Remote Desktop Protocol PCoIP VMware View and Citrix ICA. All of them rely on some kind of compression technique applied to the frame display buffer. Given the property that the frame buffer content changes incrementally a video compression scheme is most suited. Video compression is a technique which lends itself to adaptive compression based on instantaneous network bandwidth availability. Video compression technique does have a few limitations. These include 

The evolution of the graphics API has also created a relatively low albeit variable bandwidth interface at the API level. There are different resources surfaces indices vertices constant buffers shader programs textures needed by the GPU for processing. In graphics processing these resources get reused for multiple frames and enable cross frame caching. Vertex and texture data are the biggest consumers of the available video memory foot print but only a small percentage of the data is actually used and the utilization is spread across multiple frames.

The above described property of the 3D API is exploited to develop the scheme of API remoting. A server client co processing model has been developed to significantly trim the bandwidth requirements and enable API remoting. The server operates as a stand alone system with all the desktop graphics applications being run on the server. During the execution key information is gathered which identifies the minimal set of data needed for execution of the same on the client side. The data is then transferred over the network. The API interface bandwidth being variable one cannot guarantee adequate bandwidth availability. Hence an adaptive technique is adopted whereby when the API remoting bandwidth needs exceed the available bandwidth the display frame which was anyhow created on the server side to generate the statistics for minimal data transfer is video encoded and sent over the network. The decision is made at frame granularity.

Data in memory is stored in the form of cache lines. A bit map is maintained on the server side which tracks the status of each cache line. The bit map indicates

When a particular cache line is accessed and its status is 0 the accessed data is placed in a network ring and the status is updated to 1 . If the network ring overflows i.e. the required bandwidth for API remoting exceeds the available network bandwidth execution continues but does not update the bitmap network ring. The data in the network ring is trickled down to the client. After the creation of the final display buffer it is adaptively video encoded for transmission. Over time the bandwidth requirements for API remoting will gradually reduce and will eventually enable it.

A dedicated Wide Local Area Graphics Network WAGN LAGN is implemented to carry the graphics network data from the server to the client. A hybrid TCP UDP protocol is implemented to provide an optimal combination of speed and reliability. The TCP protocol is used to transmit the command control packets command buffers shader programs and the UDP protocol is used to transfer the data packets index buffers vertex buffers textures constant buffers .

To avoid the need for a graphics pre processor on the server software running on the server side can generate the traffic to be sent to the client for processing. The driver stack running on the server would identify the surfaces resources state required for processing the workload and push the associated data to the client over the system network. Conceptually the above mentioned bandwidth reduction scheme running the workload on the server using a software rasterizer and identifying the minimal data for processing on the client side can also be implemented and the short listed data can be transferred to the client.

Virtualization is a technique for hiding the physical characteristics of computing resources to simplify the way in which other systems applications or end users interact with those resources. The proposal lists different features which are implemented in the hardware to assist virtualization of the graphics resource. These include 

Processors have an instruction set defined to which the device is programmed. Different instruction sets have been developed over the years. The baseline scalar instruction set for OpenCL DirectCompute defines instructions which operate on one data entity. A vector instruction set defines instructions which operate on multiple data i.e. they are SIMD. 3D graphics APIs openGl DirectX define a vector instruction set which operate on 4 channel operands.

The scheme we have here defines a technique whereby the processor core carries out adaptive execution of scalar 4 D vector instruction sets with equal efficiency. The data operands read from the on chip registers or buffers in memory are 4 the width of the ALU compute block. The data is serialized into the compute block over 4 clocks. For vector instructions the 4 sets of data correspond to one register for the execution thread. For scalar instructions the 4 sets of data correspond to one register for four execution threads. At the output of the ALU the 4 sets of result data are gathered and written back to the on chip registers.

The processors of today have multiple pipeline stages in the compute core. Keeping the pipeline fed is a challenge for designers. Fetch latencies from memory and branching are hugely detrimental to performance. To address these problems a lot of complexity is added to maintain a high efficiency in the compute pipeline. Techniques include speculative prefetching and branch prediction. These solutions are required in single threaded scenarios. Multi threaded processors lend themselves to a unique execution model to mitigate these same set of problems.

While executing a program for a thread on the multi threaded processor only one instruction cache line made up of multiple instructions time. The clocks required to process the instructions in the instruction cache line match the instruction fetch latency. This ensures that in non branch scenarios the instruction fetch latency is hidden. On reception of the instruction cache line from memory it is pre decoded. If an unconditional branch instruction is is fetched at a present the fetch for the next instruction cache line is issued from the branch instruction pointer. If a conditional branch instruction is present the fetch of the next instruction cache line is deferred until the branch is resolved. Because of the presence of multiple threads this mechanism does not result in reduction of efficiency.

While pre decoding the instruction cache line another piece of information extracted is about all the data operands required from memory. A memory fetch for all these data operands is issued at this point.

Video Decoding Involves high level parsing for stream properties stream marker identification followed by variable length parsing of the bit stream data between markers. This is implemented in the programmable processor with specialized instructions for fast parsing. For the subsequent mathematical operations Inverse Quantization IDCT Motion Compensation De blocking De ringing a byte engine to accelerate operations on byte word operands has been defined.

Video Encoding Motion Estimation is carried out to determine the best match using a high density SAD4 4 instruction each of the four 4 4 blocks in the source are compared against the sixteen different 4 4 blocks in the reference . This is followed by DCT quantization and video decoding which is carried out in the byte engine. The subsequent variable length coding is carried out with special bit stream encoding and packing instructions.

An execution instruction pointer IP is maintained along with a flag bit for each thread in the group. The flag indicates that the thread is in the same flow as the current execution and hence execution only occurs for threads that have their flag set. The flag is set for all threads at the beginning of execution. Because of a conditional branch if a thread does not take the current execution code path its flag is turned off and its execution IP is set to the pointer it needs to move to. At merge points the execution IP of threads whose flags are turned off are compared with the current execution IP. If the IPs match the flag is set. At branch points if all currently active threads take the branch the current execution IP is set to the closest minimum positive delta from the current execution IP execution IP among all threads.

The APIs categorize all of the state defining the entire pipeline into multiple groups. Maintaining orthogonality of these state groups in hardware i.e. keeping the state groups independent of each other eliminates dependencies in the driver compiler and enables a state less driver.

The final stages of the 3D pipeline operate on pixels. After the pixels are shaded the output merger state defines how the pixel values are blended combined with the co located frame buffer values.

In our programmable output merger this state is implemented as a pair of subroutines run before and after the pixel shader execution. A prefix subroutine issues a fetch of the frame buffer values. A suffix subroutine has the blend instructions. The pixel shader outputs which are created into the general purpose registers need to be combined with the frame buffer values fetched by the prefix subroutine using the blend instructions in the suffix subroutine. To maintain orthogonality with the pixel shader state the pixel shader output registers are tagged as such and a CAM Content Addressable Memory is used to access these registers in the suffix subroutine.

This is a compiler technique to optimize minimize the registers used in a program. To carry out remapping of the registers used in the shader programs a bottoms up approach is used.

This pre compiled program is then parsed bottom to top. A register map is maintained for the general purpose registers GPR which tracks the mapping between the original register number and the remapped register number. Since the registers in shader programs are 4 channel the channel enable bits are also tracked in the register map.

When a register is used as a source in an instruction and is not found in the register map the register is remapped to an unused register and it is placed in the register map.

If a register used as a source destination in an instruction is found in the register map it is renamed accordingly.

A GPR is removed from the register map if it is a destination register after it has been renamed and all the enabled channels in the register map are written to as per the destination register mask .

Once the bottom to top compile is complete the program can be recompiled top to bottom one more time to use variable length instructions. Also some registers with only a sub set of channels enabled can be merged into one single register.

Although specific embodiments have been described and illustrated the described embodiments are not to be limited to the specific forms or arrangements of parts so described and illustrated. The embodiments are limited only by the appended claims.

