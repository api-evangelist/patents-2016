---

title: Efficient data reads from distributed storage systems
abstract: A method of distributing data in a distributed storage system includes receiving a file into non-transitory memory and dividing the received file into chunks. The chunks are data-chunks and non-data chunks. The method also includes grouping one or more of the data chunks and one or more of the non-data chunks in a group. One or more chunks of the group is capable of being reconstructed from other chunks of the group. The method also includes distributing the chunks of the group to storage devices of the distributed storage system based on a hierarchy of the distributed storage system. The hierarchy includes maintenance domains having active and inactive states, each storage device associated with a maintenance domain, the chunks of a group are distributed across multiple maintenance domains to maintain the ability to reconstruct chunks of the group when a maintenance domain is in an inactive state.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09514015&OS=09514015&RS=09514015
owner: Google Inc.
number: 09514015
owner_city: Mountain View
owner_country: US
publication_date: 20160324
---
This U.S. patent application is a continuation of and claims priority under 35 U.S.C. 120 from U.S. patent application Ser. No. 14 169 322 filed on Jan. 31 2014 which is hereby incorporated by reference in its entirety.

A distributed system generally includes many loosely coupled computers each of which typically includes a computing resource e.g. one or more processors and or storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s storage resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes. Alternatively Remote Direct Memory Access RDMA primitives may be used to transfer data from server hardware to client processes.

One aspect of the disclosure provides a method of distributing data in a distributed storage system. The method includes receiving a file into non transitory memory and dividing the received file into chunks using a computer processor in communication with the non transitory memory. The method also includes grouping one or more of the data chunks and one or more of the non data chunks in a group. One or more chunks of the group are capable of being reconstructed from other chunks of the group. The method further includes distributing chunks of the group to storage devices of the distributed storage system based on a hierarchy of the distributed storage system. The hierarchy includes maintenance domains having active and inactive states. Moreover each storage device is associated with a maintenance domain. The chunks of a group are distributed across multiple maintenance domains to maintain the ability to reconstruct chunks of the group when a maintenance domain is in an inactive state.

Implementations of the disclosure may include one or more of the following features. In some implementations the method further includes restricting the number of chunks of a group distributed to storage devices of any one maintenance domain.

In some implementations the method includes determining a distribution of the chunks of a group among the storage devices by determining a first random selection of storage devices that matches a number of chunks of the group and determining if the selection of storage devices is capable of maintaining accessibility of the group when one or more units are in an inactive state. In some examples when the first random selection of storage devices is incapable of maintaining accessibility of the group when one or more maintenance domains are in an inactive state the method further includes determining a second random selection of storage devices that match the number of chunks of the group or modifying the first random selection of storage devices by adding or removing one or more randomly selected storage devices. The method may further include determining the first random selection of storage devices using a simple sampling a probability sampling a stratified sampling or a cluster sampling.

In some implementations the method includes determining a distribution of the chunks of the group among the storage devices by selecting a consecutive number of storage devices equal to a number of chunks of the group from an ordered circular list of the storage devices of the distributed storage. When the selected storage devices are collectively incapable of maintaining the accessibility of the group when one or more maintenance domains are in an inactive state the method further includes selecting another consecutive number of storage devices from the ordered circular list equal to the number of chunks of the group. The method may include determining the ordered circular list of storage devices of the distributed storage system. Adjacent storage devices on the ordered circular list are associated with different maintenance domains. In some examples a threshold number of consecutive storage devices on the ordered circular list are each associated with different maintenance domains or are each in different geographical locations.

In some implementations the method includes determining the maintenance hierarchy of maintenance domains e.g. using the computer processor where the maintenance hierarchy has maintenance levels and each maintenance level includes one or more maintenance domains. The method also includes mapping each maintenance domain to at least one storage device. In some examples each maintenance domain includes storage devices powered by a single power distribution unit or a single power bus duct.

The method may include dividing the received file into stripes. Each file includes an error correcting code. The error correcting code is one of a nested code or a layered code. The non data chunks include code check chunks word check chunks and code check word check chunks.

Another aspect of the disclosure provides a system for distributing data in a distributed storage system. The system includes non transitory memory a computer processor and storage devices. The non transitory memory receives a file. The computer processor communicates with the non transitory memory and divides the received files into chunks. The chunks are data chunks and non data chunks. The computer processor further groups one or more of the data chunks and one or more the non data chunks in a group. One or more chunks of the group are capable of being reconstructed from other chunks of the group. The storage devices communicate with the computer processor and the non transitory memory. The computer processor stores the chunks of the group on the storage devices based on a maintenance hierarchy of the distributed storage system. The maintenance hierarchy includes maintenance domains having active and inactive states. Each storage device is associated with a maintenance domain. The computer processor distributes the chunks of a group across multiple maintenance domains to maintain accessibility of the group when a maintenance domain is in an inactive state.

In some examples the computer processor restricts a number of chunks of the group distributed to storage devices of any one maintenance domain. The computer processor may determine a distribution of the chunks of the group among the storage devices by determining a first random selection of storage devices matching a number of chunks of the group and by determining if the selection of storage devices is capable of maintaining accessibility of the group when one or more maintenance domains are in an inactive state. The computer processor may determine a second random selection of storage devices matching the number of chunks of the group when the first random selection of storage devices is incapable of maintaining accessibility of the group when one or more maintenance domains are in an inactive state.

In some implementations the computer processor modifies the first random selection of storage devices by adding and removing one or more randomly selected storage devices when the first random selection of storage devices is incapable of maintaining accessibility of the file when one or more maintenance domains are in an inactive state. The computer processor may determine the first random selection of storage devices using a simple sampling a probability sampling a stratified sampling or a cluster sampling.

In some examples the computer processor determines a distribution of the chunks among the storage devices by selecting a consecutive number of storage devices equal to a number of chunks of the group from an ordered circular list of the storage devices of the distributed storage system. Moreover the computer processor may select another consecutive number of storage devices from the ordered circular list equal to the number of chunks of the group when the selected storage devices are collectively incapable of maintaining the accessibility of the group when one or more maintenance domains are in an inactive state.

In some implementations the computer processor determines the ordered circular list of storage devices of the distributed storage system where adjacent storage devices on the ordered circular list are associated with different maintenance domains. Additionally or alternatively a threshold number of consecutive storage devices on the ordered circular list may each be associated with different maintenance domains. Additionally or alternatively a threshold number of consecutive storage devices on the ordered circular list may each be in different geographical locations.

In some examples the computer processor determines a maintenance hierarchy of maintenance domains and maps each maintenance domain to at least one storage device. The maintenance hierarchy has maintenance levels with each maintenance level including one or more maintenance domains. Each maintenance domain may include storage devices powered by a single power distribution unit or a single power bus duct.

In some implementations the computer processor divides the received file into stripes with each file including an error correcting code. The error correcting code is one of a nested code or a layered code. The non data chunks include code check chunks word check chunks and code check word check chunks.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

Storage systems include multiple layers of redundancy where data is replicated and stored in multiple data centers. Data centers house computer systems and their associated components such as telecommunications and storage systems . Data centers usually include backup power supplies redundant communications connections environmental controls to maintain a constant temperature and security devices. Data centers may be large industrial scale operations that use a great amount of electricity e.g. as much as a small town . Data centers may be located in different geographical locations e.g. different cities different countries and different continents . In some examples the data centers or a portion thereof require maintenance e.g. due to a power outage or disconnecting a portion of the storage system for replacing parts or a system failure or a combination thereof . The data stored in these data centers may be unavailable to users during the maintenance period resulting in the impairment or halt of a user s operations. Therefore it is desirable to provide a distributed storage system where a user is capable of retrieving stored data or reconstructing unhealthy or lost data despite the storage system or portions thereof undergoing maintenance or a system failure.

Referring to in some implementations a distributed storage system includes loosely coupled memory hosts e.g. computers or servers each having a computing resource e.g. one or more processors or central processing units CPUs in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks that may be used for caching data . A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the memory hosts through a network e.g. via RPC .

In some implementations the distributed storage system is single sided eliminating the need for any server jobs for responding to remote procedure calls RPC from clients to store or retrieve data on their corresponding memory hosts and may rely on specialized hardware to process remote requests instead. Single sided refers to the method by which most of the request processing on the memory hosts may be done in hardware rather than by software executed on CPUs of the memory hosts . Rather than having a processor of a memory host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the memory host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This single sided distributed storage architecture offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the memory hosts . This has the effect of decoupling the requirements for storage and CPU cycles that typical two sided distributed storage systems carry. The single sided distributed storage system can utilize remote storage resources regardless of whether there are spare CPU cycles on that memory host furthermore since single sided operations do not contend for server CPU resources a single sided system can serve cache requests with very predictable low latency even when memory hosts are running at high CPU utilization. Thus the single sided distributed storage system allows higher utilization of both cluster storage and CPU resources than traditional two sided systems while delivering predictable low latency.

In some implementations the distributed storage system includes a storage logic portion a data control portion and a data storage portion . The storage logic portion may include a transaction application programming interface API e.g. a single sided transactional system client library that is responsible for accessing the underlying data for example via RPC or single sided operations. The data control portion may manage allocation and access to storage resources with tasks such as allocating storage resources registering storage resources with the corresponding network interface controller setting up connections between the client s and the memory hosts handling errors in case of machine failures etc. The data storage portion may include the loosely coupled memory hosts 

The distributed storage system may store data in dynamic random access memory DRAM and serve the data from the remote hosts via remote direct memory access RDMA capable network interface controllers . A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . Both the memory hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the memory host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the memory host creates a client key for each registered memory region 

The single sided operations performed by the network interface controllers may be limited to simple reads writes and compare and swap operations none of which may be sophisticated enough to act as a drop in replacement for the software logic implemented by a traditional cache server job to carry out cache requests and manage cache policies. The transaction API translates commands such as look up or insert data commands into sequences of primitive network interface controller operations. The transaction API interfaces with the data control and data storage portions of the distributed storage system .

The distributed storage system may include a co located software process to register memory for remote access with the network interface controllers and set up connections with client processes . Once the connections are set up client processes can access the registered memory via engines in the hardware of the network interface controllers without any involvement from software on the local CPUs of the corresponding memory hosts .

Referring to in some implementations the distributed storage system includes multiple cells each cell including memory hosts and a curator in communication with the memory hosts . The curator e.g. process may execute on a computing processor e.g. server having a non transitory memory connected to the network and manage the data storage e.g. manage a file system stored on the memory hosts control data placements and or initiate data recovery. Moreover the curator may track an existence and storage location of data on the memory hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple memory hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file into stripes in a way that accesses of sequential segments are made to different physical memory hosts e.g. cells and or memory hosts . Striping is useful when a processing device requests access to data more quickly than a memory host can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses. In some implementations discussed in more detail below each stripe may be further divided into groups G e.g. including chunks where accesses of sequential groups G are made to different physical memory hosts . Grouping of segments within a stripe may also be useful when a processing device requests access to data more quickly than a memory host can provide access. By providing segment access of a group G on multiple devices multiple segments of a group G can be accessed concurrently. This also provides more data access throughput which avoids causing the processor to idly wait for data accesses thus improving the performance of the system .

In some implementations the transaction API interfaces between a client e.g. with the client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the transaction API may find the storage location of certain data on memory host s and obtain a key that allows access to the data . The transaction API communicates directly with the appropriate memory hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a memory host is non operational or the data was moved to a different memory host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata may include a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions. For example the metadata can specify which parts of a file are stored at which data centers where redundant copies of data are stored which data chunks D and code chunks C form codewords and the like.

Referring to data may be one or more files . The curator may divide each file into a collection of stripes with each stripe being encoded independently from the remaining stripes . Each stripe may be encoded and stored on different memory hosts . As shown in each stripe is divided into data chunks D and non data chunks C based on an encoding level e.g. Reed Solomon Codes layered codes or nested codes or other hierarchical codes. The non data chunks C may be code chunks C e.g. for Reed Solomon codes . In other examples the non data chunks C may be code check chunks CC word check chunks WC and code check word check chunks CCWC for layered or nested coding . A data chunk D is a specified amount of data . In some implementations a data chunk D is a contiguous portion of data from a file . In other implementations a data chunk D is one or more non contiguous portions of data from a file . For example a data chunk D can be 256 bytes or other units of data .

A damaged chunk e.g. data chunk D or non data chunk C is a chunk containing one or more errors. Typically a damaged chunk is identified using an error detecting code . For example a damaged chunk can be completely erased e.g. if the chunk was stored in a hard drive destroyed in a hurricane or a damaged chunk can have a single bit flipped. A healthy chunk is a chunk that is not damaged. A damaged chunk can be damaged intentionally for example where a particular memory host is shut down for maintenance. A damaged chunk may be a missing or unavailable chunk. In that case damaged chunks can be identified by identifying chunks that are stored at memory hosts that are being shut down.

The non data chunks C of a file include the error correcting code chunk . The error correcting code chunks include a chunk of data based on one or more data chunks D. In some implementations each code chunk C is the same specified size e.g. 256 bytes as the data chunks D. The code chunks C are generated using an error correcting code e.g. a Maximal Distance Separable MDS code. Examples of MDS codes include Reed Solomon codes. Various techniques can be used to generate the code chunks C. For example an error correcting code can be used that can reconstruct d data chunks D from any set of unique healthy chunks either data chunks D or code chunks C .

A codeword is a set of data chunks D and code chunks C based on those data chunks D. If an MDS code is used to generate a codeword containing d data chunks D and c code chunks C then all of the chunks data or code can be reconstructed as long as any d healthy chunks data or code are available from the codeword.

Referring to in layered coding and nested coding techniques an encoded data block includes a data block having data chunks D and error correcting code chunks i.e. non data chunks C that is being stored is viewed as forming a two dimensional R C array. There are X code chunks C for each column C called code check chunks CC that can be used to reconstruct X or fewer damaged chunks per column C. There are Y code chunks C called word check chunks WC for the entire 2 D array. When there are more than X damaged chunks in one or more columns C the word check chunks WC are used in addition to other healthy chunks to reconstruct damaged chunks . Although some examples described in this specification illustrate encoded data blocks i.e. data blocks and code chunks C i.e. non data chunks C as forming a two dimensional array it is possible for coding techniques to create encoded data blocks configured differently. For instance different columns can have different numbers of code check chunk CC and columns that contain word check chunks WC can have different numbers of rows R than columns C that contain data chunks D and code check chunks C.

The codes C can be used to store data across memory hosts by allocating each column C of data chunks D to a data center. Each chunk within the column C can be allocated to a memory host within a data center. Then if X or fewer chunks are lost at a data center the chunks can be reconstructed using only intra data center communication e.g. so no other data centers have to provide data in performing reconstruction . If more than X chunks are lost in one or more data centers then the Y word check chunks WC are used to attempt reconstruction. Thus inter data center communication which may be more expensive e.g. slower than intra data center communication is only needed when more than X chunks are damaged within a single data center.

The codes can also be used within a single data center. Instead of allocating different columns C to different data centers the encoding system stores all of the columns C at a single data center. The data chunks D and code chunks C can be stored at distinct memory hosts within that data center. This is useful for example where reading data from memory hosts during reconstruction is expensive e.g. time consuming so that the encoding system can read fewer chunks during reconstruction than would be needed using conventional coding techniques. Small numbers of damaged chunks can be reconstructed by reading small numbers of other chunks code check chunks CC and other data chunks D in a column C and large numbers of damaged chunks can be reconstructed using the word check chunks WC when needed. In some examples the curator groups data chunks D and certain non data chunks C in a group G in a manner that allows the system to reconstruct missing chunks from other chunks of the group G. The group G may include one or more columns C or portions thereof.

Referring to in some implementations a layered coding technique shows data chunks D and code chunks C forming codewords. An error correcting code is in systematic form if resulting codewords can be partitioned into two sets of chunks one set including the data chunks D and one set including the code chunks C. A code in systematic form is Maximal Distance Separable MDS if it has N code chunks C and it can correct any N damaged chunks . A layered code is created from two MDS codes e.g. Reed Solomon codes or parity codes in systematic form. One code is used to create the code check chunks CC and the other code is used to create the word check chunks WC.

Referring to the example shown in a data block includes data chunks D labeled D D that are encoded with a layered code. In a first columns of data chunks D is shown D D. Two code check chunks CC are shown for the columns C and C. C and C are based on D D. Thus D D and C C form a codeword. In an encoded data block having the data block D D and six code chunks C C is shown. C C are based on D D. Thus D D and C C form a codeword.

Together D D and C and C form a codeword. The word check chunks C C are shown in the last column to the right. Together D D and C C form a codeword. C and C can be generated based on C C so that C and C and C C form a codeword.

In the example shown in the word check chunks WC fill a whole column C. However layered codes can be created with an arbitrary number of full Columns C of word check chunks WC plus an optional partial column of word check chunks WC. If the data chunks D and the word check chunks WC do not fill an integral number of columns C empty zero valued chunks can be added to the 2D array. Those chunks do not have to actually be stored and they would never be in error.

In general a layered code with X code check chunks CCper column C and N word check chunks WC can reconstruct up to X damaged chunks per column while performing only intra column C communication. If after reconstructing those damaged chunks N or fewer damaged chunks remain in the 2D array within the data plus word check chunks WC portion of the 2D array the damaged chunks can be reconstructed using the word check chunks WC and the code check chunks CC. This is true because N or fewer damaged chunks in the data chunks D plus the word check chunks WC can be reconstructed using only the word check chunks WC. Then if any code check chunks CCare damaged they can be reconstructed from the data chunks D of their respective column C.

Referring to in some implementations the curator distributes data using a layered code. The curator receives a data block that includes data chunks D step . For example the data block can be from a file that is being stored. The data block can include m ndata chunks C mis a number of data rows and nis a number of data columns and mand nare greater than or equal to one. The encoded block includes m n chunks that include m n where m is the total number of rows R of data chunks D and non data chunks C and n is the number of columns C of data chunks D and non data chunks C m and n are greater than or equal to one. The curator generates one or more columns C of word check chunks WC using a first error correcting code in systematic form and the data chunks D step . The columns C of word check chunks WC can have different numbers of word check chunks WC in the column C. The data chunks D and the word check chunks WC taken together form a codeword.

For each column C of one or more columns C of the data chunks D the curator generates one or more code check chunks CC for the column C using a second error correcting code in systematic form and the data chunks D of the column C step . The first and second error correcting codes can be distinct. The columns C can have different numbers of code check chunks CC. The system can also generate code check chunks CC for the column C of word check chunks WC. The system stores the data chunks D code check chunks CC and word check chunks WC step . In some implementations the system allocates each columns C and or the code check chunks CC within a column C to a distinct group of memory host . In other implementations the system stores the data chunks D and the code chunks C at a same group of memory host e.g. a single data center. The system may group data chunks D and certain code check chunks CC and word check chunks WC in groups G where an unhealthy chunk can be restored from one or more other chunks of the group G. Therefore the system stores chunks of a group G at different memory hosts .

When the system allocates a column C of chunks to a group of memory hosts the code check chunks CC can be generated at different locations. For example the code check chunks CC can be generated by a central encoding system e.g. the server of that performs the allocation or by the group of memory hosts after receiving a column C of data chunks D. At each group of memory hosts each of the allocated data chunks D code check chunks CC and word check chunks WC can be stored at a distinct memory host .

When the system identifies a damaged data chunk D at a first group of memory hosts the system attempts to reconstruct the damaged chunk without communication with other groups of memory hosts using the code check chunks CC of the group G of chunks . In some cases the system reconstructs as many other damaged data chunks D from the group G of chunks at the first group of memory hosts as is possible using the code check chunks CC and any healthy data chunks D allocated to the first group of memory hosts from the group G of chunks . If the system determines that the damaged chunk cannot be reconstructed without communicating with other groups of memory hosts that have other groups G of chunks the system identifies e.g. by requesting and receiving healthy chunks from other groups of memory hosts that have other groups G of chunks so that at least m n healthy chunks are available where the healthy chunks are data chunks D word check chunks WC or both and reconstructs the damaged data chunk D using the healthy chunks .

Referring to in some implementations a nested coding technique shows data chunks D and code chunks C that form a codeword. As shown the nested coding technique is a two dimensional 2D nested coding technique but a three dimensional 3D nested coding technique may also be applied.

Nested coding techniques differ from layered coding techniques by creating a different relationship between the code check chunks CC and the word check chunks WC. A 2D nested code is created from an arbitrary linear MDS code in systematic form. Word check chunks WC that are based on a data block are partitioned into two groups the first group including X code chunks C and the second group including N code chunks C. The encoded data block is viewed as forming an array of columns C and X code chunks C in the first group are used to create X column chunks per column by splitting them into separate components per column split code check chunks CC . The N code chunks C in the second group form word check chunks WC.

For example shows a data block D D and code chunks C C C that are based on the data block D D . The data chunks D D D and the code chunks C C C form a codeword. The code chunks C are partitioned into a first group that includes C C and a second group that includes C C. C C are split to form split code check chunks CC. C C are used as word check chunks WC.

The split code check chunks CC for C for each column C are generated similarly but using C instead of C. As a result C is a linear combination of C C and C is a linear Combination of C C . That is 0 0 and 1 1 1 2 

The chunks denoted as in can be generated in various ways e.g. as described further below with reference to .

In the example of the resulting encoded data block includes 42 data chunks D and 8 code chunks C. Referring to the original code used to create the encoded block the code chunks C belong to one of two groups as described above X 2 of which are in the first group and N 6 of which are in the second group. Whenever there are two or fewer X or fewer damaged chunks within one of the first seven columns the damaged chunks can be corrected using the healthy chunks of the columns C and the split code check chunks CC for the column C. To see this let j denote the column C including the two or fewer damaged chunks and consider the codeword obtained by zeroing out all the data chunks D from columns C other than j. In that codeword C C and C C . As a result the two or fewer damaged chunks in other columns as containing all zero data chunks D and by viewing the word check chunks WC as being damaged.

In the example shown in the word check chunks WC fully fill an entire column C the column to the right . 2D nested codes can be created with an arbitrary number of columns C of word check chunks WC. The columns C of word check chunks WC can have the same number of rows R as the columns of data chunks D or different numbers of rows R and the columns C of word check chunks WC can have different numbers of rows R from each other. Columns C of word check chunks WC can but do not have to have code check chunks CC i.e. code check word check chunks CCWC. Increasing the number of word check chunks WC improves the reliability of the stored data but uses more storage at memory hosts . In general for nested codes columns C include either data chunks D or word check chunks WC and not both.

In general a 2D nested code with X split code check chunks CC per column C and N word check chunks WC can be used to reconstruct X damaged chunks per column C in those columns that include data chunks D while performing only intra columns communication which is typically e.g. intra data center communication . In reconstructing multiple damaged chunks within the encoded block those damaged chunks are typically reconstructed first because intra column communication is less expensive than inter column communication but other damaged chunks may remain. If after reconstructing damaged chunks within columns N X or fewer other chunks are still damaged because they were not able to be reconstructed using intra column communication those other damaged chunks can be reconstructed using the word check chunks WC and the split code check chunks CC. The word check chunks WC in the first group C and C in can be determined from the split code check chunks CC e.g. using the formula Ci C i j even though those word check chunks WC are not explicitly stored.

To see this let Z denote the number of word check chunks WC that are damaged and let Y denote the number of word check chunks WC in the first group that cannot be reconstructed from their corresponding split code check chunks CC according to the formula Ci C to split code check chunks CC being damaged. Using that formula X Y word check chunks WC from the first group can be determined resulting in a codeword e.g. the one shown in with Y damaged word check chunks WC in the first group and Z damaged word check chunks WC in the second group. Because there are at most N X total damaged chunks there are at most N X Y Z damaged data chunks D. Thus it is possible to use the resulting codeword to reconstruct all of the damaged chunks as it includes at most N X Y Z Y Z N X damaged chunks .

Referring to in some implementations a resulting encoded block includes code check chunks CC for the word check chunks WC i.e. code check word check chunks CCWC . Compared to the encoded block of the encoded block of includes the code check chunks C and C CC in place of the locations marked with in . This is one way to provide for reconstructing damaged word check chunks WC without relying on inter column communication. The code check chunks C and C CC can be generated in various ways. For example those code check chunks CC can be generated based on C C in the same manner that C and C are generated based on D D. The resulting encoded block of using the example nested code can be used to reconstruct up to eight damaged chunks after performing intra column reconstruction whereas the resulting encoded block of using the example layered code can be used to reconstruct up to six damaged chunks after performing intra column reconstruction. Code check chunks C can be added for any number of columns that include word check chunks WC.

Referring to in some implementations the curator distributes data using a nested code . The system receives a data block step . The data block can include m ndata chunks C mis a number of data rows and nis a number of data columns and mand nare greater than or equal to one. The encoded block includes m n chunks that include m n where m is the total number of rows R of data chunks D and non data chunks C and n is the number of columns C of data chunks D and non data chunks C m and in are greater than or equal to one. The system generates one or more columns C of word check chunks WC using a first linear error correcting code in systematic form and the data chunks D step . The word check chunks WC and the data chunks D of the same row R form a codeword. For each of mrow of data chunks C the system generates one or more split code check chunks CC for the Column C step . The split code check chunks CC are generated so that a linear combination of n split code check chunks CC from different columns C forms a first word check chunk WC of a first codeword including the data chunks D and the m word check chunks WC. The first word check chunk WC and any other word check chunks WC resulting from a linear combination of split code check chunks CC from different columns C forms a codeword with the data chunks D and the word check chunks WC generated in step . For example the split code check chunks CC for each columns C can be generated using a splitting error correcting code and the mdata chunks D or the word check chunks WC wherein the splitting error correcting code includes a splitting generator matrix that codes the same as a generator matrix for the first linear error correcting code applied to the data chunks D with the data chunks D zeroed out for columns C other than the column C.

The system stores the column C of data chunks D and the split code check chunks CC and the word check chunks WC step . In some implementations the system stores all the chunks at a single group of memory hosts . In some other implementations the system allocates each column C to a distinct group of memory hosts . In some implementations the system groups chunks capable of being reconstructed from other chunks within the group G and allocates the chunks of the group G to distinct groups of memory hosts .

When the system identifies one or more damaged chunks the system can reconstruct the damaged chunks using the split code check chunks CC and the word check chunks WC. Typically the system attempts to reconstruct damaged chunks using the split code check chunks CC and other data chunks in the same column C. If after reconstructing damaged chunks using only the split code check chunks CC some damaged chunks remain the system uses the word check chunks WC for reconstruction including the word check chunks WC that can be determined by determining a linear combination of the split code check chunks CC. In addition if after reconstructing damaged chunks using only split code check chunks CC of chunks of a group G some damaged chunks remain the system uses chunks from other groups G of chunks to reconstruct the damaged chunks .

Referring back to in some implementations file descriptors stored by the curator contain metadata such as the file map which maps the stripes to data chunks and non data chunks as appropriate stored on the memory hosts . To open a file a client sends a request to the curator which returns a file descriptor . The client uses the file descriptor to translate file chunk offsets to remote memory locations . The file descriptor may include a client key e.g. a 32 bit key that is unique to a chunk on a memory host and is used to RDMA read that chunk . After the client loads the file descriptor the client may access the data of a file via RDMA or another data retrieval method.

The curator may maintain status information for all memory hosts that are part of the cell . The status information may include capacity free space load on the memory host latency of the memory host from a client s point of view and a current state. The curator may obtain this information by querying the memory hosts in the cell directly and or by querying a client to gather latency statistics from a client s point of view. In some examples the curator uses the memory host status information to make rebalancing draining recovery decisions and allocation decisions.

The curator s may allocate chunks in order to handle client requests for more storage space in a file and for rebalancing and recovery. The curator may maintain a load map of memory host load and liveliness. In some implementations the curator allocates a chunk by generating a list of candidate memory hosts and sends an allocate chunk request to each of the candidate memory hosts . If the memory host is overloaded or has no available space the memory host can deny the request . In this case the curator selects a different memory host . Each curator may continuously scan its designated portion of the file namespace examining all the metadata every minute or so. The curator may use the file scan to check the integrity of the metadata determine work that needs to be performed and or to generate statistics. The file scan may operate concurrently with other operations of the curator . The scan itself may not modify the metadata but schedules work to be done by other components of the system and computes statistics.

In some implementations the processor may group one or more of the data chunks D and one or more of the non data chunks C in a group G. The one or more chunks of the group G are capable of being reconstructed from other chunks of the group G. Therefore when reconstructing chunks of a group G the curator reads chunks of the group G to reconstruct damaged chunks within the group G. This allows more efficient reconstruction of missing chunks and the number of chunks being read is reduced. Specifically reducing the number of chunk reads can decrease the cost of the read since fewer reads to hardware devices e.g. memory hosts are performed and reduce the latency of the reconstruction since slow devices are less likely to be accessed.

Referring to the curator may determine a maintenance hierarchy of the distributed storage system to identify the levels e.g. levels 1 5 at which maintenance may occur without affecting a user s access to stored data . Maintenance may include power maintenance cooling system maintenance networking maintenance updating or replacing parts or other maintenance or power outage affecting the distributed storage system .

The maintenance hierarchy identifies levels e.g. levels 1 5 of maintenance domains where each maintenance domain may be in an active state or an inactive state. Each memory host of the distributed storage system is associated with one or more maintenance domain . Moreover the processor maps the association of the memory hosts with the maintenance domains and their components . shows a strict hierarchy where each component depends on one other component while shows a non strict hierarchy where one component has more than one input feed. In some examples the processor stores the maintenance hierarchy on the non transitory memory of the processor . For example the storage resource is mapped to a rack which is mapped to a bus duct which in turn is mapped to a power module distribution center which in turn is mapped to a power plant . The processor determines based on the mappings of the components what memory hosts are inactive when a component is undergoing maintenance. Once the system maps the maintenance domains to the storage resources the system determines a highest level e.g. levels 1 5 at which maintenance can be performed while maintaining data availability.

A maintenance domain includes a component undergoing maintenance and any components depending from that component . Therefore when one component is undergoing maintenance that component is inactive and any component in the maintenance domain of the component is also inactive. As shown in level 1 components may include the storage resources level 2 components may include racks level 3 components may include bus ducts level 4 components may include power module distribution centers and level 5 components may be the power plants providing power to levels 1 to 4 components. Other component distribution may also be available. When a memory host is undergoing maintenance a level 1 maintenance domain includes the memory host and that storage device is inactive. When a rack is undergoing maintenance a level 2 maintenance domain that includes the rack and memory hosts depending from the rack are in an inactive state. When a bus duct is undergoing maintenance a level 3 maintenance domain that includes the bus duct and any components in levels 1 and 2 that depend from the bus duct are in an inactive state. When a power module distribution center is undergoing maintenance a level 4 maintenance domain that includes the power module distribution center and any components in levels 1 to 3 depending from the power module distribution center are in an inactive state. Finally when the power plant is undergoing maintenance a level 5 maintenance domain including any power module distribution centers bus ducts racks and memory hosts depending on the power plant are inactive and therefore a user cannot access data located within the level 1 maintenance domain

In some examples as shown in a non strict hierarchy component has dual feeds i.e. the component depends on two or more other components . For example a bus duct may have a feed from two power modules and or a rack may have a dual feed from two bus ducts . As shown a first maintenance domain may include two racks and where the second rack includes two feeds from two bus ducts . Therefore the second rack is part of two maintenance domains and . Therefore the higher levels of the maintenance hierarchy are maintained without causing the loss of the lower levels of the maintenance hierarchy . This causes a redundancy in the system which allows for data accessibility. In particular the power module distribution center may be maintained without losing any of the bus ducts depending from it. In some examples the racks include a dual powered rack that allows the maintenance of the bus duct without losing power to the dual powered racks depending from it. In some examples maintenance domains that are maintained without causing outages are ignored when distributing chunks to allow for maintenance however the ignored maintenance domains may be included when distributing the chunks since an unplanned outage may still cause the loss of chunks .

In some examples as shown in the maintenance hierarchy is a cooling hierarchy or a combination of a power hierarchy and a cooling hierarchy . The cooling hierarchy maps a cooling device to the racks that it is cooling. As shown a cooling device may cool one or more racks . The processor stores the association of the memory hosts with the cooling maintenance domains . In some implementations the processor considers all possible combinations of maintenance that might occur within the storage system to determine a hierarchy or a combination of hierarchies 

Therefore when a component in the storage system is being maintained that component and any components that are mapped to or depending from that component are in an inactive state. A component in an inactive state is inaccessible by a user while a component in an active state is accessible by a user allowing a user to access data stored on that component or on a memory host mapped to that component . As previously mentioned during the inactive state a user is incapable of accessing the memory hosts associated with the maintenance domains undergoing maintenance and therefore the user is incapable of accessing the files i.e. chunks which include stripe replicas data chunks D and code chunks C .

In some implementations the processor restricts a number of chunks within a group G that are distributed to memory hosts of any one maintenance domain e.g. based on the mapping of the components . Therefore if a level 1 maintenance domain is inactive the processor maintains accessibility i.e. the unhealthy chunks can be reconstructed to the group G although some chunks may be inaccessible. In some examples for each group G of chunks the processor determines a maximum number of chunks that are placed within any memory host within a single maintenance domain so that if a maintenance domain associated with the memory host storing chunks for a file is undergoing maintenance the processor may still retrieve the chunks within the group G. The maximum number of chunks ensures that the processor is capable of reconstructing the number of chunks of the group G although some chunks may be unavailable. In some examples the maximum number of chunks of a group G is set to a lower threshold to accommodate for any system failures while still being capable of reconstructing the group G of chunks . When the processor places chunks on the memory hosts the processor ensures that within a group G of chunks of a stripe no more than the maximum number of chunks are inactive when a single maintenance domain undergoes maintenance.

Referring to in some implementations the processor determines a distribution of the chunks of a group G among the memory hosts . In some examples the processor makes a first random selection of memory hosts from an available pool of storage devices to store the chunks of a group G. The processor selects a number of memory hosts e.g. selected memory host S equal to the number of chunks in the group G. Next the processor determines if the selection of selected memory hosts S is capable of maintaining accessibility of the group G i.e. the chunks of the group G are available when one or more or a threshold number of maintenance domains are in an inactive state. The random selection has the goal of allowing reconstruction of the group G if maintenance occurs on one of the maintenance components .

Referring to in some examples when the processor determines that the first random selection of selected memory hosts S is incapable of maintaining accessibility of the group G when one or more or a threshold number of maintenance domains are in an inactive state the processor determines a second random selection of selected memory hosts S that matches the number of chunks of the group G. Then the processor determines if the second random selection of selected memory hosts S is capable of maintaining accessibility of the group G when one or more or a threshold number of maintenance domains are in an inactive state. If the processor determines that the second random selection is incapable of maintaining accessibility of the group G when one or more or a threshold number of maintenance domains are in an inactive state the processor continues to make random selections of selected memory hosts S until the processor identifies a random selection of selected memory hosts S that is capable of maintaining accessibility of the group G.

Referring to in some implementations when the processor determines that the first random selection of selected memory hosts S is incapable of maintaining accessibility of the group G when one or more or a threshold number of maintenance domains are in an inactive state the processor modifies the first random selection of selected memory hosts S by adding one or more randomly selected memory hosts S and removing a corresponding number of different memory hosts S. The processor then determines if the updated first random selection is capable of maintaining accessibility of the group G when one or more or a threshold number of maintenance domains are in an inactive state. If the processor determines that updated first random selection is incapable of maintaining accessibility of the group G when one or more or a threshold number of maintenance domains are in an inactive state the processor updates the selection of selected memory hosts S by adding and removing one or more randomly selected memory host S. The processor continues to update the random selection of memory hosts until the processor determines that the selected memory hosts S are capable of maintaining accessibility of the group G of chunks during maintenance of the distributed storage system . Once the processor makes that determination the processor moves to the next stripe or file to determine a distribution of the next stripe . In some implementations the processor determines the random selection of selected memory hosts S by using a probability sampling a simple sampling a stratified sampling a cluster sampling or a combination therefrom.

Referring to in some implementations the processor determines a number of chunks in a group G of chunks . The processor then selects a selected list having a consecutive number of memory hosts equal to a number of chunks of the file from an ordered circular list of memory hosts of the distributed storage system the ordered circular list beginning at a first memory host . The list may be stored on the non transitory memory of the processor . The processor then determines if the selected memory hosts from the selected list are collectively incapable of maintaining accessibility of the group G of chunks when one or more or a threshold number of maintenance domains are in an inactive state. If the processor determines that the selected memory hosts are collectively incapable of maintaining the accessibility of the group G of chunks when one or more or a threshold number of maintenance domains are in an inactive state the processor selects another selected list having a consecutive number of memory hosts from the ordered circular list equal to the number of chunks of the stripe or file . In some examples the processor moves to a second memory host 1 after the first memory host in the ordered circular list when the processor determines that memory hosts of the selected list are collectively incapable of maintaining the accessibility of the group G of chunks . In other examples the processor moves a predetermined number of positions down the ordered circular list . In some implementations the processor determines the ordered circular list of memory hosts of the storage system where adjacent memory hosts or a threshold number of consecutive memory hosts on the ordered circular list are associated with different maintenance domains . Additionally or alternatively the processor determines the ordered circular list of memory hosts of the storage system where adjacent memory hosts or a threshold number of consecutive memory hosts on the ordered circular list is each in different geographical locations. In some examples the memory hosts on the ordered circular list are arranged so that different maintenance domains cause the dispersion of data sequentially along the ordered list . For example as shown in the list may not contain sequentially memory hosts dependent from the same bust duct . Instead two sequential memory hosts on the list are from different maintenance domains to make sure that data accessibility is maintained.

Referring to in some implementations a method of distributing data in a distributed storage system includes receiving a file into non transitory memory and dividing the received file into chunks using a computer processor in communication with the non transitory memory . The method also includes grouping one or more of the data chunks and one or more of the non data chunks in a group G. One or more chunks of the group G are capable of being reconstructed from other chunks of the group G. The method further includes distributing chunks of the group G to storage devices of the distributed storage system based on a hierarchy of the distributed storage system . The hierarchy includes maintenance domains having active and inactive states. Moreover each storage device is associated with a maintenance domain . The chunks of a group G are distributed across multiple maintenance domains to maintain the ability to reconstruct chunks of the group G when a maintenance domain is in an inactive state.

In some implementations the method further includes restricting the number of chunks of a group G distributed to storage devices of any one maintenance domain . The method further includes determining a distribution of the chunks of a group G among the storage devices by determining a first random selection of storage devices that matches a number of chunks of the group G and determining if the selection of storage devices is capable of maintaining accessibility of the group G when one or more units are in an inactive state. In some examples when the first random selection of storage devices is incapable of maintaining accessibility of the group G when one or more maintenance domains are in an inactive state the method further includes determining a second random selection of storage devices that match the number of chunks of the group G or modifying the first random selection of storage devices by adding or removing one or more randomly selected storage devices . The method may further include determining the first random selection of storage devices using a simple sampling a probability sampling a stratified sampling or a cluster sampling.

In some implementations the method further includes determining a distribution of the chunks of the group G among the storage devices by selecting a consecutive number of storage devices equal to a number of chunks of the group G from an ordered circular list of the storage devices of the distributed storage. When the selected storage devices are collectively incapable of maintaining the accessibility of the group G when one or more maintenance domains are in an inactive state the method further includes selecting another consecutive number of storage devices from the ordered circular list equal to the number of chunks of the group G. Additionally or alternatively the method further includes determining the ordered circular list of storage devices of the distributed storage system . Adjacent storage devices on the ordered circular list are associated with different maintenance domains . In some examples a threshold number of consecutive storage devices on the ordered circular list are each associated with different maintenance domains or are each in different geographical locations.

In some implementations the method further includes determining the maintenance hierarchy of maintenance domains e.g. using the computer processor where the maintenance hierarchy has maintenance levels and each maintenance level includes one or more maintenance domains . The method also includes mapping each maintenance domain to at least one storage device . In some examples each maintenance domain includes storage devices powered by a single power distribution unit or a single power bus duct .

The method may further include dividing the received file into stripes . Each file includes an error correcting code . The error correcting code is one of a Reed Solomon code a nested code or a layered code. The non data chunks include code check chunks CC word check chunks CC and code check word check chunks CCWC.

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter affecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

