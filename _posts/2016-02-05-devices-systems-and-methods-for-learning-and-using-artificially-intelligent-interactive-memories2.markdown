---

title: Devices, systems, and methods for learning and using artificially intelligent interactive memories
abstract: Aspects of the disclosure generally relate to computing devices and may be generally directed to devices, systems, methods, and/or applications for learning conversations among two or more conversation participants, storing this knowledge in a knowledgebase (i.e. neural network, graph, sequences, etc.), and enabling a user to simulate a conversation with an artificially intelligent conversation participant.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582762&OS=09582762&RS=09582762
owner: 
number: 09582762
owner_city: 
owner_country: 
publication_date: 20160205
---
The disclosure generally relates to electronic devices. The disclosure includes devices apparatuses systems and related methods for providing advanced learning anticipation simulation and or other functionalities to enable artificially intelligent interactive memories.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

Still pictures are commonly used to record memories of persons or objects. One of still picture s limitations is the fact that it is still and that it provides no interactivity. Motion pictures are also commonly used to record memories of persons or objects including the time dimension. One of motion picture s limitations is the fact that the only operations a user can perform on a motion picture are playing pausing forwarding and rewinding the sequence of pictures. Essentially a user can only watch persons or objects depicted in a still or motion picture without being able to interact with them. Still and motion pictures fail to provide a way in which a user may want to experience content through the use of artificial intelligence on today s computing mobile and or embedded devices.

In some aspects the disclosure relates to a system for learning artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises a server including one or more processor circuits coupled to a memory unit. The system may further include a first computing device including a picture capturing device configured to capture a stream of digital pictures of a first conversation participant and include a sound capturing device configured to capture a stream of digital sound samples of the first conversation participant the first computing device coupled to the server via a network. The system may further include a second computing device including a picture capturing device configured to capture a stream of digital pictures of a second conversation participant and include a sound capturing device configured to capture a stream of digital sound samples of the second conversation participant the second computing device coupled to the server via the network. The one or more processor circuits may be configured to detect the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may be further configured to generate a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The one or more processor circuits may be further configured to cause the memory unit to store the first round of conversational exchange the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In certain embodiments the picture capturing device of the first computing device or the picture capturing device of the second computing device includes a motion picture camera. In further embodiments the stream of digital pictures of the first conversation participant includes the first conversation participant s visual expressions or communication and the stream of digital pictures of the second conversation participant includes the second conversation participant s visual expressions or communication. In further embodiments the stream of digital pictures of the first conversation participant or the stream of digital pictures of the second conversation participant includes a digital motion picture. The digital motion picture may include a MPEG motion picture an AVI motion picture a FLV motion picture a MOV motion picture a RM motion picture a SWF motion picture a WMV motion picture a DivX motion picture or a digitally encoded motion picture. In further embodiments the stream of digital pictures of the first conversation participant or the stream of digital pictures of the second conversation participant includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In some embodiments the sound capturing device of the first computing device or the sound capturing device of the second computing device includes a microphone. In further embodiments the stream of digital sound samples of the first conversation participant includes the first conversation participant s verbal expressions or communication and the stream of digital sound samples of the second conversation participant includes the second conversation participant s verbal expressions or communication. In further embodiments the stream of digital sound samples of the first conversation participant or the stream of digital sound samples of the second conversation participant includes a digital sound. The digital sound may include a WAV digital sound a WMA digital sound an AIFF digital sound a MP3 digital sound a RA digital sound a OGG digital sound or a digitally encoded sound. In further embodiments the stream of digital sound samples of the first conversation participant or the stream of digital sound samples of the second conversation participant includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the stream of digital pictures of the first conversation participant and the stream of digital sound samples of the first conversation participant are captured simultaneously. In further embodiments the stream of digital pictures of the second conversation participant and the stream of digital sound samples of the second conversation participant are captured simultaneously. In further embodiments the stream of digital pictures of the first conversation participant the stream of digital sound samples of the first conversation participant the stream of digital pictures of the second conversation participant and the stream of digital sound samples of the second conversation participant are captured simultaneously.

In certain embodiments the first conversation participant s first conversational activity or the second conversation participant s first conversational activity includes at least one of a speaking a silent facial expression a silent body movement a motionless silence an absence from the conversation or a conversational action. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the detecting the first conversation participant s speaking includes recognizing the first conversation participant s speech in the stream of digital sound samples of the first conversation participant. In further embodiments the detecting the first conversation participant s speaking includes determining a beginning and an end of the first conversation participant s speaking. The determining the beginning of the first conversation participant s speaking may include recognizing the first conversation participant s speech after a threshold period of silence in the stream of digital sound samples of the first conversation participant. The determining the end of the first conversation participant s speaking may include recognizing a threshold period of silence after the first conversation participant s speech in the stream of digital sound samples of the first conversation participant. In further embodiments the detecting the second conversation participant s silent facial expression includes recognizing the second conversation participant s facial expression in the stream of digital pictures of the second conversation participant and recognizing the second conversation participant s silence in the stream of digital sound samples of the second conversation participant. In further embodiments the detecting the second conversation participant s silent body movement includes recognizing the second conversation participant s body movement in the stream of digital pictures of the second conversation participant and recognizing the second conversation participant s silence in the stream of digital sound samples of the second conversation participant. In further embodiments the detecting the second conversation participant s motionless silence includes recognizing no motion or a marginal motion of the second conversation participant in the stream of digital pictures of the second conversation participant and recognizing the second conversation participant s silence in the stream of digital sound samples of the second conversation participant. The marginal motion of the second conversation participant may include a motion of the second conversation participant that does not exceed a threshold for motion. In further embodiments the detecting the second conversation participant s absence from the conversation includes recognizing the second conversation participant s absence in the stream of digital pictures of the second conversation participant.

In some embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the timing of the first conversation participant s first conversational activity coincides partially coincides or overlaps with the timing of the second conversation participant s first conversational activity.

In certain embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments wherein the detecting the first conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital pictures of the first conversation participant wherein the first sub stream of the stream of digital pictures of the first conversation participant comprises the first conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital sound samples of the first conversation participant wherein the first sub stream of the stream of digital sound samples of the first conversation participant comprises the first conversation participant s verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital pictures of the second conversation participant wherein the first sub stream of the stream of digital pictures of the second conversation participant comprises the second conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital sound samples of the second conversation participant wherein the first sub stream of the stream of digital sound samples of the second conversation participant comprises the second conversation participant s verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s speech or sound in the stream of digital sound samples of the first conversation participant. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s speech or sound in the stream of digital sound samples of the second conversation participant. In further embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s face or body part in the stream of digital pictures of the first conversation participant. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s face or body part in the stream of digital pictures of the second conversation participant. In further embodiments the detecting the first conversation participant s first conversational activity includes determining a beginning and an end of the first conversation participant s first conversational activity. In further embodiments the detecting the second conversation participant s first conversational activity includes determining a beginning and an end of the second conversation participant s first conversational activity.

In some embodiments the recording of the first conversation participant s first conversational activity includes the first conversation participant s visual expressions or communication in a first part of a conversation and the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity includes a first sub stream of the stream of digital pictures of the first conversation participant and a first sub stream of the stream of digital sound samples of the first conversation participant. The first sub stream of the stream of digital pictures of the first conversation participant may comprise the first conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the first conversation participant comprises the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes the second conversation participant s visual expressions or communication in a first part of a conversation and the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of the stream of digital pictures of the second conversation participant and a first sub stream of the stream of digital sound samples of the second conversation participant. The first sub stream of the stream of digital pictures of the second conversation participant may comprise the second conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the second conversation participant comprises the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity or the recording of the second conversation participant s first conversational activity includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In certain embodiments the first round of conversational exchange includes a unit of knowledge of how the first conversation participant acted relative to the second conversation participant in a first part of a conversation and how the second conversation participant acted relative to the first conversation participant in the first part of the conversation. In further embodiments the first round of conversational exchange includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the recording of the first conversation participant s first conversational activity is correlated with the recording of the second conversation participant s first conversational activity.

In some embodiments the stored plurality of rounds of conversational exchange are organized into at least one of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments each round of conversational exchange of the stored plurality of rounds of conversational exchange is included in a neuron a node a vertex or an element of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments some rounds of conversational exchange of the stored plurality of rounds of conversational exchange are interconnected.

In certain embodiments the one or more processor circuits may be further configured to compare the first round of conversational exchange with the stored plurality of rounds of conversational exchange and determine that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the first round of conversational exchange exceeds a similarity threshold.

In some embodiments the one or more processor circuits may be further configured to detect the first conversation participant s second conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s second conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may be further configured to generate a second round of conversational exchange including the first conversation participant s second conversational activity and the second conversation participant s second conversational activity. The one or more processor circuits may be further configured to cause the memory unit to store the second round of conversational exchange the second round of conversational exchange being part of the stored plurality of rounds of conversational exchange.

In some embodiments the one or more processor circuits may be further configured to create a connection between the stored first round of conversational exchange and the stored second round of conversational exchange. In further embodiments the connection between the stored first round of conversational exchange and the stored second round of conversational exchange includes or is associated with at least one of an occurrence count a weight a parameter or a data. In further embodiments the stored plurality of rounds of conversational exchange are organized into a neural network and wherein the first round of conversational exchange is stored into a first node of the neural network and the second round of conversational exchange is stored into a second node of the neural network. The first node and the second node may be connected by a connection. The first node may be part of a first layer of the neural network and the second node may be part of a second layer of the neural network. In further embodiments the stored plurality of rounds of conversational exchange are organized into a graph and wherein the first round of conversational exchange is stored into a first node of the graph and the second round of conversational exchange is stored into a second node of the graph. The first node and the second node may be connected by a connection. In further embodiments the stored plurality of rounds of conversational exchange are organized into a collection of sequences and wherein the first round of conversational exchange is stored into a first node of a sequence of the collection of sequences and the second round of conversational exchange is stored into a second node of the sequence of the collection of sequences. In further embodiments the stored plurality of rounds of conversational exchange are organized into a sequence and wherein the first round of conversational exchange is stored into a first node of the sequence and the second round of conversational exchange is stored into a second node of the sequence. In further embodiments the one or more processor circuits may be further configured to compare the second round of conversational exchange with the stored plurality of rounds of conversational exchange and determine that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the second round of conversational exchange exceeds a similarity threshold. The recording of the first conversation participant s second conversational activity may be correlated with the recording of the second conversation participant s second conversational activity.

In certain embodiments the one or more processor circuits may be further configured to update a connection between the stored first round of conversational exchange and another round of conversational exchange of the stored plurality of rounds of conversational exchange.

In some embodiments the one or more processor circuits may be further configured to detect the first conversation participant s third conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s third conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may be further configured to generate a third round of conversational exchange including a recording of the first conversation participant s third conversational activity and a recording of the second conversation participant s third conversational activity. The one or more processor circuits may be further configured to compare the third round of conversational exchange with the stored plurality of rounds of conversational exchange. The one or more processor circuits may be further configured to determine that the stored plurality of rounds of conversational exchange include a round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold. The one or more processor circuits may be further configured to update a connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold. In further embodiments the updating the connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold includes updating at least one of an occurrence count a weight a parameter or a data included in or associated with the connection. In further embodiments the recording of the first conversation participant s third conversational activity is correlated with the recording of the second conversation participant s third conversational activity.

In some embodiments the one or more processor circuits may be further configured to filter the first conversation participant s face or body part from the stream of digital pictures of the first conversation participant. In further embodiments the filtering the first conversation participant s face or body part from the stream of digital pictures of the first conversation participant includes retaining the first conversation participant s face or body part and removing an insignificant content from the stream of digital pictures of the first conversation participant.

In certain embodiments the one or more processor circuits may be further configured to filter the first conversation participant s speech or sound from the stream of digital sound samples of the first conversation participant. In further embodiments the filtering the first conversation participant s speech or sound from the stream of digital sound samples of the first conversation participant includes retaining the first conversation participant s speech or sound and removing an insignificant sound from the stream of digital sound samples of the first conversation participant.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising capturing a stream of digital pictures of a first conversation participant by a picture capturing device of a first computing device. The operations may further include capturing a stream of digital sound samples of the first conversation participant by a sound capturing device of the first computing device the first computing device coupled to a server via a network. The operations may further include capturing a stream of digital pictures of a second conversation participant by a picture capturing device of a second computing device. The operations may further include capturing a stream of digital sound samples of the second conversation participant by a sound capturing device of the second computing device the second computing device coupled to the server via the network. The operations may further include detecting the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The operations may further include detecting the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The operations may further include generating a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The operations may further include storing the first round of conversational exchange into a memory unit of the server the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In some aspects the disclosure relates to a method comprising a capturing a stream of digital pictures of a first conversation participant by a picture capturing device of a first computing device. The method may further include b capturing a stream of digital sound samples of the first conversation participant by a sound capturing device of the first computing device the first computing device coupled to a server via a network. The method may further include c capturing a stream of digital pictures of a second conversation participant by a picture capturing device of a second computing device. The method may further include d capturing a stream of digital sound samples of the second conversation participant by a sound capturing device of the second computing device the second computing device coupled to the server via the network The method may further include e detecting the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting of e performed by one or more processor circuits of the server The method may further include f detecting the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting of f performed by the one or more processor circuits of the server The method may further include g generating a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity the generating of g performed by the one or more processor circuits of the server The method may further include h storing the first round of conversational exchange into a memory unit of the server the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange the storing of h caused by the one or more processor circuits of the server.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable as well as the following embodiments.

In some embodiments the picture capturing device of the first computing device or the picture capturing device of the second computing device includes a motion picture camera. In further embodiments the stream of digital pictures of the first conversation participant includes the first conversation participant s visual expressions or communication and the stream of digital pictures of the second conversation participant includes the second conversation participant s visual expressions or communication. In further embodiments the stream of digital pictures of the first conversation participant or the stream of digital pictures of the second conversation participant includes a digital motion picture. The digital motion picture may include a MPEG motion picture an AVI motion picture a FLV motion picture a MOV motion picture a RM motion picture a SWF motion picture a WMV motion picture a DivX motion picture or a digitally encoded motion picture. In further embodiments the stream of digital pictures of the first conversation participant or the stream of digital pictures of the second conversation participant includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In certain embodiments the sound capturing device of the first computing device or the sound capturing device of the second computing device includes a microphone. In further embodiments the stream of digital sound samples of the first conversation participant includes the first conversation participant s verbal expressions or communication and the stream of digital sound samples of the second conversation participant includes the second conversation participant s verbal expressions or communication. In further embodiments the stream of digital sound samples of the first conversation participant or the stream of digital sound samples of the second conversation participant includes a digital sound. The digital sound may include a WAV digital sound a WMA digital sound an AIFF digital sound a MP3 digital sound a RA digital sound a OGG digital sound or a digitally encoded sound. In further embodiments the stream of digital sound samples of the first conversation participant or the stream of digital sound samples of the second conversation participant includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the stream of digital pictures of the first conversation participant and the stream of digital sound samples of the first conversation participant are captured simultaneously. In further embodiments the stream of digital pictures of the second conversation participant and the stream of digital sound samples of the second conversation participant are captured simultaneously. In further embodiments the stream of digital pictures of the first conversation participant the stream of digital sound samples of the first conversation participant the stream of digital pictures of the second conversation participant and the stream of digital sound samples of the second conversation participant are captured simultaneously.

In some embodiments the first conversation participant s first conversational activity or the second conversation participant s first conversational activity includes at least one of a speaking a silent facial expression a silent body movement a motionless silence an absence from the conversation or a conversational action. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the detecting the first conversation participant s speaking includes recognizing the first conversation participant s speech in the stream of digital sound samples of the first conversation participant. In further embodiments the detecting the first conversation participant s speaking includes determining a beginning and an end of the first conversation participant s speaking. The determining the beginning of the first conversation participant s speaking may include recognizing the first conversation participant s speech after a threshold period of silence in the stream of digital sound samples of the first conversation participant. The determining the end of the first conversation participant s speaking may include recognizing a threshold period of silence after the first conversation participant s speech in the stream of digital sound samples of the first conversation participant. In further embodiments the detecting the second conversation participant s silent facial expression includes recognizing the second conversation participant s facial expression in the stream of digital pictures of the second conversation participant and recognizing the second conversation participant s silence in the stream of digital sound samples of the second conversation participant. In further embodiments the detecting the second conversation participant s silent body movement includes recognizing the second conversation participant s body movement in the stream of digital pictures of the second conversation participant and recognizing the second conversation participant s silence in the stream of digital sound samples of the second conversation participant. In further embodiments the detecting the second conversation participant s motionless silence includes recognizing no motion or a marginal motion of the second conversation participant in the stream of digital pictures of the second conversation participant and recognizing the second conversation participant s silence in the stream of digital sound samples of the second conversation participant. The marginal motion of the second conversation participant may include a motion of the second conversation participant that does not exceed a threshold for motion. In further embodiments the detecting the second conversation participant s absence from the conversation includes recognizing the second conversation participant s absence in the stream of digital pictures of the second conversation participant. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the timing of the first conversation participant s first conversational activity coincides partially coincides or overlaps with the timing of the second conversation participant s first conversational activity.

In certain embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital pictures of the first conversation participant wherein the first sub stream of the stream of digital pictures of the first conversation participant comprises the first conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital sound samples of the first conversation participant wherein the first sub stream of the stream of digital sound samples of the first conversation participant comprises the first conversation participant s verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital pictures of the second conversation participant wherein the first sub stream of the stream of digital pictures of the second conversation participant comprises the second conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a first sub stream of the stream of digital sound samples of the second conversation participant wherein the first sub stream of the stream of digital sound samples of the second conversation participant comprises the second conversation participant s verbal expressions or communication in a first part of a conversation.

In some embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s speech or sound in the stream of digital sound samples of the first conversation participant. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s speech or sound in the stream of digital sound samples of the second conversation participant. In further embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s face or body part in the stream of digital pictures of the first conversation participant. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s face or body part in the stream of digital pictures of the second conversation participant. In further embodiments the detecting the first conversation participant s first conversational activity includes determining a beginning and an end of the first conversation participant s first conversational activity. In further embodiments the detecting the second conversation participant s first conversational activity includes determining a beginning and an end of the second conversation participant s first conversational activity.

In certain embodiments the recording of the first conversation participant s first conversational activity includes the first conversation participant s visual expressions or communication in a first part of a conversation and the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity includes a first sub stream of the stream of digital pictures of the first conversation participant and a first sub stream of the stream of digital sound samples of the first conversation participant. The first sub stream of the stream of digital pictures of the first conversation participant may comprise the first conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the first conversation participant may comprise the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes the second conversation participant s visual expressions or communication in a first part of a conversation and the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of the stream of digital pictures of the second conversation participant and a first sub stream of the stream of digital sound samples of the second conversation participant. The first sub stream of the stream of digital pictures of the second conversation participant may comprise the second conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the second conversation participant may comprise the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity or the recording of the second conversation participant s first conversational activity includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In some embodiments the first round of conversational exchange includes a unit of knowledge of how the first conversation participant acted relative to the second conversation participant in a first part of a conversation and how the second conversation participant acted relative to the first conversation participant in the first part of the conversation. In further embodiments the first round of conversational exchange includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the recording of the first conversation participant s first conversational activity is correlated with the recording of the second conversation participant s first conversational activity.

In certain embodiments the stored plurality of rounds of conversational exchange are organized into at least one of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments each round of conversational exchange of the stored plurality of rounds of conversational exchange is included in a neuron a node a vertex or an element of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments some rounds of conversational exchange of the stored plurality of rounds of conversational exchange are interconnected.

In certain embodiments the non transitory computer storage medium and or the method further comprise comparing the first round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits of the server and determining that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the first round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits of the server.

In some embodiments the non transitory computer storage medium and or the method further comprise detecting the first conversation participant s second conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise detecting the second conversation participant s second conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise generating a second round of conversational exchange including the first conversation participant s second conversational activity and the second conversation participant s second conversational activity the generating performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise storing the second round of conversational exchange into the memory unit the second round of conversational exchange being part of the stored plurality of rounds of conversational exchange the storing caused by the one or more processor circuits of the server.

In certain embodiments the non transitory computer storage medium and or the method further comprise creating a connection between the stored first round of conversational exchange and the stored second round of conversational exchange the creating performed by the one or more processor circuits of the server. The connection between the stored first round of conversational exchange and the stored second round of conversational exchange may include or be associated with at least one of an occurrence count a weight a parameter or a data. In further embodiments the stored plurality of rounds of conversational exchange are organized into a neural network and wherein the first round of conversational exchange is stored into a first node of the neural network and the second round of conversational exchange is stored into a second node of the neural network. The first node and the second node may be connected by a connection. The first node may be part of a first layer of the neural network and the second node may be part of a second layer of the neural network. In further embodiments the stored plurality of rounds of conversational exchange are organized into a graph and wherein the first round of conversational exchange is stored into a first node of the graph and the second round of conversational exchange is stored into a second node of the graph. The first node and the second node may be connected by a connection. In further embodiments the stored plurality of rounds of conversational exchange are organized into a collection of sequences and wherein the first round of conversational exchange is stored into a first node of a sequence of the collection of sequences and the second round of conversational exchange is stored into a second node of the sequence of the collection of sequences. In further embodiments the stored plurality of rounds of conversational exchange are organized into a sequence and wherein the first round of conversational exchange is stored into a first node of the sequence and the second round of conversational exchange is stored into a second node of the sequence. The non transitory computer storage medium and or the method further comprise comparing the second round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits of the server and determining that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the second round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits of the server. The recording of the first conversation participant s second conversational activity is correlated with the recording of the second conversation participant s second conversational activity.

In some embodiments the non transitory computer storage medium and or the method further comprise updating a connection between the stored first round of conversational exchange and another round of conversational exchange of the stored plurality of rounds of conversational exchange the updating performed by the one or more processor circuits of the server.

In certain embodiments the non transitory computer storage medium and or the method further comprise detecting the first conversation participant s third conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting performed by the one or more processor circuits of the server. The non transitory computer storage medium and or the method further comprise detecting the second conversation participant s third conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise generating a third round of conversational exchange including a recording of the first conversation participant s third conversational activity and a recording of the second conversation participant s third conversational activity the generating performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise comparing the third round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise determining that the stored plurality of rounds of conversational exchange include a round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits of the server The non transitory computer storage medium and or the method further comprise updating a connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold the updating performed by the one or more processor circuits of the server. In further embodiments the updating the connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold includes updating at least one of an occurrence count a weight a parameter or a data included in or associated with the connection. In further embodiments the recording of the first conversation participant s third conversational activity is correlated with the recording of the second conversation participant s third conversational activity.

In some embodiments the non transitory computer storage medium and or the method further comprise filtering the first conversation participant s face or body part from the stream of digital pictures of the first conversation participant the filtering performed by the one or more processor circuits of the server. In further embodiments the filtering the first conversation participant s face or body part from the stream of digital pictures of the first conversation participant includes retaining the first conversation participant s face or body part and removing an insignificant content from the stream of digital pictures of the first conversation participant.

In certain embodiments the non transitory computer storage medium and or the method further comprise filtering the first conversation participant s speech or sound from the stream of digital sound samples of the first conversation participant the filtering performed by the one or more processor circuits of the server. In further embodiments the filtering the first conversation participant s speech or sound from the stream of digital sound samples of the first conversation participant includes retaining the first conversation participant s speech or sound and removing an insignificant sound from the stream of digital sound samples of the first conversation participant.

In some aspects the disclosure relates to a system for learning artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises one or more processor circuits coupled to a memory unit The system may further include a first picture capturing device configured to capture a stream of digital pictures of a first conversation participant and a second picture capturing device configured to capture a stream of digital pictures of a second conversation participant the first and the second picture capturing devices coupled to the one or more processor circuits The system may further include a first sound capturing device configured to capture a stream of digital sound samples of the first conversation participant and a second sound capturing device configured to capture a stream of digital sound samples of the second conversation participant the first and the second sound capturing devices coupled to the one or more processor circuits. The one or more processor circuits may be configured to detect the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may be further configured to generate a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The one or more processor circuits may be further configured to cause the memory unit to store the first round of conversational exchange the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In certain embodiments the one or more processor circuits the memory unit the first picture capturing device the second picture capturing device the first sound capturing device and the second sound capturing device of the system are included in a single device. In further embodiments at least one of the one or more processor circuits or the memory unit of the system are included in a server and wherein the first picture capturing device and the first sound capturing device of the system are included in a first computing device and the second picture capturing device and the second sound capturing device of the system are included in a second computing device the first and the second computing devices coupled to the server via a network.

In some embodiments the one or more processor circuits may be further configured to compare the first round of conversational exchange with the stored plurality of rounds of conversational exchange and determine that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the first round of conversational exchange exceeds a similarity threshold.

In some embodiments the one or more processor circuits may be further configured to detect the first conversation participant s second conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s second conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant The one or more processor circuits may be further configured to generate a second round of conversational exchange including the first conversation participant s second conversational activity and the second conversation participant s second conversational activity The one or more processor circuits may be further configured to cause the memory unit to store the second round of conversational exchange the second round of conversational exchange being part of the stored plurality of rounds of conversational exchange. The one or more processor circuits may be further configured to create a connection between the stored first round of conversational exchange and the stored second round of conversational exchange. The one or more processor circuits may be further configured to compare the second round of conversational exchange with the stored plurality of rounds of conversational exchange and determine that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the second round of conversational exchange exceeds a similarity threshold.

In certain embodiments the one or more processor circuits may be further configured to detect the first conversation participant s third conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s third conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may be further configured to generate a third round of conversational exchange including a recording of the first conversation participant s third conversational activity and a recording of the second conversation participant s third conversational activity The one or more processor circuits may be further configured to compare the third round of conversational exchange with the stored plurality of rounds of conversational exchange. The one or more processor circuits may be further configured to determine that the stored plurality of rounds of conversational exchange include a round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold. The one or more processor circuits may be further configured to update a connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold.

In some embodiments the one or more processor circuits may be further configured to filter the first conversation participant s face or body part from the stream of digital pictures of the first conversation participant.

In certain embodiments the one or more processor circuits may be further configured to filter the first conversation participant s speech or sound from the stream of digital sound samples of the first conversation participant.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising capturing a stream of digital pictures of a first conversation participant by a first picture capturing device. The operations may further include capturing a stream of digital sound samples of the first conversation participant by a first sound capturing device. The operations may further include capturing a stream of digital pictures of a second conversation participant by a second picture capturing device. The operations may further include capturing a stream of digital sound samples of the second conversation participant by a second sound capturing device. The operations may further include detecting the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The operations may further include detecting the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The operations may further include generating a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The operations may further include storing the first round of conversational exchange into a memory unit the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In some aspects the disclosure relates to a method comprising a capturing a stream of digital pictures of a first conversation participant by a first picture capturing device. The method may further include b capturing a stream of digital sound samples of the first conversation participant by a first sound capturing device. The method may further include c capturing a stream of digital pictures of a second conversation participant by a second picture capturing device. The method may further include d capturing a stream of digital sound samples of the second conversation participant by a second sound capturing device. The method may further include e detecting the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting of e performed by one or more processor circuits. The method may further include f detecting the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting of f performed by the one or more processor circuits. The method may further include g generating a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity the generating of g performed by the one or more processor circuits. The method may further include h storing the first round of conversational exchange into a memory unit the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange the storing of h caused by the one or more processor circuits.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable as well as the following embodiments.

In some embodiments the one or more processor circuits the memory unit the first picture capturing device the second picture capturing device the first sound capturing device and the second sound capturing device of the system are included in a single device. In further embodiments at least one of the one or more processor circuits or the memory unit of the system are included in a server and wherein the first picture capturing device and the first sound capturing device of the system are included in a first computing device and the second picture capturing device and the second sound capturing device of the system are included in a second computing device the first and the second computing devices coupled to the server via a network.

In certain embodiments the non transitory computer storage medium and or the method further comprise comparing the first round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits and determining that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the first round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits.

In some embodiments the non transitory computer storage medium and or the method further comprise detecting the first conversation participant s second conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise detecting the second conversation participant s second conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise generating a second round of conversational exchange including the first conversation participant s second conversational activity and the second conversation participant s second conversational activity the generating performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise storing the second round of conversational exchange into a memory unit the second round of conversational exchange being part of the stored plurality of rounds of conversational exchange the storing caused by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise creating a connection between the stored first round of conversational exchange and the stored second round of conversational exchange the creating performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise comparing the second round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits and determining that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the second round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits.

In certain embodiments the non transitory computer storage medium and or the method further comprise detecting the first conversation participant s third conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise detecting the second conversation participant s third conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise generating a third round of conversational exchange including a recording of the first conversation participant s third conversational activity and a recording of the second conversation participant s third conversational activity the generating performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise comparing the third round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise determining that the stored plurality of rounds of conversational exchange include a round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise updating a connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold the updating performed by the one or more processor circuits.

In some embodiments the non transitory computer storage medium and or the method further comprise filtering the first conversation participant s face or body part from the stream of digital pictures of the first conversation participant the filtering performed by the one or more processor circuits.

In certain embodiments the non transitory computer storage medium and or the method further comprise filtering the first conversation participant s speech or sound from the stream of digital sound samples of the first conversation participant the filtering performed by the one or more processor circuits.

In some aspects the disclosure relates to a system for learning artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises one or more processor circuits coupled to a memory unit. The system may further include a picture capturing device configured to capture a stream of digital pictures of a first and a second conversation participants the picture capturing device coupled to the one or more processor circuits. The system may further include a sound capturing device configured to capture a stream of digital sound samples of the first and the second conversation participants the sound capturing device coupled to the one or more processor circuits. The one or more processor circuits may be configured to detect the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to detect the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to generate a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The one or more processor circuits may be further configured to cause the memory unit to store the first round of conversational exchange the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In certain embodiments the one or more processor circuits the memory unit the picture capturing device and the sound capturing device of the system are included in a single device. In further embodiments at least one of the one or more processor circuits or the memory unit of the system are included in a server and wherein the picture capturing device and the sound capturing device of the system are included in a computing device the computing device coupled to the server via a network.

In some embodiments the stream of digital pictures of the first and the second conversation participants includes the first and the second conversation participants visual expressions or communication. In further embodiments the stream of digital sound samples of the first and the second conversation participants includes the first and the second conversation participants verbal expressions or communication. In further embodiments the stream of digital pictures of the first and the second conversation participants and the stream of digital sound samples of the first and the second conversation participants are captured simultaneously.

In certain embodiments the first conversation participant s first conversational activity or the second conversation participant s first conversational activity includes at least one of a speaking a silent facial expression a silent body movement a motionless silence an absence from the conversation or a conversational action. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the detecting the first conversation participant s speaking includes recognizing the first conversation participant s speech in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the first conversation participant s speaking includes determining a beginning and an end of the first conversation participant s speaking. The determining the beginning of the first conversation participant s speaking may include recognizing the first conversation participant s speech after a threshold period of silence in the stream of digital sound samples of the first and the second conversation participants. The determining the end of the first conversation participant s speaking may include recognizing a threshold period of silence after the first conversation participant s speech in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the second conversation participant s silent facial expression includes recognizing the second conversation participant s facial expression in the stream of digital pictures of the first and the second conversation participants and recognizing the second conversation participant s silence in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the second conversation participant s silent body movement includes recognizing the second conversation participant s body movement in the stream of digital pictures of the first and the second conversation participants and recognizing the second conversation participant s silence in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the second conversation participant s motionless silence includes recognizing no motion or a marginal motion of the second conversation participant in the stream of digital pictures of the first and the second conversation participants and recognizing the second conversation participant s silence in the stream of digital sound samples of the first and the second conversation participants. The marginal motion of the second conversation participant may include a motion of the second conversation participant that does not exceed a threshold for motion. In further embodiments the detecting the second conversation participant s absence from the conversation includes recognizing the second conversation participant s absence in the stream of digital pictures of the first and the second conversation participants. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation.

In some embodiments the timing of the first conversation participant s first conversational activity coincides partially coincides or overlaps with the timing of the second conversation participant s first conversational activity. In further embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s visual and verbal expressions or communication in a first part of a conversation.

In certain embodiments the detecting the first conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital pictures of the first and the second conversation participants wherein the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the first conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants wherein the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the first conversation participant s verbal expressions or communication in a first part of a conversation.

In some embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital pictures of the first and the second conversation participants wherein the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the second conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants wherein the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the second conversation participant s verbal expressions or communication in a first part of a conversation.

In certain embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s speech or sound in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the recognizing the first conversation participant s speech or sound includes utilizing at least one of a speaker dependent speech recognition or a speech or sound segmentation. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s speech or sound in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the recognizing the second conversation participant s speech or sound includes utilizing at least one of a speaker dependent speech recognition or a speech or sound segmentation. In further embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s face or body part in the stream of digital pictures of the first and the second conversation participants. In further embodiments the recognizing the first conversation participant s face or body part includes utilizing a picture segmentation. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s face or body part in the stream of digital pictures of the first and the second conversation participants. In further embodiments the recognizing the second conversation participant s face or body part includes utilizing a picture segmentation. In further embodiments the detecting the first conversation participant s first conversational activity includes determining a beginning and an end of the first conversation participant s first conversational activity. In further embodiments the detecting the second conversation participant s first conversational activity includes determining a beginning and an end of the second conversation participant s first conversational activity.

In some embodiments the recording of the first conversation participant s first conversational activity includes the first conversation participant s visual expressions or communication in a first part of a conversation and the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity includes a parallel sub stream of the stream of digital pictures of the first and the second conversation participants and a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants. In further embodiments the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the first conversation participant s visual expressions or communication in a first part of a conversation and the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the first conversation participant s verbal expressions or communication in the first part of the conversation.

In certain embodiments the recording of the second conversation participant s first conversational activity includes the second conversation participant s visual expressions or communication in a first part of a conversation and the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes a parallel sub stream of the stream of digital pictures of the first and the second conversation participants and a paralel sub stream of the stream of digital sound samples of the first and the second conversation participants. In further embodiments the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the second conversation participant s visual expressions or communication in a first part of a conversation and the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the second conversation participant s verbal expressions or communication in the first part of the conversation.

In some embodiments the one or more processor circuits may be further configured to compare the first round of conversational exchange with the stored plurality of rounds of conversational exchange and determine that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the first round of conversational exchange exceeds a similarity threshold.

In certain embodiments the one or more processor circuits may be further configured to detect the first conversation participant s second conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to detect the second conversation participant s second conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to generate a second round of conversational exchange including the first conversation participant s second conversational activity and the second conversation participant s second conversational activity. The one or more processor circuits may be further configured to cause the memory unit to store the second round of conversational exchange the second round of conversational exchange being part of the stored plurality of rounds of conversational exchange. The one or more processor circuits may be further configured to create a connection between the stored first round of conversational exchange and the stored second round of conversational exchange. The one or more processor circuits may be further configured to compare the second round of conversational exchange with the stored plurality of rounds of conversational exchange and determine that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the second round of conversational exchange exceeds a similarity threshold.

In some embodiments the one or more processor circuits may be further configured to detect the first conversation participant s third conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to detect the second conversation participant s third conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to generate a third round of conversational exchange including the first conversation participant s third conversational activity and the second conversation participant s third conversational activity. The one or more processor circuits may be further configured to compare the third round of conversational exchange with the stored plurality of rounds of conversational exchange. The one or more processor circuits may be further configured to determine that the stored plurality of rounds of conversational exchange include a round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold. The one or more processor circuits may be further configured to update a connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold.

In certain embodiments the one or more processor circuits may be further configured to filter the first and the second conversation participants faces or body parts from the stream of digital pictures of the first and the second conversation participants. The one or more processor circuits may be further configured to the filtering the first and the second conversation participants faces or body parts from the stream of digital pictures of the first and the second conversation participants includes retaining the first and the second conversation participants faces or body parts and removing an insignificant content from the stream of digital pictures of the first and the second conversation participants.

In some embodiments the one or more processor circuits may be further configured to filter the first and the second conversation participants speeches or sounds from the stream of digital sound samples of the first and the second conversation participants. The one or more processor circuits may be further configured to the filtering the first and the second conversation participants speeches or sounds from the stream of digital sound samples of the first and the second conversation participants includes retaining the first and the second conversation participants speeches or sounds and removing an insignificant sound from the stream of digital sound samples of the first and the second conversation participants.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising capturing a stream of digital pictures of a first and a second conversation participants by a picture capturing device. The operations may further include capturing a stream of digital sound samples of the first and the second conversation participants by a sound capturing device. The operations may further include detecting the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The operations may further include detecting the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants. The operations may further include generating a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The operations may further include storing the first round of conversational exchange into a memory unit the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In some aspects the disclosure relates to a method comprising a capturing a stream of digital pictures of a first and a second conversation participants by a picture capturing device. The method may further include b capturing a stream of digital sound samples of the first and the second conversation participants by a sound capturing device. The method may further include c detecting the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants the detecting of c performed by one or more processor circuits. The method may further include d detecting the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants the detecting of d performed by the one or more processor circuits. The method may further include e generating a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity the generating of e performed by the one or more processor circuits. The method may further include f storing the first round of conversational exchange into a memory unit the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange the storing of f caused by the one or more processor circuits.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable as well as the following embodiments.

In certain embodiments the one or more processor circuits the memory unit the picture capturing device and the sound capturing device of the system are included in a single device. In further embodiments at least one of the one or more processor circuits or the memory unit of the system are included in a server and wherein the picture capturing device and the sound capturing device of the system are included in a computing device the computing device coupled to the server via a network.

In some embodiments the stream of digital pictures of the first and the second conversation participants includes the first and the second conversation participants visual expressions or communication. In further embodiments the stream of digital sound samples of the first and the second conversation participants includes the first and the second conversation participants verbal expressions or communication. In further embodiments the stream of digital pictures of the first and the second conversation participants and the stream of digital sound samples of the first and the second conversation participants are captured simultaneously.

In certain embodiments the first conversation participant s first conversational activity or the second conversation participant s first conversational activity includes at least one of a speaking a silent facial expression a silent body movement a motionless silence an absence from the conversation or a conversational action. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the detecting the first conversation participant s speaking includes recognizing the first conversation participant s speech in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the first conversation participant s speaking includes determining a beginning and an end of the first conversation participant s speaking. The determining the beginning of the first conversation participant s speaking may include recognizing the first conversation participant s speech after a threshold period of silence in the stream of digital sound samples of the first and the second conversation participants. The determining the end of the first conversation participant s speaking may include recognizing a threshold period of silence after the first conversation participant s speech in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the second conversation participant s silent facial expression includes recognizing the second conversation participant s facial expression in the stream of digital pictures of the first and the second conversation participants and recognizing the second conversation participant s silence in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the second conversation participant s silent body movement includes recognizing the second conversation participant s body movement in the stream of digital pictures of the first and the second conversation participants and recognizing the second conversation participant s silence in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the detecting the second conversation participant s motionless silence includes recognizing no motion or a marginal motion of the second conversation participant in the stream of digital pictures of the first and the second conversation participants and recognizing the second conversation participant s silence in the stream of digital sound samples of the first and the second conversation participants. The marginal motion of the second conversation participant may include a motion of the second conversation participant that does not exceed a threshold for motion. In further embodiments the detecting the second conversation participant s absence from the conversation includes recognizing the second conversation participant s absence in the stream of digital pictures of the first and the second conversation participants. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s speaking and the second conversation participant s first conversational activity includes a second conversation participant s speaking. In further embodiments the first conversation participant s first conversational activity includes a first conversation participant s silent facial expression a first conversation participant s silent body movement a first conversation participant s motionless silence or a first conversation participant s absence from the conversation and the second conversation participant s first conversational activity includes a second conversation participant s silent facial expression a second conversation participant s silent body movement a second conversation participant s motionless silence or a second conversation participant s absence from the conversation. In further embodiments the timing of the first conversation participant s first conversational activity coincides partially coincides or overlaps with the timing of the second conversation participant s first conversational activity.

In certain embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital pictures of the first and the second conversation participants wherein the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the first conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the first conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants wherein the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the first conversation participant s verbal expressions or communication in a first part of a conversation.

In some embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s visual and verbal expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital pictures of the first and the second conversation participants wherein the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the second conversation participant s visual expressions or communication in a first part of a conversation. In further embodiments the detecting the second conversation participant s first conversational activity includes identifying a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants wherein the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the second conversation participant s verbal expressions or communication in a first part of a conversation.

In certain embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s speech or sound in the stream of digital sound samples of the first and the second conversation participants. In further embodiments wherein the recognizing the first conversation participant s speech or sound includes utilizing at least one of a speaker dependent speech recognition or a speech or sound segmentation. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s speech or sound in the stream of digital sound samples of the first and the second conversation participants. In further embodiments the recognizing the second conversation participant s speech or sound includes utilizing at least one of a speaker dependent speech recognition or a speech or sound segmentation. In further embodiments the detecting the first conversation participant s first conversational activity includes recognizing the first conversation participant s face or body part in the stream of digital pictures of the first and the second conversation participants. In further embodiments the recognizing the first conversation participant s face or body part includes utilizing a picture segmentation. In further embodiments the detecting the second conversation participant s first conversational activity includes recognizing the second conversation participant s face or body part in the stream of digital pictures of the first and the second conversation participants. In further embodiments the recognizing the second conversation participant s face or body part includes utilizing a picture segmentation. In further embodiments the detecting the first conversation participant s first conversational activity includes determining a beginning and an end of the first conversation participant s first conversational activity. In further embodiments the detecting the second conversation participant s first conversational activity includes determining a beginning and an end of the second conversation participant s first conversational activity.

In some embodiments the recording of the first conversation participant s first conversational activity includes the first conversation participant s visual expressions or communication in a first part of a conversation and the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity includes a parallel sub stream of the stream of digital pictures of the first and the second conversation participants and a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants. In further embodiments the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the first conversation participant s visual expressions or communication in a first part of a conversation and the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the first conversation participant s verbal expressions or communication in the first part of the conversation.

In certain embodiments the recording of the second conversation participant s first conversational activity includes the second conversation participant s visual expressions or communication in a first part of a conversation and the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes a parallel sub stream of the stream of digital pictures of the first and the second conversation participants and a parallel sub stream of the stream of digital sound samples of the first and the second conversation participants. In further embodiments the parallel sub stream of the stream of digital pictures of the first and the second conversation participants comprises the second conversation participant s visual expressions or communication in a first part of a conversation and the parallel sub stream of the stream of digital sound samples of the first and the second conversation participants comprises the second conversation participant s verbal expressions or communication in the first part of the conversation.

In some embodiments the non transitory computer storage medium and or the method further comprise comparing the first round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits and determining that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the first round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits.

In certain embodiments the non transitory computer storage medium and or the method further comprise detecting the first conversation participant s second conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise detecting the second conversation participant s second conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise generating a second round of conversational exchange including the first conversation participant s second conversational activity and the second conversation participant s second conversational activity the generating performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise storing the second round of conversational exchange into a memory unit the second round of conversational exchange being part of the stored plurality of rounds of conversational exchange the storing caused by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise creating a connection between the stored first round of conversational exchange and the stored second round of conversational exchange the creating performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise comparing the second round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits and determining that the stored plurality of rounds of conversational exchange do not include a round of conversational exchange whose similarity with the second round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits.

In certain embodiments the non transitory computer storage medium and or the method further comprise detecting the first conversation participant s third conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise detecting the second conversation participant s third conversational activity from at least one of the stream of digital pictures of the first and the second conversation participants or the stream of digital sound samples of the first and the second conversation participants the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise generating a third round of conversational exchange including the first conversation participant s third conversational activity and the second conversation participant s third conversational activity the generating performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise comparing the third round of conversational exchange with the stored plurality of rounds of conversational exchange the comparing performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise determining that the stored plurality of rounds of conversational exchange include a round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold the determining performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise updating a connection between the stored first round of conversational exchange and the round of conversational exchange whose similarity with the third round of conversational exchange exceeds a similarity threshold the updating performed by the one or more processor circuits.

In some embodiments the non transitory computer storage medium and or the method further comprise filtering the first and the second conversation participants faces or body parts from the stream of digital pictures of the first and the second conversation participants the filtering performed by the one or more processor circuits. In further embodiments the filtering the first and the second conversation participants faces or body parts from the stream of digital pictures of the first and the second conversation participants includes retaining the first and the second conversation participants faces or body parts and removing an insignificant content from the stream of digital pictures of the first and the second conversation participants.

In certain embodiments the non transitory computer storage medium and or the method further comprise filtering the first and the second conversation participants speeches or sounds from the stream of digital sound samples of the first and the second conversation participants the filtering performed by the one or more processor circuits. In further embodiments the filtering the first and the second conversation participants speeches or sounds from the stream of digital sound samples of the first and the second conversation participants includes retaining the first and the second conversation participants speeches or sounds and removing an insignificant sound from the stream of digital sound samples of the first and the second conversation participants.

In some aspects the disclosure relates to a system for learning artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises one or more processor circuits coupled to a memory unit. The system may further include a first picture capturing device configured to capture a stream of digital pictures of a first conversation participant and a second picture capturing device configured to capture a stream of digital pictures of a second conversation participant the first and the second picture capturing devices coupled to the one or more processor circuits. The system may further include a first sound capturing device configured to capture a stream of digital sound samples of the first conversation participant and a second sound capturing device configured to capture a stream of digital sound samples of the second conversation participant the first and the second sound capturing devices coupled to the one or more processor circuits. The one or more processor circuits may be configured to detect the first conversation participant s first and second conversational activities from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The one or more processor circuits may be further configured to detect the second conversation participant s first and second conversational activities from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may be further configured to generate a first round of conversational exchange including recordings of the first conversation participant s first and second conversational activities and recordings of the second conversation participant s first and second conversational activities. The one or more processor circuits may be further configured to cause the memory unit to store the first round of conversational exchange the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising capturing a stream of digital pictures of a first conversation participant by a first picture capturing device. The operations may further include capturing a stream of digital sound samples of the first conversation participant by a first sound capturing device. The operations may further include capturing a stream of digital pictures of a second conversation participant by a second picture capturing device. The operations may further include capturing a stream of digital sound samples of the second conversation participant by a second sound capturing device. The operations may further include detecting the first conversation participant s first and second conversational activities from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. The operations may further include detecting the second conversation participant s first and second conversational activities from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The operations may further include generating a first round of conversational exchange including recordings of the first conversation participant s first and second conversational activities and recordings of the second conversation participant s first and second conversational activities. The operations may further include storing the first round of conversational exchange into a memory unit the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange.

In some aspects the disclosure relates to a method comprising a capturing a stream of digital pictures of a first conversation participant by a first picture capturing device. The method may further include b capturing a stream of digital sound samples of the first conversation participant by a first sound capturing device. The method may further include c capturing a stream of digital pictures of a second conversation participant by a second picture capturing device. The method may further include d capturing a stream of digital sound samples of the second conversation participant by a second sound capturing device. The method may further include e detecting the first conversation participant s first and second conversational activities from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant the detecting of e performed by one or more processor circuits. The method may further include f detecting the second conversation participant s first and second conversational activities from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant the detecting of f performed by the one or more processor circuits. The method may further include g generating a first round of conversational exchange including recordings of the first conversation participant s first and second conversational activities and recordings of the second conversation participant s first and second conversational activity the generating of g performed by the one or more processor circuits. The method may further include h storing the first round of conversational exchange into a memory unit the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange the storing of h caused by the one or more processor circuits.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable.

In some aspects the disclosure relates to a system for using artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises a server including one or more processor circuits. The system may further include a memory unit coupled to the one or more processor circuits that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity. The system may further include a user s computing device including a picture capturing device configured to capture a stream of digital pictures of the user and include a sound capturing device configured to capture a stream of digital sound samples of the user the user s computing device coupled to the server via a network. The one or more processor circuits may be configured to detect the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The one or more processor circuits may be further configured to compare at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The one or more processor circuits may be further configured to determine that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The one or more processor circuits may be further configured to cause a display and a sound producing device of the user s computing device to play at least one portion of the recording of the second conversation participant s first conversational activity.

In certain embodiments the recording of the first conversation participant s first conversational activity includes the first conversation participant s visual expressions or communication in a first part of a conversation and the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant and a first sub stream of a stream of digital sound samples of the first conversation participant. In further embodiments the first sub stream of the stream of digital pictures of the first conversation participant comprises the first conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the first conversation participant comprises the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes the second conversation participant s visual expressions or communication in a first part of a conversation and the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the second conversation participant and a first sub stream of a stream of digital sound samples of the second conversation participant. In further embodiments the first sub stream of the stream of digital pictures of the second conversation participant comprises the second conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the second conversation participant comprises the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity or the recording of the second conversation participant s first conversational activity includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In some embodiments the first round of conversational exchange includes a unit of knowledge of how the first conversation participant acted relative to the second conversation participant in a first part of a conversation and how the second conversation participant acted relative to the first conversation participant in the first part of the conversation. In further embodiments the first round of conversational exchange includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the recording of the first conversation participant s first conversational activity is correlated with the recording of the second conversation participant s first conversational activity.

In certain embodiments the stored plurality of rounds of conversational exchange are organized into at least one of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments each round of conversational exchange of the stored plurality of rounds of conversational exchange is included in a neuron a node a vertex or an element of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments some rounds of conversational exchange of the stored plurality of rounds of conversational exchange are interconnected.

In some embodiments the picture capturing device includes a motion picture camera. In further embodiments the stream of digital pictures of the user includes the user s visual expressions or communication. In further embodiments the stream of digital pictures of the user includes a digital motion picture. The digital motion picture may include a MPEG motion picture an AVI motion picture a FLV motion picture a MOV motion picture a RM motion picture a SWF motion picture a WMV motion picture a DivX motion picture or a digitally encoded motion picture. In further embodiments the stream of digital pictures of the user includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In certain embodiments the sound capturing device includes a microphone. In further embodiments the stream of digital sound samples of the user includes the user s verbal expressions or communication. In further embodiments the stream of digital sound samples of the user includes a digital sound. The digital sound may include a WAV digital sound a WMA digital sound an AIFF digital sound a MP3 digital sound a RA digital sound a OGG digital sound or a digitally encoded sound. In further embodiments the stream of digital sound samples of the user includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the stream of digital pictures of the user and the stream of digital sound samples of the user are captured simultaneously.

In some embodiments the user s first conversational activity includes at least one of a user s speaking a user s silent facial expression a user s silent body movement a user s motionless silence a user s absence from the conversation or a user s conversational action. In further embodiments the detecting the user s speaking includes recognizing the user s speech in the stream of digital sound samples of the user. In further embodiments the detecting the user s speaking includes determining a beginning and an end of the user s speaking. The determining the beginning of the user s speaking may include recognizing the user s speech after a threshold period of silence in the stream of digital sound samples of the user. The determining the end of the user s speaking may include recognizing a threshold period of silence after the user s speech in the stream of digital sound samples of the user. In further embodiments the detecting the user s silent facial expression includes recognizing the user s facial expression in the stream of digital pictures of the user and recognizing the user s silence in the stream of digital sound samples of the user. In further embodiments the detecting the user s silent body movement includes recognizing the user s body movement in the stream of digital pictures of the user and recognizing the user s silence in the stream of digital sound samples of the user. In further embodiments the detecting the user s motionless silence includes recognizing no motion or a marginal motion of the user in the stream of digital pictures of the user and recognizing the user s silence in the stream of digital sound samples of the user. The marginal motion of the user may include a motion of the user that does not exceed a threshold for motion. In further embodiments the detecting the user s absence from the conversation includes recognizing the user s absence in the stream of digital pictures of the user. In further embodiments the detecting the user s first conversational activity includes recognizing the user s visual and verbal expressions or communication in a first part of a simulated conversation. In further embodiments the detecting the user s first conversational activity includes identifying a first sub stream of the stream of digital pictures of the user wherein the first sub stream of the stream of digital pictures of the user comprises the user s visual expressions or communication in a first part of a simulated conversation. In further embodiments the detecting the user s first conversational activity includes identifying a first sub stream of the stream of digital sound samples of the user wherein the first sub stream of the stream of digital sound samples of the user comprises the user s verbal expressions or communication in a first part of a simulated conversation.

In certain embodiments the detecting the user s first conversational activity includes recognizing the user s speech or sound in the stream of digital sound samples of the user. In further embodiments the detecting the user s first conversational activity includes recognizing the user s face or body part in the stream of digital pictures of the user. In further embodiments the detecting the user s first conversational activity includes determining a beginning and an end of the user s first conversational activity.

In some embodiments the recording of the user s first conversational activity includes the user s visual expressions or communication in a first part of a simulated conversation and the user s verbal expressions or communication in the first part of the simulated conversation.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital sound samples of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one portion of the first sub stream of the stream of digital sound samples of the user with at least one portion of the first sub stream of the stream of digital sound samples of the first conversation participant. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the user includes one or more words one or more features or one or more sound samples of the first sub stream of the stream of digital sound samples of the user. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the first conversation participant includes one or more words one or more features or one or more sound samples of the first sub stream of a stream of digital sound samples of the first conversation participant.

In some embodiments In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one word recognized from the recording of the user s first conversational activity with at least one word recognized from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one sound feature from the recording of the user s first conversational activity with at least one sound feature from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one sound sample from the recording of the user s first conversational activity with at least one sound sample from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of amplitude adjustment sample rate adjustment noise reduction or temporal alignment of one or more sound samples in the recording of the user s first conversational activity or the recording of the first conversation participant s first conversational activity.

In some embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital sound samples of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of amplitude adjustment sample rate adjustment noise reduction or temporal alignment in the first sub stream of the stream of digital sound samples of the user or the first sub stream of the stream of digital sound samples of the first conversation participant. In further embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital pictures of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one portion of the first sub stream of the stream of digital pictures of the user with at least one portion of the first sub stream of the stream of digital pictures of the first conversation participant. In further embodiments the portion of the first sub stream of the stream of digital pictures of the user includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of the stream of digital pictures of the user. In further embodiments the portion of the first sub stream of the stream of digital pictures of the first conversation participant includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of the stream of digital pictures of the first conversation participant. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one picture from the recording of the user s first conversational activity with at least one picture from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one region of a picture from the recording of the user s first conversational activity with at least one region of a picture from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one picture feature from the recording of the user s first conversational activity with at least one picture feature from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one pixel from the recording of the user s first conversational activity with at least one pixel from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of color adjustment size adjustment transparency utilization mask utilization or temporal alignment of one or more pictures in the recording of the user s first conversational activity or the recording of the first conversation participant s first conversational activity.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital pictures of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of color adjustment size adjustment transparency utilization mask utilization or temporal alignment in the first sub stream of the stream of digital pictures of the user or the first sub stream of the stream of digital pictures of the first conversation participant.

In certain embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing an extra information included in the recording of the user s first conversational activity with an extra information included in the recording of the first conversation participant s first conversational activity. In further embodiments the extra information includes a time information a location information an observed information or a contextual information.

In some embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a similarity between an extra information included in the recording of the user s first conversational activity and an extra information included in the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. In further embodiments the extra information includes a time information a location information an observed information or a contextual information.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital sound samples of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the first conversation participant wherein the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a similarity between at least one portion of the first sub stream of the stream of digital sound samples of the user and at least one portion of the first sub stream of the stream of digital sound samples of the first conversation participant exceeds a similarity threshold. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the user includes one or more words one or more features or one or more sound samples of the first sub stream of the stream of digital sound samples of the user. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the first conversation participant includes one or more words one or more features or one or more sound samples of the first sub stream of the stream of digital sound samples of the first conversation participant.

In some embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching words recognized from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching words recognized from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a word a type of a word an importance of a word a semantic variation of a word a concept of a word or a threshold for a difference in a word. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching sound features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching sound features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a sound feature a type of a sound feature an importance of a sound feature or a threshold for a difference in a sound feature. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching sound samples from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching sound samples from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a sound sample an importance of a sound sample or a threshold for a difference in a sound sample. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes recognizing a speech or a sound of a same person in the at least one portion of the recording of the user s first conversational activity and the at least one portion of the recording of the first conversation participant s first conversational activity.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital pictures of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant and

wherein the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a similarity between at least one portion of the first sub stream of the stream of digital pictures of the user and at least one portion of the first sub stream of the stream of digital pictures of the first conversation participant exceeds a similarity threshold. In further embodiments the portion of the first sub stream of the stream of digital pictures of the user includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of the stream of digital pictures of the user. In further embodiments the portion of the first sub stream of the stream of digital pictures of the first conversation participant includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of a stream of digital pictures of the first conversation participant.

In some embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching pictures from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching pictures from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a picture or a threshold for a difference in a picture. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching regions of pictures from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching regions of a picture from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of a location of a region or a threshold for a difference in a region. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching picture features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching picture features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of a type of a picture feature an importance of a picture feature a location of a picture feature in a region of interest or a threshold for a difference in a picture feature. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching pixels from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching pixels from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of a location of a pixel in a region of interest or a threshold for a difference in a pixel.

In some embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes recognizing a same person or object in the at least one portion of the recording of the user s first conversational activity and the at least one portion of the recording of the first conversation participant s first conversational activity.

In certain embodiments the playing the at least one portion of the recording of the second conversation participant s first conversational activity is performed concurrently with at least one portion of the user s first conversational activity. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the second conversation participant wherein the playing the at least one portion of the recording of the second conversation participant s first conversational activity includes playing the at least one portion of the first sub stream of the stream of digital pictures of the second conversation participant. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the second conversation participant wherein the playing the at least one portion of the recording of the second conversation participant s first conversational activity includes playing the at least one portion of the first sub stream of the stream of digital sound samples of the second conversation participant.

In some embodiments the stored plurality of rounds of conversational exchange include a second round of conversational exchange the second round of conversational exchange comprising a recording of a first conversation participant s second conversational activity and a recording of a second conversation participant s second conversational activity. The one or more processor circuits may be further configured to detect the user s second conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The one or more processor circuits may be further configured to compare at least one portion of a recording of the user s second conversational activity with at least one portion of the recording of the first conversation participant s second conversational activity. The one or more processor circuits may be further configured to determine that a similarity between at least one portion of the recording of the user s second conversational activity and at least one portion of the recording of the first conversation participant s second conversational activity exceeds a similarity threshold. The one or more processor circuits may be further configured to cause the display and the sound producing device of the user s computing device to play at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the first round of conversational exchange is connected to the second round of conversational exchange by a connection. The connection between the first round of conversational exchange and the second round of conversational exchange may include or be associated with at least one of an occurrence count a weight a parameter or a data. In further embodiments the stored plurality of rounds of conversational exchange are organized into a neural network and wherein the first round of conversational exchange is stored into a first node of the neural network and the second round of conversational exchange is stored into a second node of the neural network. The first node and the second node may be connected by a connection. The first node may be part of a first layer of the neural network and the second node may be part of a second layer of the neural network. In further embodiments the stored plurality of rounds of conversational exchange are organized into a graph and wherein the first round of conversational exchange is stored into a first node of the graph and the second round of conversational exchange is stored into a second node of the graph. The first node and the second node may be connected by a connection. In further embodiments the stored plurality of rounds of conversational exchange are organized into a collection of sequences and wherein the first round of conversational exchange is stored into a first node of a sequence of the collection of sequences and the second round of conversational exchange is stored into a second node of the sequence of the collection of sequences. In further embodiments the stored plurality of rounds of conversational exchange are organized into a sequence and wherein the first round of conversational exchange is stored into a first node of the sequence and the second round of conversational exchange is stored into a second node of the sequence.

In some embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed concurrently with the at least one portion of the user s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed subsequent to the at least one portion of the user s first conversational activity.

In certain embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes at least one of moving centering aligning resizing or transforming one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes adjusting a lighting or a color of one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes a cut or a dissolve between one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes morphing of one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity.

In some embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity includes interpolation inbetweening extrapolation or picture generation between one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity includes playing or replaying one or more pictures of the recording of the second conversation participant s first conversational activity.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising accessing a memory unit of a server that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity. The operations may further include capturing a stream of digital pictures of a user by a picture capturing device of a user s computing device. The operations may further include capturing a stream of digital sound samples of the user by a sound capturing device of the user s computing device the user s computing device coupled to the server via a network. The operations may further include detecting the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The operations may further include comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The operations may further include determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The operations may further include playing at least one portion of the recording of the second conversation participant s first conversational activity by a display and a sound producing device of the user s computing device.

In some aspects the disclosure relates to a method comprising a accessing a memory unit of a server that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity the accessing of a performed by one or more processor circuits of the server. The method may further include b capturing a stream of digital pictures of a user by a picture capturing device of a user s computing device. The method may further include c capturing a stream of digital sound samples of the user by a sound capturing device of the user s computing device the user s computing device coupled to the server via a network. The method may further include d detecting the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user the detecting of d performed by the one or more processor circuits of the server. The method may further include e comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity the comparing of e performed by the one or more processor circuits of the server. The method may further include f determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold the determining of f performed by the one or more processor circuits of the server. The method may further include g playing at least one portion of the recording of the second conversation participant s first conversational activity by a display and a sound producing device of the user s computing device the playing of g caused by the one or more processor circuits of the server.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable as well as the folowing embodiments.

In certain embodiments the recording of the first conversation participant s first conversational activity includes the first conversation participant s visual expressions or communication in a first part of a conversation and the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant and a first sub stream of a stream of digital sound samples of the first conversation participant. In further embodiments the first sub stream of the stream of digital pictures of the first conversation participant comprises the first conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the first conversation participant comprises the first conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes the second conversation participant s visual expressions or communication in a first part of a conversation and the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the second conversation participant and a first sub stream of a stream of digital sound samples of the second conversation participant. In further embodiments the first sub stream of the stream of digital pictures of the second conversation participant comprises the second conversation participant s visual expressions or communication in a first part of a conversation and the first sub stream of the stream of digital sound samples of the second conversation participant comprises the second conversation participant s verbal expressions or communication in the first part of the conversation. In further embodiments the recording of the first conversation participant s first conversational activity or the recording of the second conversation participant s first conversational activity includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In certain embodiments the first round of conversational exchange includes a unit of knowledge of how the first conversation participant acted relative to the second conversation participant in a first part of a conversation and how the second conversation participant acted relative to the first conversation participant in the first part of the conversation. In further embodiments the first round of conversational exchange includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the recording of the first conversation participant s first conversational activity is correlated with the recording of the second conversation participant s first conversational activity.

In some embodiments the stored plurality of rounds of conversational exchange are organized into at least one of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments each round of conversational exchange of the stored plurality of rounds of conversational exchange is included in a neuron a node a vertex or an element of a neural network a graph a collection of sequences a sequence a knowledgebase a knowledge structure or a data structure. In further embodiments some rounds of conversational exchange of the stored plurality of rounds of conversational exchange are interconnected.

In certain embodiments the picture capturing device includes a motion picture camera. In further embodiments the stream of digital pictures of the user includes the user s visual expressions or communication. In further embodiments the stream of digital pictures of the user includes a digital motion picture. The digital motion picture may include a MPEG motion picture an AVI motion picture a FLV motion picture a MOV motion picture a RM motion picture a SWF motion picture a WMV motion picture a DivX motion picture or a digitally encoded motion picture. In further embodiments the stream of digital pictures of the user includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information.

In some embodiments the sound capturing device includes a microphone. In further embodiments the stream of digital sound samples of the user includes the user s verbal expressions or communication. In further embodiments the stream of digital sound samples of the user includes a digital sound. The digital sound may include a WAV digital sound a WMA digital sound an AIFF digital sound a MP3 digital sound a RA digital sound a OGG digital sound or a digitally encoded sound. In further embodiments the stream of digital sound samples of the user includes or is associated with an extra information the extra information comprising a time information a location information an observed information or a contextual information. In further embodiments the stream of digital pictures of the user and the stream of digital sound samples of the user are captured simultaneously.

In certain embodiments the user s first conversational activity includes at least one of a user s speaking a user s silent facial expression a user s silent body movement a user s motionless silence a user s absence from the conversation or a user s conversational action. In further embodiments the detecting the user s speaking includes recognizing the user s speech in the stream of digital sound samples of the user. In further embodiments the detecting the user s speaking includes determining a beginning and an end of the user s speaking. The determining the beginning of the user s speaking may include recognizing the user s speech after a threshold period of silence in the stream of digital sound samples of the user. The determining the end of the user s speaking includes recognizing a threshold period of silence after the user s speech in the stream of digital sound samples of the user. In further embodiments the detecting the user s silent facial expression includes recognizing the user s facial expression in the stream of digital pictures of the user and recognizing the user s silence in the stream of digital sound samples of the user. In further embodiments the detecting the user s silent body movement includes recognizing the user s body movement in the stream of digital pictures of the user and recognizing the user s silence in the stream of digital sound samples of the user. In further embodiments the detecting the user s motionless silence includes recognizing no motion or a marginal motion of the user in the stream of digital pictures of the user and recognizing the user s silence in the stream of digital sound samples of the user. The marginal motion of the user includes a motion of the user that does not exceed a threshold for motion. In further embodiments the detecting the user s absence from the conversation includes recognizing the user s absence in the stream of digital pictures of the user.

In some embodiments In further embodiments the detecting the user s first conversational activity includes recognizing the user s visual and verbal expressions or communication in a first part of a simulated conversation. In further embodiments the detecting the user s first conversational activity includes identifying a first sub stream of the stream of digital pictures of the user wherein the first sub stream of the stream of digital pictures of the user comprises the user s visual expressions or communication in a first part of a simulated conversation. In further embodiments the detecting the user s first conversational activity includes identifying a first sub stream of the stream of digital sound samples of the user wherein the first sub stream of the stream of digital sound samples of the user comprises the user s verbal expressions or communication in a first part of a simulated conversation.

In certain embodiments the detecting the user s first conversational activity includes recognizing the user s speech or sound in the stream of digital sound samples of the user. In further embodiments the detecting the user s first conversational activity includes recognizing the user s face or body part in the stream of digital pictures of the user. In further embodiments the detecting the user s first conversational activity includes determining a beginning and an end of the user s first conversational activity.

In certain embodiments the recording of the user s first conversational activity includes the user s visual expressions or communication in a first part of a simulated conversation and the user s verbal expressions or communication in the first part of the simulated conversation.

In some embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital sound samples of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one portion of the first sub stream of the stream of digital sound samples of the user with at least one portion of the first sub stream of the stream of digital sound samples of the first conversation participant. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the user includes one or more words one or more features or one or more sound samples of the first sub stream of the stream of digital sound samples of the user. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the first conversation participant includes one or more words one or more features or one or more sound samples of the first sub stream of a stream of digital sound samples of the first conversation participant.

In certain embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one word recognized from the recording of the user s first conversational activity with at least one word recognized from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one sound feature from the recording of the user s first conversational activity with at least one sound feature from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one sound sample from the recording of the user s first conversational activity with at least one sound sample from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of amplitude adjustment sample rate adjustment noise reduction or temporal alignment of one or more sound samples in the recording of the user s first conversational activity or the recording of the first conversation participant s first conversational activity.

In some embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital sound samples of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of amplitude adjustment sample rate adjustment noise reduction or temporal alignment in the first sub stream of the stream of digital sound samples of the user or the first sub stream of the stream of digital sound samples of the first conversation participant.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital pictures of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one portion of the first sub stream of the stream of digital pictures of the user with at least one portion of the first sub stream of the stream of digital pictures of the first conversation participant. In further embodiments the portion of the first sub stream of the stream of digital pictures of the user includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of the stream of digital pictures of the user. In further embodiments the portion of the first sub stream of the stream of digital pictures of the first conversation participant includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of the stream of digital pictures of the first conversation participant.

In some embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one picture from the recording of the user s first conversational activity with at least one picture from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one region of a picture from the recording of the user s first conversational activity with at least one region of a picture from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one picture feature from the recording of the user s first conversational activity with at least one picture feature from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing at least one pixel from the recording of the user s first conversational activity with at least one pixel from the recording of the first conversation participant s first conversational activity. In further embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of color adjustment size adjustment transparency utilization mask utilization or temporal alignment of one or more pictures in the recording of the user s first conversational activity or the recording of the first conversation participant s first conversational activity.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital pictures of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant wherein the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes performing at least one of color adjustment size adjustment transparency utilization mask utilization or temporal alignment in the first sub stream of the stream of digital pictures of the user or the first sub stream of the stream of digital pictures of the first conversation participant.

In some embodiments the comparing at least one portion of the recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity includes comparing an extra information included in the recording of the user s first conversational activity with an extra information included in the recording of the first conversation participant s first conversational activity. The extra information may include a time information a location information an observed information or a contextual information.

In certain embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a similarity between an extra information included in the recording of the user s first conversational activity and an extra information included in the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The extra information may include a time information a location information an observed information or a contextual information.

In some embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital sound samples of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the first conversation participant wherein the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a similarity between at least one portion of the first sub stream of the stream of digital sound samples of the user and at least one portion of the first sub stream of the stream of digital sound samples of the first conversation participant exceeds a similarity threshold. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the user includes one or more words one or more features or one or more sound samples of the first sub stream of the stream of digital sound samples of the user. In further embodiments the portion of the first sub stream of the stream of digital sound samples of the first conversation participant includes one or more words one or more features or one or more sound samples of the first sub stream of the stream of digital sound samples of the first conversation participant.

In certain embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching words recognized from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching words recognized from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a word a type of a word an importance of a word a semantic variation of a word a concept of a word or a threshold for a difference in a word. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching sound features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching sound features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a sound feature a type of a sound feature an importance of a sound feature or a threshold for a difference in a sound feature. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching sound samples from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching sound samples from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a sound sample an importance of a sound sample or a threshold for a difference in a sound sample.

In some embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes recognizing a speech or a sound of a same person in the at least one portion of the recording of the user s first conversational activity and the at least one portion of the recording of the first conversation participant s first conversational activity.

In certain embodiments the recording of the user s first conversational activity includes a first sub stream of the stream of digital pictures of the user and the recording of the first conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the first conversation participant wherein the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a similarity between at least one portion of the first sub stream of the stream of digital pictures of the user and at least one portion of the first sub stream of the stream of digital pictures of the first conversation participant exceeds a similarity threshold. In further embodiments the portion of the first sub stream of the stream of digital pictures of the user includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of the stream of digital pictures of the user. In further embodiments the portion of the first sub stream of the stream of digital pictures of the first conversation participant includes one or more pictures one or more regions of a picture one or more features of a picture or one or more pixels of a picture of the first sub stream of a stream of digital pictures of the first conversation participant.

In some embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching pictures from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching pictures from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of an order of a picture or a threshold for a difference in a picture. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching regions of pictures from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching regions of a picture from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of a location of a region or a threshold for a difference in a region. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching picture features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching picture features from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of a type of a picture feature an importance of a picture feature a location of a picture feature in a region of interest or a threshold for a difference in a picture feature. In further embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes determining that a number or a percentage of matching pixels from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity exceeds a threshold. In further embodiments the matching pixels from the recording of the user s first conversational activity and from the recording of the first conversation participant s first conversational activity are matched factoring in at least one of a location of a pixel in a region of interest or a threshold for a difference in a pixel.

In certain embodiments the determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold includes recognizing a same person or object in the at least one portion of the recording of the user s first conversational activity and the at least one portion of the recording of the first conversation participant s first conversational activity.

In some embodiments the playing the at least one portion of the recording of the second conversation participant s first conversational activity is performed concurrently with at least one portion of the user s first conversational activity. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of a stream of digital pictures of the second conversation participant wherein the playing the at least one portion of the recording of the second conversation participant s first conversational activity includes playing the at least one portion of the first sub stream of the stream of digital pictures of the second conversation participant. In further embodiments the recording of the second conversation participant s first conversational activity includes a first sub stream of a stream of digital sound samples of the second conversation participant wherein the playing the at least one portion of the recording of the second conversation participant s first conversational activity includes playing the at least one portion of the first sub stream of the stream of digital sound samples of the second conversation participant.

In certain embodiments the stored plurality of rounds of conversational exchange include a second round of conversational exchange the second round of conversational exchange comprising a recording of a first conversation participant s second conversational activity and a recording of a second conversation participant s second conversational activity. The non transitory computer storage medium and or the method further comprise detecting the user s second conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user the detecting performed by the one or more processor circuits of the server. The non transitory computer storage medium and or the method further comprise comparing at least one portion of a recording of the user s second conversational activity with at least one portion of the recording of the first conversation participant s second conversational activity the comparing performed by the one or more processor circuits of the server. The non transitory computer storage medium and or the method further comprise determining that a similarity between at least one portion of the recording of the user s second conversational activity and at least one portion of the recording of the first conversation participant s second conversational activity exceeds a similarity threshold the determining performed by the one or more processor circuits of the server. The non transitory computer storage medium and or the method further comprise playing at least one portion of the recording of the second conversation participant s second conversational activity by the display and the sound producing device of the user s computing device the playing caused by the one or more processor circuits of the server. In further embodiments the first round of conversational exchange is connected to the second round of conversational exchange by a connection. The connection between the first round of conversational exchange and the second round of conversational exchange may include or be associated with at least one of an occurrence count a weight a parameter or a data. In further embodiments the stored plurality of rounds of conversational exchange are organized into a neural network and wherein the first round of conversational exchange is stored into a first node of the neural network and the second round of conversational exchange is stored into a second node of the neural network. The first node and the second node may be connected by a connection. The first node may be part of a first layer of the neural network and the second node may be part of a second layer of the neural network. In further embodiments the stored plurality of rounds of conversational exchange are organized into a graph and wherein the first round of conversational exchange is stored into a first node of the graph and the second round of conversational exchange is stored into a second node of the graph. In further embodiments the first node and the second node may be connected by a connection. In further embodiments the stored plurality of rounds of conversational exchange are organized into a collection of sequences and wherein the first round of conversational exchange is stored into a first node of a sequence of the collection of sequences and the second round of conversational exchange is stored into a second node of the sequence of the collection of sequences. In further embodiments the stored plurality of rounds of conversational exchange are organized into a sequence and wherein the first round of conversational exchange is stored into a first node of the sequence and the second round of conversational exchange is stored into a second node of the sequence.

In some embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed concurrently with the at least one portion of the user s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed subsequent to the at least one portion of the user s first conversational activity.

In certain embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes at least one of moving centering aligning resizing or transforming one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes adjusting a lighting or a color of one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes a cut or a dissolve between one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity includes morphing of one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity.

In some embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity includes interpolation inbetweening extrapolation or picture generation between one or more pictures of the recording of the second conversation participant s first conversational activity and one or more pictures of the recording of the second conversation participant s second conversational activity. In further embodiments the bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity includes playing or replaying one or more pictures of the recording of the second conversation participant s first conversational activity.

In some aspects the disclosure relates to a system for using artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises one or more processor circuits. The system may further include a memory unit coupled to the one or more processor circuits that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity. The system may further include a picture capturing device coupled to the one or more processor circuits configured to capture a stream of digital pictures of a user. The system may further include a sound capturing device coupled to the one or more processor circuits configured to capture a stream of digital sound samples of the user The one or more processor circuits may be configured to detect the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The one or more processor circuits may be further configured to compare at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The one or more processor circuits may be further configured to determine that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The one or more processor circuits may be further configured to cause a display and a sound producing device to play at least one portion of the recording of the second conversation participant s first conversational activity.

In certain embodiments the one or more processor circuits the memory unit the picture capturing device the sound capturing device the display and the sound producing device of the system are included in a single device. In further embodiments at least one of the one or more processor circuits or the memory unit of the system are included in a server and wherein the picture capturing device the sound capturing device the display and the sound producing device of the system are included in a user device the user device coupled to the server via a network.

In some embodiments the stored plurality of rounds of conversational exchange include a second round of conversational exchange the second round of conversational exchange comprising a recording of a first conversation participant s second conversational activity and a recording of a second conversation participant s second conversational activity. The one or more processor circuits may be further configured to detect the user s second conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The one or more processor circuits may be further configured to compare at least one portion of a recording of the user s second conversational activity with at least one portion of the recording of the first conversation participant s second conversational activity. The one or more processor circuits may be further configured to determine that a similarity between at least one portion of the recording of the user s second conversational activity and at least one portion of the recording of the first conversation participant s second conversational activity exceeds a similarity threshold The one or more processor circuits may be further configured to cause a display and a sound producing device to play at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed concurrently with the at least one portion of the user s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed subsequent to the at least one portion of the user s first conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising accessing a memory unit that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity. The operations may further include capturing a stream of digital pictures of a user by a picture capturing device. The operations may further include capturing a stream of digital sound samples of the user by a sound capturing device. The operations may further include detecting the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The operations may further include comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The operations may further include determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The operations may further include playing at least one portion of the recording of the second conversation participant s first conversational activity by a display and a sound producing device.

In some aspects the disclosure relates to a method comprising a accessing a memory unit that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity the accessing of a performed by one or more processor circuits. The method may further include b capturing a stream of digital pictures of a user by a picture capturing device that is coupled to the one or more processor circuits. The method may further include c capturing a stream of digital sound samples of the user by a sound capturing device that is coupled to the one or more processor circuits. The method may further include d detecting the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user the detecting of d performed by the one or more processor circuits. The method may further include e comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity the comparing of e performed by the one or more processor circuits. The method may further include f determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold the determining of f performed by the one or more processor circuits. The method may further include g playing at least one portion of the recording of the second conversation participant s first conversational activity by a display and a sound producing device the playing of g caused by the one or more processor circuits.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable as well as the following embodiments.

In certain embodiments the one or more processor circuits the memory unit the picture capturing device the sound capturing device the display and the sound producing device of the system are included in a single device. In further embodiments at least one of the one or more processor circuits or the memory unit of the system are included in a server and wherein the picture capturing device the sound capturing device the display and the sound producing device of the system are included in a user device the user device coupled to the server via a network.

In some embodiments the stored plurality of rounds of conversational exchange include a second round of conversational exchange the second round of conversational exchange comprising a recording of a first conversation participant s second conversational activity and a recording of a second conversation participant s second conversational activity. The non transitory computer storage medium and or the method further comprise detecting the user s second conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user the detecting performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise comparing at least one portion of a recording of the user s second conversational activity with at least one portion of the recording of the first conversation participant s second conversational activity the comparing performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise determining that a similarity between at least one portion of the recording of the user s second conversational activity and at least one portion of the recording of the first conversation participant s second conversational activity exceeds a similarity threshold the determining performed by the one or more processor circuits. The non transitory computer storage medium and or the method further comprise playing at least one portion of the recording of the second conversation participant s second conversational activity by the display and the sound producing device the playing caused by the one or more processor circuits. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed concurrently with the at least one portion of the user s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed subsequent to the at least one portion of the user s first conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes transitioning from the at least one portion of the recording of the second conversation participant s first conversational activity to the at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity includes bridging between the at least one portion of the recording of the second conversation participant s first conversational activity and the at least one portion of the recording of the second conversation participant s second conversational activity.

In some aspects the disclosure relates to a system for using artificially intelligent interactive memories. The system may be implemented at least in part on one or more computing devices. In some embodiments the system comprises one or more processor circuits. The system may further include a memory unit coupled to the one or more processor circuits that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising recordings of a first conversation participant s first and second conversational activities and recordings of a second conversation participant s first and second conversational activities. The system may further include a picture capturing device coupled to the one or more processor circuits configured to capture a stream of digital pictures of a user. The system may further include a sound capturing device coupled to the one or more processor circuits configured to capture a stream of digital sound samples of the user. The one or more processor circuits may be configured to detect the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The one or more processor circuits may be further configured to compare at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The one or more processor circuits may be further configured to determine that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The one or more processor circuits may be further configured to cause a display and a sound producing device to play at least one portion of the recording of the second conversation participant s first conversational activity.

In certain embodiments the first conversation participant s first conversational activity is correlated with the second conversation participant s first conversational activity and the first conversation participant s second conversational activity is correlated with the second conversation participant s second conversational activity.

In some embodiments the one or more processor circuits may be further configured to cause the display and the sound producing device to play at least one portion of the recording of the second conversation participant s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed concurrently with the at least one portion of the user s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed subsequent to the at least one portion of the user s first conversational activity.

In some aspects the disclosure relates to a non transitory computer storage medium having a computer program stored thereon the program comprising instructions that when executed by one or more computing devices cause the one or more computing devices to perform operations comprising accessing a memory unit that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising recordings of a first conversation participant s first and second conversational activities and recordings of a second conversation participant s first and second conversational activities. The operations may further include capturing a stream of digital pictures of a user by a picture capturing device. The operations may further include capturing a stream of digital sound samples of the user by a sound capturing device. The operations may further include detecting the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The operations may further include comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The operations may further include determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The operations may further include playing at least one portion of the recording of the second conversation participant s first conversational activity by a display and a sound producing device.

In some aspects the disclosure relates to a method comprising a accessing a memory unit that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising recordings of a first conversation participant s first and second conversational activities and recordings of a second conversation participant s first and second conversational activities the accessing of a performed by one or more processor circuits The method may further include b capturing a stream of digital pictures of a user by a picture capturing device that is coupled to the one or more processor circuits. The method may further include c capturing a stream of digital sound samples of the user by a sound capturing device that is coupled to the one or more processor circuits. The method may further include d detecting the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user the detecting of d performed by the one or more processor circuits. The method may further include e comparing at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity the comparing of e performed by the one or more processor circuits. The method may further include f determining that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold the determining of f performed by the one or more processor circuits. The method may further include g playing at least one portion of the recording of the second conversation participant s first conversational activity by a display and a sound producing device the playing of g caused by the one or more processor circuits.

The operations or steps of the non transitory computer storage medium and or the method may be performed by any of the elements of the above described system as applicable. The non transitory computer storage medium and or the method may include any of the operations steps and embodiments of the above described system as applicable as well as the folowing embodiments.

In certain embodiments the first conversation participant s first conversational activity is correlated with the second conversation participant s first conversational activity and the first conversation participant s second conversational activity is correlated with the second conversation participant s second conversational activity.

In some embodiments the non transitory computer storage medium and or the method further comprise playing at least one portion of the recording of the second conversation participant s second conversational activity by the display and the sound producing device the playing caused by the one or more processor circuits. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed concurrently with the at least one portion of the user s second conversational activity. In further embodiments the playing the at least one portion of the recording of the second conversation participant s second conversational activity is performed subsequent to the at least one portion of the user s first conversational activity.

Other features and advantages of the disclosure will become apparent from the following description including the claims and drawings.

Like reference numerals in different figures indicate like elements. Horizontal or vertical . . . or other such indicia may be used to indicate additional instances of the same type of element. n m or other such letters or indicia represent integers or other sequential numbers that follow the sequence where they are indicated. It should be noted that n m and or other such letters or indicia may represent different numbers in different elements even where the elements are depicted in the same figure. In general n m and or other such letters or indicia follow the immediate sequence and or context where they are indicated. Any of these or other such indicia may be used interchangeably according to the context and space available. The drawings are not necessarily to scale with emphasis instead being placed upon illustrating the embodiments principles and concepts of the disclosure. A line or arrow between any of the disclosed elements comprises an interface that enables the coupling connection and or interaction between the elements. Arrows are used for enhanced illustration of the concepts and do not require the indicated directions. Therefore any arrow can be replaced with an undirected line in alternate embodiments. For clarity of illustration white colored streams and sub streams of digital pictures and sound samples are associated with one conversation participant whereas gray colored streams and sub streams of digital pictures and sound samples are associated with another or counterpart conversation participant.

The disclosed devices systems and methods for learning and using artificially intelligent interactive memories comprise apparatuses systems methods features functionalities and or applications for learning conversations among two or more conversation participants and storing this knowledge in a knowledgebase i.e. neural network graph sequences etc. . Then using this stored knowledge the disclosed devices systems and methods enable a user to simulate a conversation with an artificially intelligent conversation participant. The disclosed devices systems and methods for learning and using artificially intelligent interactive memories any of their elements any of their embodiments or a combination thereof can generally be referred to as AIIM AIIM application or as other similar name or reference.

Referring now to an embodiment is illustrated of Computing Device also referred to simply as computing device or other similar name or reference etc. that can provide processing capabilities used in some embodiments of the forthcoming disclosure. Later described devices and systems in combination with processing capabilities of Computing Device enable learning and using artificially intelligent interactive memories and or other functionalities described herein. Various embodiments of the disclosed devices systems and or methods include hardware functions logic programs and or a combination thereof that can be provided or implemented on any type or form of computing or other device such as a mobile device a computer a computing capable telephone a server a cloud device a gaming device a television device a digital camera a GPS receiver a media player an embedded device a supercomputer a wearable device an implantable device or any other type or form of computing or other device capable of performing the operations described herein.

In some designs Computing Device comprises hardware processing techniques or capabilities programs or a combination thereof. Computing Device includes one or more central processing units which may also be referred to as processors . Processor includes one or more memory ports and or one or more input output ports also referred to as I O ports such as I O ports A and B. Processor may be special or general purpose. Computing Device may further include memory which can be connected to the remainder of the components of Computing Device via bus . Memory can be connected to processor via memory port . Computing Device may also include display device such as a monitor projector glasses and or other display device. Computing Device may also include Human machine Interface such as a keyboard a pointing device a mouse a touchscreen a joystick and or other input device that can be connected with the remainder of the Computing Device components via I O control . In some implementations Human machine Interface can be connected with bus or directly connected with specific components of Computing Device . Computing Device may include additional elements such as one or more input output devices . Processor may include or be interfaced with cache memory . Storage may include memory which provides an operating system also referred to as OS additional application programs operating on OS and or data space in which additional data or information can be stored. Alternative memory device can be connected to the remaining components of Computing Device via bus . Network interface can also be connected with bus and be used to communicate with external computing devices via a network. Some or all described elements of Computing Device can be directly or operatively connected or coupled with each other using any other connection means known in art. Other additional elements may be included as needed or some of the disclosed ones may be excluded or a combination thereof may be utilized in alternate implementations of Computing Device .

Processor includes any logic circuitry that can respond to or process instructions fetched from memory or other element. Processor may also include any combination of hardware and or processing techniques or capabilities for implementing or executing logic functions or programs. Processor may include a single core or a multi core processor. Processor includes the functionality for loading operating system and operating any application programs thereon. In some embodiments Processor can be provided in a microprocessing or a processing unit such as for example Snapdragon processor produced by Qualcomm Inc. processor by Intel Corporation of Mountain View Calif. processor manufactured by Motorola Corporation of Schaumburg Ill. processor manufactured by Transmeta Corporation of Santa Clara Calif. the RS 6000 processor processor manufactured by International Business Machines of White Plains N.Y. processor manufactured by Advanced Micro Devices of Sunnyvale Calif. or any computing unit for performing similar functions. In other embodiments processor can be provided in a graphics processor unit GPU visual processor unit VPU or other highly parallel processing unit or circuit such as for example nVidia GeForce line of GPUs AMD Radeon line of GPUs and or others. Such GPUs or other highly parallel processing units may provide superior performance in processing operations on later described neural networks. Computing Device can be based on one or more of these or other processors capable of operating as described herein.

Memory includes one or more memory chips capable of storing data and allowing any storage location to be accessed by processor such as Static random access memory SRAM Flash memory Burst SRAM or SynchBurst SRAM BSRAM Dynamic random access memory DRAM Fast Page Mode DRAM FPM DRAM Enhanced DRAM EDRAM Extended Data Output RAM EDO RAM Extended Data Output DRAM EDO DRAM Burst Extended Data Output DRAM BEDO DRAM Enhanced DRAM EDRAM synchronous DRAM SDRAM JEDEC SRAM PC100 SDRAM Double Data Rate SDRAM DDR SDRAM Enhanced SDRAM ESDRAM SyncLink DRAM SLDRAM Direct Rambus DRAM DRDRAM Ferroelectric RAM FRAM and or others. Memory can be based on any of the above described memory chips or any other available memory chips capable of operating as described herein. In some embodiments processor can communicate with memory via a system bus . In other embodiments processor can communicate directly with memory via a memory port .

Processor can communicate directly with cache memory via a connection means such as a secondary bus which may also sometimes be referred to as a backside bus. In some embodiments processor can communicate with cache memory using the system bus . Cache memory may typically have a faster response time than main memory and can include a type of memory which is considered faster than main memory such as for example SRAM BSRAM or EDRAM. Cache memory includes any structure such as multilevel caches for example. In some embodiments processor can communicate with one or more I O devices via a system bus . Various busses can be used to connect processor to any of the I O devices such as a VESA VL bus an ISA bus an EISA bus a MicroChannel Architecture MCA bus a PCI bus a PCI X bus a PCI Express bus a NuBus and or others. In some embodiments processor can communicate directly with I O device via HyperTransport Rapid I O or InfiniBand. In further embodiments local busses and direct communication can be mixed. For example processor can communicate with an I O device using a local interconnect bus and communicate with another I O device directly. Similar configurations can be used for any other components described herein.

Computing Device may further include alternative memory such as a SD memory slot a USB memory stick an optical drive such as a CD ROM drive a CD RIRW drive a DVD ROM drive or a BlueRay disc a hard drive and or any other device comprising non volatile memory suitable for storing data or installing application programs. Computing Device may further include a storage device comprising any type or form of non volatile memory for storing an operating system OS such as any type or form of Windows OS Mac OS Unix OS Linux OS Android OS iPhone OS mobile version of Windows OS an embedded OS or any other OS that can operate on Computing Device . Computing Device may also include application programs and or data space for storing additional data or information. In some embodiments alternative memory can be used as or similar to storage device . Additionally OS and or application programs can be operable from a bootable medium such as for example a flash drive a micro SD card a bootable CD or DVD and or other bootable medium.

Application Program also referred to as program computer program application script code etc. comprises instructions that can provide functionality when executed by processor . Application program can be implemented in a high level procedural or object oriented programming language or in a low level machine or assembly language. Any language used can be compiled interpreted or otherwise translated into machine language. Application program can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing system. Application program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that may hold other programs or data in a single file dedicated to the program or in multiple files i.e. files that store one or more modules sub programs or portions of code etc. . Application program can be deployed to be executed on one computer or on multiple computers i.e. cloud distributed or parallel computing etc. or at one site or distributed across multiple sites interconnected by a communication network.

Network interface can be utilized for interfacing Computing Device with other devices via a network through a variety of connections including standard telephone lines wired or wireless connections LAN or WAN links i.e. 802.11 T1 T3 56 kb X.25 etc. broadband connections i.e. ISDN Frame Relay ATM etc. or a combination thereof. Examples of networks include the Internet an intranet an extranet a local area network LAN a wide area network WAN a personal area network PAN a home area network HAN a campus area network CAN a metropolitan area network MAN a global area network GAN a storage area network SAN virtual network a virtual private network VPN Bluetooth network a wireless network a wireless LAN a radio network a HomePNA a power line communication network a G.hn network an optical fiber network an Ethernet network an active networking network a client server network a peer to peer network a bus network a star network a ring network a mesh network a star bus network a tree network a hierarchical topology network and or other networks known in art. Network interface may include a built in network adapter network interface card PCMCIA network card card bus network adapter wireless network adapter Bluetooth network adapter WiFi network adapter USB network adapter modem and or any other device suitable for interfacing Computing Device with any type of network capable of communication and or operations described herein.

Still referring to I O devices may be present in various shapes or forms in Computing Device . Examples of I O device capable of input include a joystick a keyboard a mouse a trackpad a trackpoint a touchscreen a trackball a microphone a drawing tablet a glove a tactile input device a still or video camera and or other input device. Examples of I O device capable of output include a video display a touchscreen a projector a glasses a speaker a tactile output device and or other output device. Examples of I O device capable of input and output include a disk drive an optical storage device a modem a network card and or other input output device. I O device can be interfaced with processor via an I O port for example. I O device can also be controlled by I O control in some implementations. I O control may control one or more I O devices such as Human machine Interface i.e. keyboard pointing device touchscreen joystick mouse optical pen etc. . I O control enables any type or form of a device such as for example a video camera or microphone to be interfaced with other components of Computing Device . Furthermore I O device may also provide storage such as or similar to storage and or alternative memory such as or similar to alternative memory in some implementations.

An output interface such as a graphical user interface an acoustical output interface a tactile output interface any device driver i.e. audio video or other driver and or other output interface or system can be utilized to process output from elements of Computing Device for conveyance on an output device such as Display . In some aspects Display or other output device itself may include an output interface for processing output from elements of Computing Device . Further an input interface such as a keyboard listener a touchscreen listener a mouse listener any device driver i.e. audio video keyboard mouse touchscreen or other driver a speech recognizer a video interpreter and or other input interface or system can be utilized to process input from Human machine Interface or other input device for use by elements of Computing Device . In some aspects Human machine Interface or other input device itself may include an input interface for processing input for use by elements of Computing Device .

Computing Device may include or be connected to multiple display devices . Display devices can each be of the same or different type or form. Computing Device and or its elements comprise any type or form of suitable hardware programs or a combination thereof to support enable or provide for the connection and use of multiple display devices . In one example Computing Device includes any type or form of video adapter video card driver and or library to interface communicate connect or otherwise use display devices . In some aspects a video adapter may include multiple connectors to interface to multiple display devices . In other aspects Computing Device includes multiple video adapters with each video adapter connected to one or more display devices . In some embodiments Computing Device s operating system can be configured for using multiple displays . In other embodiments one or more display devices can be provided by one or more other computing devices such as remote computing devices connected to Computing Device via a network.

In some embodiments I O device can be a bridge between system bus and an external communication bus such as a USB bus an Apple Desktop Bus an RS 232 serial connection a SCSI bus a FireWire bus a FireWire 800 bus an Ethernet bus an AppleTalk bus a Gigabit Ethernet bus an Asynchronous Transfer Mode bus a HIPPI bus a Super HIPPI bus a SerialPlus bus a SCI LAMP bus a FibreChannel bus a Serial Attached small computer system interface bus and or other bus.

Computing Device can operate under the control of an operating system which may support Computing Device s basic functions interface with and manage hardware resources interface with and manage peripherals provide common services for application programs schedule tasks and or perform other functionalities. A modern operating system enables features and functionalities such as a high resolution display graphical user interface GUI touchscreen cellular network connectivity i.e. mobile operating system etc. Bluetooth connectivity WiFi connectivity global positioning system GPS capabilities mobile navigation microphone speaker still picture camera video camera voice recorder speech recognition music player video player near field communication personal digital assistant PDA and or other features functionalities or applications. For example Computing Device can use any conventional operating system any embedded operating system any real time operating system any open source operating system any video gaming operating system any proprietary operating system any online operating system any operating system for mobile computing devices or any other operating system capable of running on Computing Device and performing operations described herein. Typical operating systems include Windows XP Windows 7 Windows 8 etc. manufactured by Microsoft Corporation of Redmond Wash. Mac OS iPhone OS etc. manufactured by Apple Computer of Cupertino Calif. OS 2 manufactured by International Business Machines of Armonk N.Y. Linux a freely available operating system distributed by Caldera Corp. of Salt Lake City Utah or any type or form of a Unix operating system among others. Any operating systems such as the ones for Android devices can similarly be utilized.

Computing Device can be implemented as or be part of various different model architectures such as web services distributed computing grid computing cloud computing and or other architectures or environments. For example in addition to the traditional desktop server or mobile operating system architectures a cloud based operating system can be utilized to provide the structure on which embodiments of the disclosure can be implemented. Other aspects of Computing Device can also be implemented in the cloud without departing from the spirit and scope of the disclosure. For example memory storage processing and or other elements can be hosted in the cloud. In some embodiments Computing Device can be implemented on multiple devices. For example a portion of Computing Device can be implemented on a mobile device and another portion can be implemented on wearable electronics.

Computing Device can be or include any mobile device a mobile phone a smartphone i.e. iPhone Windows phone Blackberry Android phone etc. a tablet a personal digital assistant PDA wearable electronics implantable electronics or another mobile device capable of implementing the functionalities described herein. In other embodiments Computing Device can be or include an embedded device which can be any device or system with a dedicated function within another device or system. Embedded systems range from the simplest ones dedicated to one task with no user interface to complex ones with advanced user interface that may resemble modern desktop computer systems. Examples of devices comprising an embedded device include a mobile telephone a personal digital assistant PDA a gaming device a media player a digital still or video camera a pager a television device a set top box a personal navigation device a global positioning system GPS receiver a portable storage device i.e. a USB flash drive etc. a digital watch a DVD player a printer a microwave oven a washing machine a dishwasher a gateway a router a hub an automobile entertainment system an automobile navigation system a refrigerator a washing machine a factory automation device an assembly line device a factory floor monitoring device a thermostat an automobile a factory controller a telephone a network bridge and or other devices. An embedded device can operate under the control of an operating system for embedded devices such as MicroC OS II QNX VxWorks eCos TinyOS Windows Embedded Embedded Linux and or other embedded device operating systems.

Various implementations of the disclosed devices systems and or methods can be realized in digital electronic circuitry integrated circuitry logic gates specially designed application specific integrated circuits ASICs field programmable gate arrays FPGAs computer hardware firmware programs virtual machines and or combinations thereof including their structural logical and or physical equivalents.

The disclosed devices systems and or methods may include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of a client and server may arise by virtue of computer programs running on the respective computers and having a client server relationship to each other.

The disclosed devices systems and or methods can be implemented in a computing system that includes a back end component a middleware component a front end component or any combination thereof. The components of the system can be interconnected by any form or medium of digital data communication such as for example a network.

Computing Device may include or be interfaced with a computer program product comprising instructions or logic encoded on a computer readable medium that when performed in a computing device configure a processor to perform the operations and or functionalities disclosed herein. For example a computer program can be provided or encoded on a computer readable medium such as an optical medium i.e. DVD ROM etc. flash drive hard drive any memory firmware or other medium. Computer program can be installed onto a computing device to cause the computing device to perform the operations and or functionalities disclosed herein. As used in this disclosure machine readable medium computer readable medium or other such terms may refer to any computer program product apparatus and or device for providing instructions and or data to a programmable processor. As such machine readable medium includes any medium that can send or receive machine instructions as a machine readable signal. The term machine readable signal may refer to any signal used for providing instructions and or data to a programmable processor. Examples of a machine readable medium include a volatile and or non volatile medium a removable and or non removable medium a communication medium a storage medium and or other medium. A communication medium for example can transmit computer readable instructions and or data in a modulated data signal such as a carrier wave or other transport technique and may include any other form of information delivery medium known in art. A non transitory machine readable medium comprises all machine readable media except for a transitory propagating signal.

Where a reference to a specific file or file type is used herein other files file types or formats can be substituted.

Where a reference to a data structure is used herein it should be understood that any variety of data structures can be used such as for example array list linked list doubly linked list queue tree heap graph map grid matrix multi dimensional matrix table database database management system DBMS file neural network and or any other type or form of a data structure including a custom one. A data structure may include one or more fields or data fields that are part of or associated with the data structure. A field or data field may include a data an object a data structure and or any other element or a reference pointer thereto. A data structure can be stored in one or more memories files or other repositories. A data structure and or any elements thereof when stored in a memory file or other repository may be stored in a different arrangement than the arrangement of the data structure and or any elements thereof. For example a sequence of elements can be stored in an arrangement other than a sequence in a memory file or other repository.

Where a reference to a repository is used herein it should be understood that a repository may be or include one or more files or file systems one or more storage locations or structures one or more storage systems one or more data structures or objects one or more memory locations or structures and or other storage memory or data arrangements.

Where a reference to an interface is used herein it should be understood that the interface comprises any hardware device system program method and or combination thereof that enable direct or operative coupling connection and or interaction of the elements between which the interface is indicated. A line or arrow shown in the figures between any of the depicted elements comprises such interface. Examples of an interface include a direct connection an operative connection a wired connection i.e. wire cable etc. a wireless connection a device a network a bus a circuit a firmware a driver a bridge a program a combination thereof and or others.

Where a reference to an element coupled or connected to a processor is used herein it should be understood that the element may be part of or operating on the processor. Also an element coupled or connected to another element may include the element in communication or any other interactive relationship with the other element. Furthermore an element coupled or connected to another element can be coupled or connected to any other element in alternate implementations. Terms coupled connected interfaced or other such terms may be used interchangeably herein.

Where a mention of a function method routine subroutine or other such procedure is used herein it should be understood that the function method routine subroutine or other such procedure comprises a call reference or pointer to the function method routine subroutine or other such procedure.

Where a mention of data object data structure item element or thing is used herein it should be understood that the data object data structure item element or thing comprises a reference or pointer to the data object data structure item element or thing.

The term operating or operation can refer to processing executing or other such actions and vice versa. Therefore the terms operating operation processing executing or other such actions may be used interchangeably herein.

The term collection of elements can refer to plurality of elements without implying that the collection is an element itself.

Referring to an embodiment of utilizing System for Learning AIIMs in a dedicated device is illustrated. In some aspects the device can be placed on a table between Conversation Participants and as shown. In other aspects the device can be mounted attached or placed on a wall ceiling or other convenient object or location.

Referring to an embodiment of internal structure of System for Learning AIIMs in a dedicated device is illustrated. System for Learning AIIMs comprises interconnected Knowledge Structuring Unit Knowledgebase Picture capturing Devices and Sound capturing Devices and and Activity Detectors and . Some embodiments of Activity Detector may include Picture Recognizer and Speech Sound Recognizer whereas some embodiments of Activity Detector may include Picture Recognizer and Speech Sound Recognizer . Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments.

System for Learning AIIMs comprises any hardware programs or a combination thereof. System for Learning AIIMs comprises the functionality for learning conversations. As the participants in a conversation exchange verbal and visual expressions or communication System for Learning AIIMs may capture and learn these conversational exchanges. Additionally System for Learning AIIMs may interconnect interrelate or interlink rounds of conversational exchange into a knowledge structure such as Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data or knowledge structure. System for Learning AIIMs also comprises the functionality for learning a person s conversational style or character. Knowledge learned by System for Learning AIIMs can be used to simulate a conversation with a person i.e. AI Conversation Participant later described etc. in the person s absence after the person is deceased or in any situation where a conversation may be needed with the person. For example knowledge learned by System for Learning AIIMs can be used by a System for Using AIIMs later described to simulate a conversation with a parent predecessor i.e. grandparent etc. spouse friend historical figure famous person i.e. scientist athlete actor musician politician etc. and or other persons. System for Learning AIIMs comprises other functionalities disclosed herein. Statistical artificial intelligence machine learning and or other models or techniques are utilized to implement the disclosed devices systems and methods.

In some embodiments the disclosed systems for learning and using AIIMs i.e. System for Learning AIIMs System for Using AIIMs later described etc. or elements thereof can be implemented entirely or in part in a device i.e. microchip circuitry logic gates electronic device computing device special or general purpose processor etc. or system that comprises i.e. hard coded internally stored etc. or is provided with i.e. externally stored etc. instructions for implementing AIIM functionalities. As such the disclosed systems for learning and using AIIMs or elements thereof may include the processing memory storage and or other features functionalities and embodiments of Processor and or other elements of Computing Device . Such device or system can operate on its own i.e. standalone device etc. be embedded in another device or system i.e. a television device a set top box a gaming device a smartphone a still or motion picture camera and or any other device capable of housing the elements needed for AIIM functionalities work in combination with other devices or systems or be available in any other configuration. In other embodiments the disclosed systems for learning and using AIIMs or elements thereof can be implemented entirely or in part as a computer program and executed by one or more Processors . Such system or elements thereof can be implemented in one or more modules or units of a single or multiple computer programs. In yet other embodiments the disclosed systems for learning and using AIIMs or elements thereof may be included in Alternative Memory that provides instructions for implementing AIIM functionalities to one or more Processors . In further embodiments the disclosed systems for learning and using AIIMs or elements thereof can be implemented as a network web distributed cloud or other such application accessed on one or more remote computing devices i.e. servers cloud etc. via Network Interface such remote computing devices including processing capabilities and instructions for implementing AIIM functionalities. In some aspects the disclosed systems for learning and using AIIMs or elements thereof can be attached to or interfaced with any computing device or application program included as a feature of an operating system running on a computing device built i.e. hard coded etc. into any computing device or application program and or available in any other configuration to provide its functionalities.

In one example the teaching presented by the disclosure can be implemented in a device or system for learning AIIMs. The device or system may include one or more processor circuits coupled to a memory unit. The device or system may further include a first picture capturing device configured to capture a stream of digital pictures of a first conversation participant and a second picture capturing device configured to capture a stream of digital pictures of a second conversation participant the first and the second picture capturing devices coupled to the one or more processor circuits. The device or system may further include a first sound capturing device configured to capture a stream of digital sound samples of the first conversation participant and a second sound capturing device configured to capture a stream of digital sound samples of the second conversation participant the first and the second sound capturing devices coupled to the one or more processor circuits. The one or more processor circuits may be configured to detect the first conversation participant s first conversational activity from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant and detect the second conversation participant s first conversational activity from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. The one or more processor circuits may also be configured to generate a first round of conversational exchange including a recording of the first conversation participant s first conversational activity and a recording of the second conversation participant s first conversational activity. The one or more processor circuits may also be configured to cause the memory unit to store the first round of conversational exchange the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange. Any of the operations of the described elements can be performed repeatedly and or in different orders in alternate embodiments. In some aspects the one or more processor circuits the memory unit the first picture capturing device the second picture capturing device the first sound capturing device and the second sound capturing device are part of a single device. In other aspects at least one of the one or more processor circuits or the memory unit are part of a server whereas the first picture capturing device and the first sound capturing device are part of a first computing device and the second picture capturing device and the second sound capturing device are part of a second computing device the first and the second computing devices coupled to the server via a network. Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments. The device or system for learning AIIMs may include any actions or operations of any of the disclosed methods such as methods and or all later described .

In another example any of the systems disclosed herein i.e. System for Learning AIIMs System for Using AIIMs later described etc. or elements thereof can be implemented at least in part as a computer program such as a Java application or program. Java provides a robust and flexible environment for application programs including flexible user interfaces robust security built in network protocols powerful application programming interfaces database or DBMS connectivity and interfacing functionalities file manipulation capabilities support for networked applications and or other features or functionalities. Application programs based on Java can be portable across many devices yet leverage each device s native capabilities. Java supports the feature sets of most smartphones and a broad range of connected devices while still fitting within their resource constraints. Various Java platforms include virtual machine features comprising a runtime environment for application programs such as some embodiments of the systems disclosed herein i.e. System for Learning AIIMs System for Using AIIMs etc. or elements thereof. Java platforms provide a wide range of user level functionalities that can be implemented in application programs such as an Internet browser displaying text and graphics playing and recording audio content displaying and recording visual content communicating with another computing device and or other functionalities. It should be understood that the systems disclosed herein i.e. System for Learning AIIMs System for Using AIIMs etc. or elements thereof are programming language platform and operating system independent. Examples of programming languages that can be used instead of or in addition to Java include C C Cobol Python Java Script Tcl Visual Basic Pascal VB Script Perl PHP Ruby and or other programming languages capable of implementing the functionalities described herein.

Knowledgebase comprises the functionality for storing the knowledge of one or more conversations and or other functionalities. In some embodiments Knowledgebase may be or include Neural Network later described . In other embodiments Knowledgebase may be or include Graph later described . In further embodiments Knowledgebase may be or include Collection of Sequences later described . In further embodiments Knowledgebase may be or include Sequence later described . In general Knowledgebase may be or include any knowledge or data structure capable of storing the knowledge of one or more conversations and or other data. Knowledgebase may reside locally on System for Learning AIIMs or Computing Device or remotely i.e. remote Knowledgebase etc. on a remote computing device i.e. server cloud etc. accessible over a network.

Picture capturing Device comprises the functionality for capturing one or more pictures and or other functionalities. As such Picture capturing Device can be used to capture pictures comprising a Conversation Participant s visual expressions or communication. Most modern computing and mobile devices include Picture capturing Device as one of the input devices. In some embodiments Picture capturing Device may be or comprises a motion or still picture camera or other picture capturing device. In general Picture capturing Device may capture any light i.e. visible light infrared light ultraviolet light x ray light etc. across the electromagnetic spectrum onto a light sensitive material. In one example a digital Picture capturing Device can utilize a charge coupled device CCD a CMOS sensor and or other electronic image sensor to capture a Stream of Digital Pictures later described that can then be stored in a memory storage or transmitted to a processing element such as Activity Detector Knowledge Structuring Unit and or other disclosed elements. In another example analog Picture capturing Device can utilize an analog to digital converter to produce a Stream of Digital Pictures . In some embodiments Picture capturing Device can be built embedded or integrated in System for Learning AIIMs . In other embodiments Picture capturing Device can be an external Picture capturing Device connected with System for Learning AIIMs . In further embodiments Picture capturing Device comprises Computing Device or elements thereof. In general Picture capturing Device can be implemented in any suitable configuration to provide its functionalities.

Sound capturing Device comprises the functionality for capturing one or more sounds and or other functionalities. As such Sound capturing Device can be used to capture sounds comprising a Conversation Participant s verbal expressions or communication. Most modern computing and mobile devices include Sound capturing Device as one of the input devices. In some embodiments Sound capturing Device may be or comprises a microphone or other sound capturing device. In some designs microphone may be a directional microphone that enables capturing sounds from a direction where it is pointed while ignoring or being insensitive to sounds from other directions. Such directional microphone may be used for capturing sounds of a particular Conversation Participant while ignoring or being insensitive to other sounds i.e. ambient sounds noise sounds of other Conversation Participants etc. . In general Sound capturing Device may produce electrical signal from air pressure variations. Samples of the electrical signal can then be read to produce a stream of digital sound samples used herein. In one example a digital Sound capturing Device may include an integrated analog to digital converter to capture a Stream of Digital Sound Samples that can then be stored in a memory storage or transmitted to a processing element such as Activity Detector Knowledge Structuring Unit and or other disclosed elements. In another example analog Sound capturing Device may utilize an external analog to digital converter to produce a Stream of Digital Sound Samples . In some embodiments Sound capturing Device can be built embedded or integrated in System for Learning AIIMs . In other embodiments Sound capturing Device can be an external Sound capturing Device connected with System for Learning AIIMs . In further embodiments Sound capturing Device comprises Computing Device or elements thereof. In general Sound capturing Device can be implemented in any suitable configuration to provide its functionalities.

Stream of Digital Pictures comprises the functionality for storing a plurality of digital pictures and or other functionalities. In some aspects Stream of Digital Pictures comprises a live feed from Picture capturing Device . In other aspects Stream of Digital Pictures comprises previously captured and stored Stream of Digital Pictures . System for Learning AIIMs can use the previously captured and stored Streams of Digital Pictures to implement its learning functionalities in which case Picture capturing Device can optionally be omitted. Pictures i.e. frames in a stream of digital pictures can be time stamped or sequenced. In some embodiments Stream of Digital Pictures comprises any type or form of digital motion picture such as MPEG AVI FLV MOV RM SWF WMV DivX and or other digitally encoded motion picture. In other embodiments Stream of Digital Pictures comprises a plurality of any type or form of digital pictures such as digital bitmaps JPEG pictures GIF pictures TIFF pictures PDF pictures and or others. In yet other embodiments Stream of Digital Pictures comprises any computer generated pictures such as views of a 3D game 3D application or CAD CAM application captured or rendered as a stream of digital pictures. In further embodiments Stream of Digital Pictures comprises any application or process that can generate a stream of digital pictures or other visual content. A Stream of Digital Pictures and a Stream of Digital Sound Samples may commonly be included in a file i.e. video file etc. comprising both the Stream of Digital Pictures and the Stream of Digital Sound Samples . Stream of Digital Pictures includes any features functionalities and embodiments of Sub stream of Digital Pictures later described and vice versa. Also any operations on Stream of Digital Pictures can be similarly performed on Sub stream of Digital Pictures and vice versa.

Stream of Digital Sound Samples comprises the functionality for storing a plurality of digital sound samples and or other functionalities. In some aspects Stream of Digital Sound Samples comprises a live feed from Sound capturing Device . In other aspects Stream of Digital Sound Samples comprises previously captured and stored Stream of Digital Sound Samples . System for Learning AIIMs can use the previously captured and stored Streams of Digital Sound Samples to implement its learning functionalities in which case Sound capturing Device can optionally be omitted. Sound samples in a stream of digital sound samples can be time stamped or sequenced. In some embodiments Stream of Digital Sound Samples comprises any type or form of digital sound such as WAV WMA AIFF MP3 RA OGG and or other digitally encoded sound. In other embodiments Stream of Digital Sound Samples comprises any computer generated stream of digital sound samples such as synthesized sound. In further embodiments Stream of Digital Sound Samples comprises any application or process that can generate a stream of digital sound samples or other audio content. Stream of Digital Sound Samples includes any features functionalities and embodiments of Sub stream of Digital Sound Samples later described and vice versa. Also any operations on Stream of Digital Sound Samples can be similarly performed on Sub stream of Digital Sound Samples and vice versa.

In some embodiments Picture capturing Device may capture Stream of Digital Pictures comprising Conversation Participant s visual expressions or communication. Sound capturing Device may capture Stream of Digital Sound Samples comprising Conversation Participant s verbal expressions or communication. In some aspects Stream of Digital Pictures is captured simultaneously with Stream of Digital Sound Samples . Stream of Digital Pictures may therefore be associated with or correspond to Stream of Digital Sound Samples . Similarly Picture capturing Device may capture Stream of Digital Pictures comprising Conversation Participant s visual expressions or communication. Sound capturing Device may capture Stream of Digital Sound Samples comprising Conversation Participant s verbal expressions or communication. In some aspects Stream of Digital Pictures is captured simultaneously with Stream of Digital Sound Samples . Stream of Digital Pictures may therefore be associated with or correspond to Stream of Digital Sound Samples

Activity Detector comprises the functionality for identifying detecting or determining conversational activities also referred to simply as activities etc. of a Conversation Participant and or other functionalities. Examples of conversational activities include speaking silent facial expressions silent body movements motionless silence absence from the conversation and or others. Silent facial expressions silent body movements motionless silence and or other silent activities may be referred to as observing conversational activities. Some aspects of a conversation may involve exchanging verbal communication i.e. speech sounds etc. among Conversation Participants . Other aspects of a conversation may involve exchanging visual communication i.e. facial expressions gestures body language etc. among Conversation Participants . These and or other forms of communication or expressions may also be combined. Activity Detector can detect conversational activities of a Conversation Participant by processing either or both Stream of Digital Pictures comprising the Conversation Participant s visual expressions or communication and or Stream of Digital Sound Samples comprising the Conversation Participant s verbal expressions or communication. Activity Detector comprises the functionality for identifying Sub streams of Digital Pictures in a Stream of Digital Pictures . A Sub stream of Digital Pictures may include a Conversation Participant s visual expressions or communication in a part of a conversation. Similarly Activity Detector comprises the functionality for identifying Sub streams of Digital Sound Samples in a Stream of Digital Sound Samples . A Sub stream of Digital Sound Samples may include a Conversation Participant s verbal expressions or communication in a part of a conversation. Activity Detector comprises the functionality for creating or generating a Conversational Activity also referred to as activity Conv Activity etc. and storing one or more Sub streams of Digital Pictures one or more Sub streams of Digital Sound Samples and or other data i.e. Extra Info later described etc. into the Conversational Activity . As such Conversational Activity comprises the functionality for storing one or more Sub streams of Digital Pictures one or more Sub streams of Digital Sound Samples and or other data. Conversational Activity may therefore include a recording or data structure of a Conversation Participant s conversational activity. Activity Detector also comprises the functionality for extracting or filtering persons and or objects of interest from Sub streams of Digital Pictures and extracting or filtering speech and or sounds of interest from Sub streams of Digital Sound Samples as later described.

In one example Activity Detector can detect Conversation Participant s speaking activity by recognizing Conversation Participant s speech in Stream of Digital Sound Samples . Specifically for instance Activity Detector can identify a beginning of Conversation Participant s speaking activity by recognizing Conversation Participant s speech in Stream of Digital Sound Samples after a threshold period of silence i.e. no speech or sound etc. . Further Activity Detector can identify an end of Conversation Participant s speaking activity by recognizing a threshold period of silence in Stream of Digital Sound Samples after the Conversation Participant s speech. Recognizing silence in Stream of Digital Sound Samples may mark a beginning of a new activity such as silent facial expressions silent body movements motionless silence absence from the conversation and or other activity. In another example Activity Detector can detect Conversation Participant s silent facial expressions activity by recognizing Conversation Participant s facial expressions i.e. smiling lifting eyebrows etc. in Stream of Digital Pictures and by recognizing Conversation Participant s silence i.e. no speech or sound etc. in Stream of Digital Sound Samples . In another example Activity Detector can detect Conversation Participant s silent body movements activity by recognizing Conversation Participant s body movements i.e. nodding head shaking head shrugging shoulders pointing finger pointing fist etc. in Stream of Digital Pictures and by recognizing Conversation Participant s silence i.e. no speech or sound etc. in Stream of Digital Sound Samples . In a further example Activity Detector can detect Conversation Participant s motionless silence activity by recognizing no or marginal motion i.e. no facial change no body movement etc. of Conversation Participant in Stream of Digital Pictures and by recognizing Conversation Participant s silence i.e. no speech or sound etc. in Stream of Digital Sound Samples . For instance marginal motion of Conversation Participant may include comparing one picture of Stream of Digital Pictures with another i.e. subsequent etc. picture of Stream of Digital Pictures and determining that a number or percentage difference between regions of the two pictures comprising Conversation Participant does not exceed a threshold. Other techniques known in art for determining marginal motion can be utilized. In a further example Activity Detector can detect Conversation Participant s absence from the conversation activity by recognizing Conversation Participant s absence i.e. Conversation Participant missing from the field of view etc. in Stream of Digital Pictures and or by recognizing Conversation Participant s silence i.e. no speech or sound etc. in Stream of Digital Sound Samples . In some aspects detecting any new conversational activity may mark an end to a previously detected conversational activity. In other aspects Conversation Participant s conversational activity may affect the determination of Conversation Participant s conversational activity i.e. one conversation participant s conversational activity may be related or depend on another conversation participant s conversational activity etc. . Activity Detector may identify Sub stream of Digital Pictures in Stream of Digital Pictures the Sub stream of Digital Pictures comprising Conversation Participant s visual expressions or communication in a part of a conversation. Similarly Activity Detector may identify Sub stream of Digital Sound Samples in Stream of Digital Sound Samples the Sub stream of Digital Sound Samples comprising Conversation Participant s verbal expressions or communication in a part of a conversation. Activity Detector may then create or generate Conversational Activity comprising Sub stream of Digital Pictures Sub stream of Digital Sound Samples and or other data i.e. Extra Info later described etc. . Activity Detector may detect Conversation Participant s activities and create or generate Conversation Participant s Conversational Activities using similar techniques as described above.

In some embodiments Activity Detector can utilize Picture Recognizer to detect persons objects and or their activities in Stream of Digital Pictures . Similarly Activity Detector can utilize Speech Sound Recognizer to detect speech and or sounds in Stream of Digital Sound Samples . In general Activity Detector and or other disclosed elements can use Picture Recognizer and or Speech Sound Recognizer for any other operation supported by Picture Recognizer and or Speech Sound Recognizer .

Picture Recognizer comprises the functionality for detecting or recognizing persons or objects in visual data. Picture Recognizer comprises the functionality for detecting activities in visual data. Picture Recognizer comprises the functionality for tracking persons objects and or their activities in visual data. Picture Recognizer comprises other disclosed functionalities. Visual data includes digital motion pictures digital still pictures i.e. bitmaps etc. and or other visual data. Examples of file formats that can be utilized to store visual data include AVI DivX MPEG JPEG GIF TIFF PNG PDF and or other file formats. Picture Recognizer may detect or recognize a person and or his her activities as well as track the person and or his her activities in Stream of Digital Pictures . Picture Recognizer may detect or recognize a human head or face upper body full body or portions combinations thereof. In some aspects Picture Recognizer may detect or recognize persons objects and or their activities from a picture i.e. frame etc. of Stream of Digital Pictures by comparing regions of pixels from the picture i.e. frame etc. with collections of pixels comprising known persons objects and or their activities. The collections of pixels comprising known persons objects and or their activities can be learned or manually programmatically or otherwise defined. The collections of pixels comprising known persons objects and or their activities can be stored in any data structure or repository i.e. database etc. that resides locally on System for Learning AIIMs or Computing Device or remotely on a remote computing device i.e. server cloud etc. accessible over a network. In other aspects Picture Recognizer may detect or recognize persons objects and or their activities from a picture i.e. frame etc. of Stream of Digital Pictures by comparing features i.e. lines edges ridges corners blobs regions etc. of the picture i.e. frame etc. with features of known persons objects and or their activities. The features of known persons objects and or their activities can be learned or manually programmatically or otherwise defined. The features of known persons objects and or their activities can be stored in any data structure or repository i.e. neural network database etc. that resides locally on System for Learning AIIMs or Computing Device or remotely on a remote computing device i.e. server cloud etc. accessible over a network. Typical steps or elements in a feature oriented picture recognition include pre processing feature extraction detection segmentation decision making and or others or a combination thereof each of which may include its own sub steps or sub elements depending on the application. In further aspects Picture Recognizer may detect or recognize multiple persons objects and or their activities from a picture i.e. frame etc. of Stream of Digital Pictures using the aforementioned pixel or feature comparisons and or other detection or recognition techniques. For example a picture of Stream of Digital Pictures may depict two persons in two of its regions both of whom Picture Recognizer can detect simultaneously. This functionality can be used in embodiments where Conversation Participants need to be detected or recognized in a single Stream of Digital Pictures . In further aspects where persons objects and or their activities span multiple pictures Picture Recognizer may detect or recognize persons objects and or their activities by applying the aforementioned pixel or feature comparisons and or other detection or recognition techniques over a stream or sub stream of pictures. For example once a person is detected in a picture of Stream of Digital Pictures the region of pixels comprising the detected person or the person s features can be searched in other pictures of Stream of Digital Pictures thereby tracking the person through the Stream of Digital Pictures . In further aspects Picture Recognizer may detect or recognize a person s activities by identifying and or analyzing differences between a detected region of pixels of one picture and detected regions of pixels of other pictures in Stream of Digital Pictures . For example a region of pixels comprising a person s face can be detected in multiple consecutive pictures of Stream of Digital Pictures . Differences among the detected regions of the consecutive pictures may be identified in the mouth part of the person s face and determined to be continuous over a time period i.e. as opposed to a single random mouth movement etc. and diverse in variety i.e. as opposed to a smile etc. to indicate a speaking activity. Any technique for recognizing speech from mouth lip movements can be used in this and other examples. In further aspects Picture Recognizer may detect or recognize persons objects and or their activities using one or more artificial neural networks which may include statistical techniques. Examples of artificial neural networks that can be used in Picture Recognizer include convolutional neural networks CNNs time delay neural networks TDNNs deep neural networks and or others. In one example picture recognition techniques and or tools involving convolutional neural networks may include identifying and or analyzing tiled and or overlapping regions or features of a picture which may then be used to search for pictures with matching regions or features. In another example features of different convolutional neural networks responsible for spatial and temporal streams can be fused to detect persons objects and or their activities in motion pictures. In general Picture Recognizer may include any machine learning deep learning and or other artificial intelligence techniques. Any other techniques known in art can be utilized in Picture Recognizer . For example thresholds for similarity statistical and or optimization techniques can be utilized to determine a match in any of the above described detection or recognition techniques. Picture Recognizer comprises any features functionalities and embodiments of Substantial Similarity Comparison later described .

In some exemplary embodiments facial recognition techniques and or tools such as OpenCV Open Source Computer Vision library Animetrics FaceR API Lambda Labs Facial Recognition API Face SDK Neven Vision also known as N Vision Engine and or others can be utilized for detecting or recognizing faces in digital pictures. In some aspects facial recognition techniques and or tools involve identifying and or analyzing facial features such as the relative position size and or shape of the eyes nose cheekbones jaw etc. which may then be used to search for pictures with matching features. For example FaceR API can detect a person s face in Stream of Digital Pictures captured by Picture capturing Device or stored in an electronic repository which can then be utilized in Activity Detector Knowledge Structuring Unit and or other elements.

In other exemplary embodiments object recognition techniques and or tools such as OpenCV Open Source Computer Vision library CamFind API Kooaba 6px API Dextro API and or others can be utilized for detecting or recognizing objects i.e. objects animals people etc. in digital pictures. In some aspects object recognition techniques and or tools involve identifying and or analyzing object features such as lines edges ridges corners blobs regions and or their relative positions sizes shapes etc. which may then be used to search for pictures with matching features. For example OpenCV library can detect an object i.e. car pedestrian door building animal person etc. in Stream of Digital Pictures captured by Picture capturing Device or stored in an electronic repository which can then be utilized in Activity Detector Knowledge Structuring Unit and or other elements.

Speech Sound Recognizer comprises the functionality for detecting or recognizing speech or sounds in audio data. Speech Sound Recognizer comprises other disclosed functionalities. Audio data includes digital sound and or other audio data. Examples of file formats that can be utilized to store audio data include WAV WMA AIFF MP3 RA OGG and or other file formats. Speech Sound Recognizer may detect or recognize speech or sounds of a person in Stream of Digital Sound Samples . In some aspects Speech Sound Recognizer may detect or recognize a person s speech or sounds from Stream of Digital Sound Samples by comparing collections of sound samples from the Stream of Digital Sound Samples with collections of known sound samples. The collections of known sound samples can be learned or manually programmatically or otherwise defined. The collections of known sound samples can be stored in any data structure or repository i.e. database etc. that resides locally on System for Learning AIIMs or Computing Device or remotely on a remote computing device i.e. server cloud etc. accessible over a network. In other aspects Speech Sound Recognizer may detect or recognize a person s speech or sounds from Stream of Digital Sound Samples by comparing features from the Stream of Digital Sound Samples with features of known sounds. The features of known sounds can be learned or manually programmatically or otherwise defined. The features of known sounds can be stored in any data structure or repository i.e. database neural network etc. that resides locally on System for Learning AIIMs or Computing Device or remotely on a remote computing device i.e. server cloud etc. accessible over a network. Typical steps or elements in a feature oriented speech or sound recognition include pre processing feature extraction acoustic modeling language modeling and or others or a combination thereof each of which may include its own sub steps or sub elements depending on the application. Either or both acoustic modeling and or language modeling can be used in speech or sound recognition. Acoustic features can be used for classification of non verbal vocal outbursts such as laughter or sighs whereas linguistic features can be used to transcribe the linguistic message such as words phrases or sentences. In further aspects Speech Sound Recognizer may be implemented as speaker dependent or speaker independent. Speaker dependent speech or sound recognition may identify a speaker in addition to recognizing his her speech. In further aspects Speech Sound Recognizer may detect or recognize speeches or sounds of multiple persons from Stream of Digital Sound Samples using the aforementioned sound sample or feature comparisons using speaker dependent speech sound recognition and or using other detection or recognition techniques. In one example speaker dependent speech sound recognition may involve analyzing characteristics of a person s voice or speech thereby distinguishing it from other persons voices or speeches. A person s voice or speech characteristics needed for speaker dependent speech recognition can be learned automatically through the learning of conversations disclosed herein. For instance such automatic learning of a person s voice or speech characteristics can be accomplished in situations where the person s voice is completely or relatively isolated from other persons voices i.e. where conversation participants are remote from one another in videoconferencing etc. . In another example a person s voice or speech characteristics needed for speaker dependent speech recognition can be learned through a training session where a person may read text or isolated vocabulary and the system may analyze the person s voice or speech characteristics to fine tune the recognition of that person s speech. Speaker dependent speech recognition functionality can be used in embodiments where speeches of multiple Conversation Participants need to be detected or recognized in a single Stream of Digital Sound Samples . Speaker dependent speech recognition functionality can also be used in cases where Conversation Participants are in close proximity to one another i.e. in the same room etc. and where all of their voices can be detected by one or more Sound capturing Devices . In further aspects Speech Sound Recognizer may detect or recognize a variety of sounds from Stream of Digital Sound Samples using the aforementioned sound sample or feature comparisons and or other detection or recognition techniques. For example sound of wind door closing car passing and or other sounds can be detected and used as extra or contextual information i.e. Extra Info later described etc. in matching decision making and or other elements or functionalities herein. In further aspects Speech Sound Recognizer may be implemented as keyword spotting or as full speech recognition. Keyword spotting may attempt to find only a select group of words and or phrases and because of this limited lexicon consumes fewer resources. Full speech recognition may attempt to find all the words and or phrases and because of this broader lexicon consumes significant resources. In further aspects Speech Sound Recognizer may detect or recognize speech or sounds using Hidden Markov Models HMM Artificial Neural Networks Dynamic Time Warping DTW Gaussian Mixture Models GMM and or other models or techniques or a combination thereof. Some or all of these models or techniques may include statistical techniques. Examples of artificial neural networks that can be used in Speech Sound Recognizer include recurrent neural networks time delay neural networks TDNNs deep neural networks and or others. In general Speech Sound Recognizer may include any machine learning deep learning and or other artificial intelligence techniques. Any other techniques known in art can be utilized in Speech Sound Recognizer . For example thresholds for similarity statistical and or optimization techniques can be utilized to determine a match in any of the above described detection or recognition techniques. Speech Sound Recognizer comprises any features functionalities and embodiments of Substantial Similarity Comparison later described .

In some exemplary embodiments operating system s speech sound recognition functionalities such as iOS s Voice Services Siri and or others can be utilized in Speech Sound Recognizer . For example iOS Voice Services can detect speech sound in Stream of Digital Sound Samples captured by Sound capturing Device or stored in an electronic repository which can then be utilized in Activity Detector Knowledge Structuring Unit and or other elements.

In other exemplary embodiments Java Speech API JSAPI implementation such as The Cloud Garden Sphinx and or others can be utilized in Speech Sound Recognizer . For example Cloud Garden JSAPI can detect speech sound in Stream of Digital Sound Samples captured by Sound capturing Device or stored in an electronic repository which can then be utilized in Activity Detector Knowledge Structuring Unit and or other elements. Any other programming language s or platform s speech or sound processing API can similarly be utilized.

In further exemplary embodiments applications or engines providing speech sound recognition functionalities such as HTK Hidden Markov Model Toolkit Kaldi OpenEars Dragon Mobile Julius iSpeech CeedVocal and or others can be utilized in Speech Sound Recognizer . For example Kaldi SDK can detect speech sound in Stream of Digital Sound Samples captured by Sound capturing Device or stored in an electronic repository which can then be utilized in Activity Detector Knowledge Structuring Unit and or other elements.

Referring to an embodiment of Activity Detector extracting or filtering persons and or objects of interest is illustrated. This way learning of conversations can focus on Conversation Participants visual expressions or communication regardless of and across different visual backgrounds surrounding objects and or other insignificant content. In one example a picture of Sub stream of Digital Pictures may include Conversation Participant and background objects such as clouds sky and or other objects i.e. trees buildings vehicles etc. as shown. Activity Detector can detect Conversation Participant s face using Picture Recognizer i.e. facial recognition etc. and or other techniques. Once detected Activity Detector can change all pixels of the picture except for the region of pixels comprising Conversation Participant s face into a uniform color i.e. white blue gray etc. so that the region of pixels comprising Conversation Participant s face becomes prominent and Insignificant Content becomes suppressed or removed. Activity Detector can perform similar picture processing on other pictures in Sub stream of Digital Pictures . This way the processed Sub stream of Digital Pictures would include only Conversation Participant s face without Insignificant Content . In another example Activity Detector can extract the region of pixels comprising Conversation Participant s face from a picture of the Sub stream of Digital Pictures . The extracted region of pixels can then be stored back into the original picture replacing or overwriting all of its original pixels. The extracted region of pixels can alternatively be stored into a new picture a plurality of which may form a new Sub stream of Digital Pictures comprising Conversation Participant s face. Activity Detector can perform similar picture processing on other pictures in Sub stream of Digital Pictures . In some aspects Activity Detector can store the processed pictures into the Sub stream of Digital Pictures so that both original and processed pictures are available in separate channels or repositories within Sub stream of Digital Pictures . In addition to the previously described Picture Recognizer that itself includes segmentation functionalities any picture segmentation techniques can be utilized solely in part or in combination with other techniques in extracting or filtering persons and or objects of interest from pictures of Sub stream of Digital Pictures . Examples of picture segmentation techniques include thresholding clustering region growing edge detection curve propagation level sets graph partitioning model based segmentation trainable segmentation i.e. artificial neural networks etc. and or others. Extracting or filtering persons and or objects of interest can optionally be performed within another disclosed element i.e. Picture capturing Device etc. or by an additional element instead of within Activity Detector in alternate embodiments.

Referring to an embodiment of Activity Detector extracting or filtering speech and or sounds of interest is illustrated. This way learning of conversations can focus on Conversation Participants verbal expressions or communication regardless of and across different acoustic backgrounds ambient noises and or other insignificant sounds. As sound can be captured in various environments before or as part of the extraction or filtering Activity Detector can perform denoising of entire Sub stream of Digital Sound Samples . Noise may include any signal that degrades the quality of speech or sounds of interest such as equipment related noise electrical or electromagnetic noise or environmental noise. Examples of denoising techniques include Adaptive Wiener Filtering Spectral Subtraction Methods i.e. cepstral mean normalization etc. Spectral Restoration i.e. speech enhancement etc. Harmonic Decomposition Nonnegative Matrix Factorization NMF and or others. In one example Sub stream of Digital Sound Samples may include Conversation Participant s speech and ambient sounds such as sound of water waves laughter of nearby persons music and or other sounds as shown. Activity Detector can detect Conversation Participant s speech using Speech Sound Recognizer and or other techniques. Once detected Activity Detector can change all sound samples of Sub stream of Digital Sound Samples except for the sound samples comprising Conversation Participant s speech into silence so that Conversation Participant s speech becomes prominent and Insignificant Sounds become suppressed or removed. This way the processed Sub stream of Digital Sound Samples would include only speech of Conversation Participant without Insignificant Sounds . In some aspects Activity Detector can change i.e. reduce etc. the intensities of all sound samples of Sub stream of Digital Sound Samples except for the sound samples comprising Conversation Participant s speech so that Conversation Participant s speech becomes prominent and Insignificant Sounds become suppressed or removed. In further aspects since Conversation Participant s sounds may be highest in amplitude Activity Detector can reduce i.e. reduce to zero etc. the intensities of sound samples that are below a certain amplitude or intensity threshold so that Conversation Participant s speech becomes prominent and Insignificant Sounds become suppressed or removed. In yet some aspects since sounds can overlap in time Activity Detector can change the intensities of all sound samples of Sub stream of Digital Sound Samples including the sound samples comprising Conversation Participant s speech so that Conversation Participant s speech becomes prominent and Insignificant Sounds become suppressed or removed. In another example Activity Detector can extract sound samples comprising Conversation Participant s speech from Sub stream of Digital Sound Samples . The extracted sound samples can then be stored back into Sub stream of Digital Sound Samples replacing all of its original sound samples. The extracted sound samples can alternatively be stored into a new Sub stream of Digital Sound Samples comprising Conversation Participant s speech. In some aspects Activity Detector can store the extracted sound samples into Sub stream of Digital Sound Samples so that both original and extracted sound samples are available in separate channels or repositories within Sub stream of Digital Sound Samples . In addition to the previously described Speech Sound Recognizer that itself includes segmentation functionalities any speech or sound segmentation techniques can be utilized solely in part or in combination with other techniques in extracting or filtering speech and or sounds of interest from Sub stream of Digital Sound Samples . Examples of speech or sound segmentation techniques include whole word models subword models decomposition models phonotactic models any of the aforementioned denoising techniques and or others. Extracting or filtering speech and or sounds of interest can optionally be performed within another disclosed element i.e. Sound capturing Device etc. or by an additional element instead of within Activity Detector in alternate embodiments.

Referring to an embodiment of Knowledge Structuring Unit is illustrated. Knowledge Structuring Unit comprises the functionality for structuring the knowledge of one or more conversations and or other functionalities. Knowledge Structuring Unit comprises the functionality for correlating Conversational Activities . Knowledge Structuring Unit comprises the functionality for creating or generating a Round of Conversational Exchange and storing one or more Conversational Activities and or other data i.e. Extra Info later described etc. into the Round of Conversational Exchange . As such Round of Conversational Exchange also referred to as Round of Cony etc. comprises the functionality for storing one or more Conversational Activities and or other data. Once created or generated Rounds of Conversational Exchange can be used in as neurons nodes vertices or other elements in any of the knowledge or data structures arrangements i.e. neural networks graphs sequences etc. used for storing the knowledge of conversations and facilitating learning functionalities herein.

In some embodiments Knowledge Structuring Unit may receive one or more Conversational Activities of Conversation Participants and from Activity Detectors and respectively. Knowledge Structuring Unit may then correlate the one or more Conversational Activities of Conversation Participant with the one or more Conversational Activities of Conversation Participant and store the correlated Conversational Activities into a Round of Conversational Exchange as shown. In effect Round of Conversational Exchange includes a unit of knowledge i.e. correlated Conversational Activities etc. of how one Conversation Participant acted relative to another Conversation Participant and vice versa in a part of a conversation. When Conversational Activities with similar content structure and or other properties are detected involving a user in the future the learned Conversational Activities of one or more Conversation Participants stored in Rounds of Conversational Exchange can be anticipated thereby simulating a conversation with one or more AI Conversation Participants as later described. In one example Conversation Participant may speak while Conversation Participant observes i.e. silent facial expressions silent body movements motionless silence etc. in a particular part of a conversation therefore a Round of Conversational Exchange may include Conversation Participant s speaking Conversational Activity correlated with Conversation Participant s silent facial expressions Conversational Activity . In another example both Conversation Participants and may observe in a particular part of a conversation therefore a Round of Conversational Exchange may include Conversation Participant s silent body movements Conversational Activity correlated with Conversation Participant s silent facial expressions Conversational Activity . In a further example both Conversation Participants and may speak in a particular part of a conversation therefore a Round of Conversational Exchange may include Conversation Participant s speaking Conversational Activity correlated with Conversation Participant s speaking Conversational Activity .

Referring to another embodiment of Knowledge Structuring Unit is illustrated. In some aspects the timing of Conversational Activities of different Conversation Participants may coincide. In other aspects the timing of Conversational Activities of different Conversation Participants can partially coincide or overlap. In further aspects the number of Conversational Activities of one Conversation Participant may equal the number of Conversational Activities of another Conversation Participant . In further aspects the number of Conversational Activities of one Conversation Participant can differ from the number of Conversational Activities of another Conversation Participant . In general a Round of Conversational Exchange may include any number types timing and or other properties of Conversational Activities of any number of Conversation Participants arranged in any conceivable combination.

Referring to an exemplary embodiment of Round of Conversational Exchange comprising one Conversational Activity of Conversation Participant and one Conversational Activity of Conversation Participant that temporally coincide i.e. Conversational Activities of both Conversation Participants may start and end at the same time etc. is illustrated. For example speaking Conversational Activity of Conversation Participant correlated with silent body movements Conversational Activity of Conversation Participant can be stored in a Round of Conversational Exchange .

Referring to an exemplary embodiment of Round of Conversational Exchange comprising one Conversational Activity of Conversation Participant and one Conversational Activity of Conversation Participant that temporally partially coincide is illustrated.

Referring to an exemplary embodiment of Round of Conversational Exchange comprising two Conversational Activities of Conversation Participant and two Conversational Activities of Conversation Participant that temporally coincide i.e. Conversational Activities of both Conversation Participants may start and end at same times etc. is illustrated. For example speaking Conversational Activity of Conversation Participant correlated with silent facial expressions Conversational Activity of Conversation Participant and subsequent motionless silence Conversational Activity of Conversation Participant correlated with subsequent speaking Conversational Activity of Conversation Participant can be stored in a Round of Conversational Exchange .

Referring to an exemplary embodiment of Round of Conversational Exchange comprising two Conversational Activities of Conversation Participant and two Conversational Activities of Conversation Participant that temporally partially coincide is illustrated.

Referring to an exemplary embodiment of Round of Conversational Exchange comprising two or more Conversational Activities of Conversation Participant and two or more Conversational Activities of Conversation Participant is illustrated. Some of the Conversational Activities may temporally coincide as shown. In one example two Conversational Activities of Conversation Participant correlated with three Conversational Activities of Conversation Participant can be stored in a Round of Conversational Exchange . Specifically in this example speaking and silent body movements Conversational Activities of Conversation Participant correlated with silent facial expressions silent body movements and speaking Conversational Activities of Conversation Participant can be stored in a Round of Conversational Exchange . In another example two Conversational Activities of Conversation Participant correlated with one Conversational Activity of Conversation Participant can be stored in a Round of Conversational Exchange .

Referring to another exemplary embodiment of Round of Conversational Exchange comprising two or more Conversational Activities of Conversation Participant and two or more Conversational Activities of Conversation Participant is illustrated. Some of the Conversational Activities may temporally partially coincide as shown.

Referring to exemplary embodiments of Round of Conversational Exchange comprising one Conversational Activity of Conversation Participant or is illustrated. Such Rounds of Conversational Exchange can be used to store some Conversational Activities of Conversation Participants and while omitting others. For example speaking Conversational Activities of Conversation Participants and can be stored in Rounds of Conversational Exchange while observing Conversational Activities i.e. silent facial expressions silent body movements motionless silence etc. can be omitted. In some aspects Rounds of Conversational Exchange comprising a single Conversational Activity may themselves be correlated or interconnected indicating their relationships.

Referring to an exemplary embodiment of Round of Conversational Exchange comprising one Conversational Activity of Conversation Participant and one Conversational Activity of Conversation Participant that temporally extend i.e. Conversational Activity of one Conversation Participant starts where Conversational Activity of another Conversation Participant ends etc. one another is illustrated. Such Rounds of Conversational Exchange can be used to store some Conversational Activities of Conversation Participants and while omitting others. For example speaking Conversational Activities of Conversation Participants and can be stored in a Round of Conversational Exchange while observing Conversational Activities i.e. silent facial expressions silent body movements motionless silence etc. can be omitted.

Referring to an exemplary embodiment of Round of Conversational Exchange comprising one Conversational Activity of Conversation Participant and one Conversational Activity of Conversation Participant that temporally extend one another and overlap is illustrated.

Referring to an exemplary embodiment of Round of Conversational Exchange comprising one Conversational Activity of Conversation Participant one Conversational Activity of Conversation Participant and one Conversational Activity of Conversation Participant not shown that temporally coincide i.e. Conversational Activities of all Conversation Participants may start and end at the same time etc. is illustrated. Round of Conversational Exchange may include Conversational Activities of any number of Conversation Participants . For example speaking Conversational Activity of Conversation Participant correlated with silent body movements Conversational Activity of Conversation Participant and silent facial expressions Conversational Activity of Conversation Participant can be stored in a Round of Conversational Exchange . Any of the previously described partially coinciding overlapping and or extending Conversational Activities can similarly be used in Round of Conversational Exchange comprising Conversational Activities of more than two Conversation Participants . Round of Conversational Exchange comprising Conversational Activities of more than two Conversation Participants can be used to learn conversations among more than two persons. As such Round of Conversational Exchange comprising Conversational Activities of more than two Conversation Participants may include a unit of knowledge i.e. correlated Conversational Activities etc. of how multiple Conversation Participants acted relative to other Conversation Participants in a part of a conversation.

One of ordinary skill in art will understand that Rounds of Conversational Exchange in the preceding figures are described merely as examples of a variety of possible implementations and that any number or arrangement of Conversational Activities can be used in a Round of Conversational Exchange in alternate embodiments. Also it should be understood that the various data structures such as Conversational Activity Round of Conversational Exchange and or others are used to organize the disclosed elements in particular embodiments and that other additional data structures can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments. In one example Rounds of Conversational Exchange as containers for Conversational Activities can be omitted in which case Conversational Activities can be stored directly into nodes of neural network graph sequence and or other knowledge or data structure. In another example Conversational Activities as containers for Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples can be omitted in which case Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples can be stored directly into Rounds of Conversational Exchange or into nodes of neural network graph sequence and or other knowledge or data structure.

Referring to embodiments of Extra Information also referred to as Extra Info are illustrated. Extra Info comprises the functionality for storing any information useful in simulating AI Conversation Participant later described and or other functionalities. In one example Extra Info can be stored in or associated with a Sub stream of Digital Pictures or Sub stream of Digital Sound Samples as shown in . In another example Extra Info can be stored in or associated with Conversational Activity as shown in . In a further example Extra Info can be stored in or associated with a Round of Conversational Exchange as shown in . In general Extra Info related to any other element can similarly be stored in or associated with that element. In some embodiments Knowledge Structuring Unit can obtain and or store Extra Info in its creation of Round of Conversational Exchange . In other embodiments Activity Detector can obtain and or store Extra Info in its creation of Conversational Activity . Examples of Extra Info include time information location information observed information contextual information and or other information. Any information can be utilized that can provide additional information for enhanced simulation of AI Conversation Participant later described . Which information is stored in Extra Info can be set by a user by AIIM system administrator or automatically by the system. Extra Info may include or be referred to as contextual information and vice versa. Therefore these terms may be used interchangeably herein. In some aspects time information i.e. time stamp etc. stored in Extra Info can be useful in anticipating AI Conversation Participant s Conversational Activities related to a specific time period as people sometimes talk specific topics at certain parts of day month year and or other time periods. Time information can be obtained from the system clock or other time source. In one example people may speak about how nice or bad a morning is early in the day and how sleepy or tired they are late in the day. In another example people may speak about how expensive the monthly mortgage payment is in the beginning of the month. In a further example people may speak about sun and sunny days in the summer about falling leaves and rain in the fall and about snow and ice in the winter. In a further example people may speak about specific holiday topics on particular dates of the year. In other aspects location information i.e. latitude longitude altitude coordinates address etc. stored in Extra Info can be useful in anticipating AI Conversation Participant s Conversational Activities related to a specific place as people sometimes talk about specific topics at certain places i.e. frequently visited or other places etc. . Location information can be obtained from a positioning system i.e. radio signal triangulation in smartphones or tablets GPS capabilities in some high end digital cameras etc. if one is available. For example people may speak about school related topics at school work related topics at work religious topics at a house of worship and or other topics related to other places. In further aspects observed information stored in Extra Info can be useful in anticipating AI Conversation Participant s Conversational Activities related to a specific object or environment as people sometimes talk about proximal objects or environments. An object or environment can be recognized by processing Sub stream of Digital Pictures and or Sub stream of Digital Sound Samples . For example the system may recognize a specific object or environment such as library park beach gym and or others in Sub stream of Digital Pictures and or in Sub stream of Digital Sound Samples . Any features functionalities and embodiments of Picture Recognizer and or Speech Sound Recognizer can be utilized for such recognizing. For example book shelves recognized in the background of Sub stream of Digital Pictures may indicate a library or book store trees recognized in the background of Sub stream of Digital Pictures may indicate a park sound of waves recognized in Sub stream of Digital Sound Samples may indicate a beach and or others.

Referring to an embodiment of utilizing System for Learning AIIMs implemented as a network service is illustrated.

Referring to an embodiment of internal structure of System for Learning AIIMs implemented as a network service is illustrated. System for Learning AIIMs comprises interconnected Knowledge Structuring Unit Knowledgebase and Activity Detectors and . Some embodiments of Activity Detector may include Picture Recognizer and Speech Sound Recognizer whereas some embodiments of Activity Detector may include Picture Recognizer and Speech Sound Recognizer . System for Learning AIIMs or any element thereof may reside or operate on Server i.e. also referred to as remote computing device etc. which is accessible by User Devices and over Network . User Devices and comprise Picture capturing Devices and respectively and Sound capturing Devices and respectively. Server and User Devices and may include any features functionalities and embodiments of the previously described Computing Device . Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments.

In some embodiments System for Learning AIIMs can learn conversations among Conversation Participants in a video call. A video call may be facilitated by services or applications such as Apple FaceTime Google Hangouts Skype Viber and or other video call services or applications. Such services commonly use one or more Servers accessible over Network i.e. Internet intranet etc. to facilitate video calls for their users. Server may be or include any type or form of a remote computing device such as an application server a network service server a cloud server a cloud and or other remote computing device. In addition to traditional networks i.e. Internet intranet etc. Network may include any type or form of direct connection among User Devices such as wired or wireless i.e. Bluetooth WiFi etc. direct connection. Two or more Conversation Participants may engage in a video call using their respective User Devices . User Device comprises any computing mobile telecommunication electronic and or other device that can facilitate AIIM functionalities. Examples of User Device include a smartphone a personal computer a mobile computer i.e. tablet laptop etc. and or others. User Device may include any features functionalities and embodiments of the previously described Computing Device . Most modern User Devices comprise Picture capturing Device i.e. built in camera etc. and Sound capturing Device i.e. built in microphone etc. . It should be understood that Server does not have to be a separate or central computing device in between User Devices and and that Server or portions thereof can be implemented on one or more of User Devices and or . In some designs Server may indeed be omitted in which case all of its elements and functionalities would be distributed or performed on User Devices and or . In such implementations Knowledge Structuring Unit Knowledgebase Activity Detectors and and or other elements of System for Learning AIIMs can reside on User Devices and or depending on design.

In other embodiments System for Learning AIIMs can learn conversations among Conversation Participants in a text messaging or textual chat. System for Learning AIIMs may include a text input device i.e. keyboard keypad touch screen etc. instead of or in addition to Picture capturing Device and or Sound capturing Device . The text input device can be used for exchanging textual expressions or communication among Conversation Participants . In such implementations System for Learning AIIMs can learn textual expressions or communication instead of visual and or verbal expressions or communication. For example Conversation Participants may engage in text messaging or textual chat using their respective User Devices . Text messaging or textual chat may be facilitated by services or applications such as Apple iMessage Google Messenger Skype Instant Messaging Textra SMS IRC and or others. Such services commonly use one or more Servers accessible over Network i.e. Internet intranet etc. to facilitate text messaging or textual chat for their users although text messaging or textual chat can be peer to peer without a server. As Conversation Participants exchange textual expressions or communication System for Learning AIIMs can learn the textual conversations among the Conversation Participants as described herein with respect to learning visual and verbal expressions or communication. Specifically Conversational Activity would include text instead of Stream of Digital Pictures and or Stream of Digital Sound Samples . In some aspects learning textual expressions or communication may be easier to implement since it is easier or even apparent to detect conversational activities in textual communication i.e. a conversational activity may include a single or group of text messages etc. . Also less processing is required for text relative to visual and verbal data. Stream of Digital Pictures Stream of Digital Sound Samples some parts of Activity Detector and or other elements can be optionally omitted in System for Learning AIIMs that learns textual conversations.

Referring to an embodiment of utilizing System for Learning AIIMs embedded in Host Device is illustrated.

Referring to an embodiment of internal structure of System for Learning AIIMs embedded in Host Device is illustrated. System for Learning AIIMs comprises interconnected Knowledge Structuring Unit Knowledgebase and Activity Detector . Some embodiments of Activity Detector may include Picture Recognizer and Speech Sound Recognizer . System for Learning AIIMs may reside or operate on Host Device . System for Learning AIIMs may utilize Host Device s Picture capturing Device and Sound capturing Device to implement its functionalities. Host Device may include any features functionalities and embodiments of the previously described Computing Device . Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments.

In some aspects System for Learning AIIMs can learn conversations among Conversation Participants engaged in a conversation near Host Device . Host Device comprises any computing mobile telecommunication electronic and or other device that can facilitate AIIM functionalities. Examples of Host Devices include a television device a set top box a disc or other media player i.e. DVD or Blue ray player etc. a gaming device i.e. Microsoft Xbox Sony PlayStation etc. a smartphone i.e. Apple iPhone Samsung Galaxy etc. a mobile computer i.e. tablet or laptop computer etc. a still or motion picture camera and or others. Host Device may include one or more Picture capturing Devices i.e. built in cameras etc. and Sound capturing Devices i.e. built in microphones etc. . In the case of more than one Picture capturing Device and Sound capturing Device each Conversation Participant may have a dedicated Picture capturing Device and Sound capturing Device and System for Learning AIIMs may operate as previously described. In the case of a single Picture capturing Device and a single Sound capturing Device Activity Detector can detect a plurality i.e. one for each Conversation Participant etc. of parallel Sub streams of Digital Pictures from a single Stream of Digital Pictures captured by the single Picture capturing Device . Similarly Activity Detector can detect a plurality i.e. one for each Conversation Participant etc. of parallel Sub streams of Digital Sound Samples from a single Stream of Digital Sound Samples captured by the single Sound capturing Device . A parallel Sub stream of Digital Pictures may include visual expressions or communication of one of the Conversation Participants in the single Stream of Digital Pictures . A parallel Sub stream of Digital Sound Samples may include verbal expressions or communication of one of the Conversation Participants in the single Stream of Digital Sound Samples . Activity Detector can utilize the previously described functionalities of Picture Recognizer i.e. facial recognition etc. to recognize and or track multiple Conversation Participants in a single Stream of Digital Pictures . Similarly Activity Detector can utilize the previously described functionalities of Speech Sound Recognizer i.e. speaker dedicated recognition etc. to recognize and or track multiple Conversation Participants speeches or sounds in a single Stream of Digital Sound Samples .

In some designs instead of being captured by Picture capturing Device in real time the single Stream of Digital Pictures may be or include any stored stream of digital pictures captured by any picture capturing device. Similarly instead of being captured by Sound capturing Device in real time the single Stream of Digital Sound Samples may be or include any stored stream of digital sound samples captured by any sound capturing device. As such Activity Detector can detect a plurality i.e. one for each Conversation Participant etc. of parallel Sub streams of Digital Pictures from a single stored Stream of Digital Pictures and a plurality i.e. one for each Conversation Participant etc. of parallel Sub streams of Digital Sound Samples from a single stored Stream of Digital Sound Samples . The single stored Stream of Digital Pictures and or the single stored Stream of Digital Sound Samples may commonly be included in a file i.e. video file etc. comprising the Stream of Digital Pictures and or the Stream of Digital Sound Samples . Examples of such files include family videos recorded video or phone conversations news interviews video databases i.e. Youtube Yahoo Video Search Google Videos etc. and or any other material comprising a recording of a conversation among two or more conversation participants. In one example System for Learning AIIMs can learn conversations from one or more family videos selected by a user. In another example System for Learning AIIMs can traverse Youtube videos to learn conversations involving a particular person. The videos involving the person may be found by searching for the person s name or other information. In a further example System for Learning AIIMs can traverse Youtube videos to learn conversations involving some or all persons depicted in some or all Youtube videos.

Referring to the teaching presented by the disclosure can be implemented to include various artificial intelligence models and or techniques. The disclosed devices systems and methods for learning and using AIIMs are independent of the artificial intelligence model and or technique used and any model and or technique can be utilized to facilitate the functionalities described herein. Examples of these models and or techniques include deep learning supervised learning unsupervised learning neural networks i.e. convolutional neural network recurrent neural network deep neural network etc. search based logic and or fuzzy logic based optimization based tree graph other data structure based hierarchical symbolic and or sub symbolic evolutionary genetic multi agent deterministic probabilistic statistical and or other models and or techniques.

In one example shown in Model A the disclosed devices systems and methods for learning and using AIIMs may include a neural network also referred to as artificial neural network etc. . As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities may include a network of Nodes also referred to as neurons in the context of neural networks etc. and Connections similar to that of a brain. Node can store any data object data structure and or other item or reference thereto. Node may also include a function for transforming or manipulating any data object data structure and or other item. Examples of such transformation functions include mathematical functions i.e. addition subtraction multiplication division sin cos log derivative integral etc. object manipulation functions i.e. creating an object modifying an object deleting an object appending objects etc. data structure manipulation functions i.e. creating a data structure modifying a data structure deleting a data structure creating a data field modifying a data field deleting a data field etc. and or other transformation functions. Connection can store or be associated with a value such as a symbolic label or numeric attribute i.e. weight cost capacity length etc. . A neural network can be utilized as a predictive modeling approach in machine learning. A computational model can be utilized to compute values from inputs based on a pre programmed or learned function or method. For example a neural network may include one or more input neurons that can be activated by inputs. Activations of these neurons can then be passed on weighted and transformed by a function to other neurons. Neural networks may range from those with only one layer of single direction logic to multi layer of multi directional feedback loops. A neural network can use weights to change the parameters of the network s throughput. A neural network can learn by input from its environment or from self teaching using written in rules. An exemplary embodiment of a neural network i.e. Neural Network etc. is described later.

In another example shown in Model B the disclosed devices systems and methods for learning and using AIIMs may include a graph or graph like data structure. As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities may include Nodes i.e. vertices points etc. and Connections i.e. edges arrows lines arcs etc. organized as a graph. A graph can be utilized as a predictive modeling approach in machine learning. In general any Node in a graph can be connected to any other Node . A Connection may include unordered pair of Nodes in an undirected graph or ordered pair of Nodes in a directed graph. Nodes can be part of the graph structure or external entities represented by indices or references. Nodes Connections and or operations of a graph may include any features functionalities and embodiments of the aforementioned Nodes Connections and or operations of a neural network and vice versa. An exemplary embodiment of a graph i.e. Graph etc. is described later.

In a further example shown in Model C the disclosed devices systems and methods for learning and using AIIMs may include a tree or tree like structure. As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities may include Nodes and Connections i.e. references edges etc. organized as a tree. A tree can be utilized as a predictive modeling approach in machine learning. In general a Node in a tree can be connected to any number i.e. including zero etc. of children Nodes i.e. similar to a tree etc. . In some aspects a collection of trees can be utilized where each tree may represent a set of related conversational paths such as for example paths concerning a topic or concept. Nodes Connections and or operations of a tree may include any features functionalities and embodiments of the aforementioned Nodes Connections and or operations of a neural network and or graph and vice versa.

In a further example shown in Model D the disclosed devices systems and methods for learning and using AIIMs may include a sequence or sequence like structure. As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities may include a structure of Nodes and Connections organized as a sequence. In some aspects Connections may be optionally omitted from a sequence. A sequence can be utilized as a predictive modeling approach in machine learning. In some aspects a sequence can be used to store a single conversation. In other aspects a sequence can be used to store multiple concatenated conversations. Nodes Connections and or operations of a sequence may include any features functionalities and embodiments of the aforementioned Nodes Connections and or operations of a neural network graph and or tree and vice versa. An exemplary embodiment of a sequence i.e. Collection of Sequences Sequence etc. is described later.

In yet another example the disclosed devices systems and methods for learning and using AIIMs may include a search based model and or technique. As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities may include searching through a collection of possible solutions. For example a search method can search through a neural network graph tree list or other data structure that includes data elements of interest. A search may use heuristics to limit the search for solutions by eliminating choices that are unlikely to lead to the goal. Heuristic techniques may provide a best guess solution. A search can also include optimization. For example a search may begin with a guess and then refine the guess incrementally until no more refinements can be made. In a further example the disclosed devices systems and methods for learning and using AIIMs may include logic based model and or technique. As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities can use formal or other type of logic. Logic based models may involve making inferences or deriving conclusions from a set of premises. As such a logic based system can extend existing knowledge or create new knowledge automatically using inferences. Examples of the types of logic that can be utilized include propositional or sentential logic that comprises logic of statements which can be true or false first order logic that allows the use of quantifiers and predicates and that can express facts about objects their properties and their relations with each other fuzzy logic that allows degrees of truth to be represented as a value between 0 and 1 rather than simply 0 false or 1 true which can be used for uncertain reasoning subjective logic that comprises a type of probabilistic logic that may take uncertainty and belief into account which can be suitable for modeling and analyzing situations involving uncertainty incomplete knowledge and different world views and or other types of logic. In a further example the disclosed devices systems and methods for learning and using AIIMs may include a probabilistic model and or technique. As such machine learning knowledge representation or structure pattern recognition decision making and or other artificial intelligence functionalities can be implemented to operate with incomplete or uncertain information where probabilities may affect outcomes. Bayesian network among other models is an example of a probabilistic tool used for purposes such as reasoning learning planning perception and or others. One of ordinary skill in art will understand that the aforementioned artificial intelligence models and or techniques are described merely as examples of a variety of possible implementations and that while all possible artificial intelligence models and or techniques are too voluminous to describe other artificial intelligence models and or techniques known in art are within the scope of this disclosure. One of ordinary skill in art will also recognize that an intelligent system may solve a specific problem by using any model and or technique that works such as for example some systems can be symbolic and logical some can be sub symbolic neural networks some can be deterministic or probabilistic some can be hierarchical some may include searching techniques some may include optimization techniques while others may use other or a combination of models and or techniques. In general any artificial intelligence model and or technique can be utilized that can support AIIM functionalities.

Referring to exemplary embodiments of interconnected Rounds of Conversational Exchange and updating weights of Connections are illustrated. As shown for example in Round of Conversational Exchange is connected to Round of Conversational Exchange and Round of Conversational Exchange by Connection and Connection respectively. Each of Connection and Connection may include or be associated with occurrence count weight any parameter and or other data. The number of occurrences may track or store the number of observations that a Round of Conversational Exchange was followed by another Round of Conversational Exchange indicating a connection or relationship between them. For example Round of Conversational Exchange was followed by Round of Conversational Exchange 10 times as indicated by the number of occurrences of Connection . Also Round of Conversational Exchange was followed by Round of Conversational Exchange 15 times as indicated by the number of occurrences of Connection . The weight of Connection can be calculated or determined as the number of occurrences of Connection divided by the sum of occurrences of all connections i.e. Connection and Connection etc. originating from Round of Conversational Exchange . Therefore the weight of Connection can be calculated or determined as 10 10 15 0.4 for example. Also the weight of Connection can be calculated or determined as 15 10 15 0.6 for example. Therefore the sum of weights of Connection Connection and or any other Connections originating from Round of Conversational Exchange may equal to 1 or 100 . As shown for example in in the case that Round of Conversational Exchange is inserted and an observation is made that Round of Conversational Exchange follows Round of Conversational Exchange Connection can be created between Round of Conversational Exchange and Round of Conversational Exchange . The occurrence count of Connection can be set to 1 and weight determined as 1 10 15 1 0.038. The weights of al other connections i.e. Connection Connection etc. originating from Round of Conversational Exchange may be updated to account for the creation of Connection . Therefore the weight of Connection can be updated as 10 10 15 1 0.385. The weight of Connection can also be updated as 15 10 15 1 0.577. As shown for example in in the case that an additional occurrence of Connection is observed i.e. Round of Conversational Exchange followed Round of Conversational Exchange etc. occurrence count of Connection and weights of all connections i.e. Connection Connection and Connection etc. originating from Round of Conversational Exchange may be updated to account for this observation. The occurrence count of Connection can be increased by 1 and its weight updated as 11 11 15 1 0.407. The weight of Connection can also be updated as 15 11 15 1 0.556. The weight of Connection can also be updated as 1 11 15 1 0.037.

In some embodiments Connection may connect not only Rounds of Conversational Exchange but also Conversational Activities and or other elements. For example a Conversational Activity in one Round of Conversational Exchange may be connected by Connection to a Conversational Activity in another Round of Conversational Exchange . In general Connections can connect any Rounds of Conversational Exchange any Conversational Activities and or other elements.

Referring to an exemplary embodiment of learning Rounds of Conversational Exchange using Neural Network is illustrated. Neural Network includes a number of neurons or Nodes interconnected by Connections as previously described. Rounds of Conversational Exchange are shown instead of Nodes to simplify the illustration as Node includes a Round of Conversational Exchange for example. Therefore Rounds of Conversational Exchange and Nodes can be used interchangeably herein depending on context. It should be noted that Node may include other elements and or functionalities instead of or in addition to Round of Conversational Exchange . Rounds of Conversational Exchange may be applied onto Neural Network individually or collectively in a learning or training process. In some designs Neural Network comprises a number of Layers each of which may include one or more Rounds of Conversational Exchange . Rounds of Conversational Exchange in successive Layers can be connected by Connections . Connection may include or be associated with occurrence count weight any parameter and or other data as previously described. Neural Network may include any number of Layers to accommodate conversations comprising any number of Rounds of Conversational Exchange . In effect Neural Network may store Rounds of Conversational Exchange interconnected by Connections where following a path through the Neural Network can later be used to simulate a conversation. It should be understood that in some embodiments Rounds of Conversational Exchange in one Layer of Neural Network need not be connected only with Rounds of Conversational Exchange in a successive Layer but also in any other Layer thereby creating shortcuts i.e. shortcut Connections etc. through Neural Network . A Round of Conversational Exchange can also be connected to itself such as for example in recurrent neural networks. In general any Round of Conversational Exchange can be connected with any other Round of Conversational Exchange anywhere else in Neural Network . In further embodiments back propagation of any data or information can be implemented. In one example back propagation of similarity i.e. similarity index etc. of compared Rounds of Conversational Exchange in a path through Neural Network can be implemented. In another example back propagation of errors can be implemented. Such back propagations can then be used to adjust occurrence counts and or weights of Connections for better future predictions for example. Any other back propagation can be implemented for other purposes. Any combination of Nodes i.e. Nodes comprising Round of Conversational Exchange etc. Connections Layers and or other elements or techniques can be implemented in alternate embodiments. Neural Network may include any type or form of a neural network known in art such as a feed forward neural network a back propagating neural network a recurrent neural network a convolutional neural network deep neural network and or others including a custom neural network.

In some embodiments Knowledge Structuring Unit creates or generates Rounds of Conversational Exchange and the system applies them onto Neural Network thereby implementing learning of Rounds of Conversational Exchange . The term apply or applying may refer to storing copying inserting updating or other similar action therefore these terms may be used interchangeably herein depending on context. The system can perform Substantial Similarity Comparisons later described of a Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding Layer of Neural Network . If a substantially similar Round of Conversational Exchange is not found in the corresponding Layer of Neural Network the system may insert i.e. copy store etc. the Round of Conversational Exchange from Knowledge Structuring Unit into the corresponding Layer of Neural Network and create a Connection to the inserted Round of Conversational Exchange from a Round of Conversational Exchange in a prior Layer including assigning an occurrence count to the new Connection calculating a weight of the new Connection and updating any other Connections originating from the Round of Conversational Exchange in the prior Layer . On the other hand if a substantially similar Round of Conversational Exchange is found in the corresponding Layer of Neural Network the system may update occurrence count and weight of Connection to that Round of Conversational Exchange from a Round of Conversational Exchange in a prior Layer and update any other Connections originating from the Round of Conversational Exchange in the prior Layer .

For example the system can perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding Layer of Neural Network . In the case that a substantially similar match is found between Round of Conversational Exchange and Round of Conversational Exchange the system may perform no action since Round of Conversational Exchange is the initial Round of Conversational Exchange . The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding Layer of Neural Network . In the case that a substantially similar match is found between Round of Conversational Exchange and Round of Conversational Exchange the system may update occurrence count and weight of Connection between Round of Conversational Exchange and Round of Conversational Exchange and update weights of other Connections originating from Round of Conversational Exchange as previously described. The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding Layer of Neural Network . In the case that a substantially similar match is not found the system may insert Round of Conversational Exchange into Layer and copy Round of Conversational Exchange into the inserted Round of Conversational Exchange . The system may also create Connection between Round of Conversational Exchange and Round of Conversational Exchange with occurrence count of 1 and weight calculated based on the occurrence count as previously described. The system may also update weights of other Connections one in this example originating from Round of Conversational Exchange as previously described. The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding Layer of Neural Network . In the case that a substantially similar match is not found the system may insert Round of Conversational Exchange into Layer and copy Round of Conversational Exchange into the inserted Round of Conversational Exchange . The system may also create Connection between Round of Conversational Exchange and Round of Conversational Exchange with occurrence count of 1 and weight of 1. The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding Layer of Neural Network . In the case that a substantially similar match is not found the system may insert Round of Conversational Exchange into Layer and copy Round of Conversational Exchange into the inserted Round of Conversational Exchange . The system may also create Connection between Round of Conversational Exchange and Round of Conversational Exchange with occurrence count of 1 and weight of 1. Applying any additional Rounds of Conversational Exchange from Knowledge Structuring Unit onto Neural Network follows similar logic or process as the above described.

Substantial Similarity Comparison also referred to simply as similarity comparison or other similar reference comprises the functionality for comparing or matching Rounds of Conversational Exchange or portions thereof and or other functionalities. Substantial Similarity Comparison comprises the functionality for comparing or matching Conversational Activities or portions thereof. Substantial Similarity Comparison comprises the functionality for comparing or matching Sub streams of Digital Pictures or portions thereof. Substantial Similarity Comparison comprises the functionality for comparing or matching Sub streams of Digital Sound Samples or portions thereof. Substantial Similarity Comparison comprises the functionality for comparing or matching text i.e. characters words phrases etc. pictures sounds data and or other elements or portions thereof. Substantial Similarity Comparison may include functions rules and or logic for performing matching or comparisons and for determining that while a perfect match is not found a similar or substantially similar match has been found. While substantial similarity may imply a substantial level of similarity substantial similarity may also depending on context include any similarity however high or low as defined by the rules for substantial similarity. The rules for substantial similarity or substantially similar match can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input. In some designs Substantial Similarity Comparison comprises the functionality to automatically define appropriately strict rules for determining substantial similarity of the compared elements. Substantial Similarity Comparison can therefore set reset and or adjust the strictness of the rules for finding or determining substantial similarity between the compared elements thereby fine tuning Substantial Similarity Comparison so that the rules for determining substantial similarity are appropriately strict. In some aspects Substantial Similarity Comparison can determine substantial similarity of compared elements if their similarity exceeds a threshold i.e. similarity threshold etc. . In other aspects Substantial Similarity Comparison can determine substantial similarity of compared elements if their difference is lower than a threshold i.e. difference threshold etc. .

In determining substantial similarity of Rounds of Conversational Exchange Substantial Similarity Comparison may compare one or more Conversational Activities or portions thereof of one Round of Conversational Exchange with one or more Conversational Activities or portions thereof of another Round of Conversational Exchange . In some embodiments total equivalence is achieved when al Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange match. If total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In some aspects substantial similarity can be achieved when most of the Conversational Activities or portions i.e. Sub streams of Digital Pictures Sub streams of Digital Sound Samples etc. thereof of the compared Rounds of Conversational Exchange match or substantially match. In other aspects substantial similarity can be achieved when at least a threshold number or percentage of Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange match or substantially match. Similarly substantial similarity can be achieved when a number or percentage of matching or substantially matching Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange exceeds a threshold. In further aspects substantial similarity can be achieved when all but a threshold number or percentage of Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange match or substantially match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input. In one example substantial similarity can be achieved when at least 1 2 3 4 or any other threshold number of Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange match or substantially match. Similarly substantial similarity can be achieved when the number of matching or substantially matching Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange exceeds 1 2 3 4 or any other threshold number. In another example substantial similarity can be achieved when at least 10 21 30 49 66 89 93 or any other percentage of Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange match or substantially match. Similarly substantial similarity can be achieved when the percentage of matching or substantially matching Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange exceeds 10 21 30 49 66 89 93 or any other threshold percentage. In other embodiments substantial similarity of the compared Rounds of Conversational Exchange can be achieved in terms of matches or substantial matches in more important i.e. as indicated by importance index later described etc. Conversational Activities or portions thereof thereby tolerating mismatches in less important Conversational Activities or portions thereof. In one example substantial similarity can be achieved when matches or substantial matches are found in speaking Conversational Activities or portions thereof of the compared Rounds of Conversational Exchange thereby tolerating mismatches in observing Conversational Activities i.e. silent facial expressions silent body movements motionless silence etc. or portions thereof. In some designs Substantial Similarity Comparison can be configured to omit any Conversational Activity or portions thereof from the comparison. In one example some or all motionless silence Conversational Activities or portions thereof can be omitted. In another example some or all absence from the conversation Conversational Activities or portions thereof or other Conversational Activities or portions thereof can be omitted. In further embodiments substantial similarity can be achieved taking into account duration type and or other features of Conversational Activities of the compared Rounds of Conversational Exchange . In one example substantial similarity can be achieved if the durations of one or more Conversational Activities of the compared Rounds of Conversational Exchange match or substantially match. In another example substantial similarity can be achieved if the types i.e. speaking silent facial expressions silent body movements motionless silence absence from the conversation etc. of one or more Conversational Activities of the compared Rounds of Conversational Exchange match or substantially match. In some aspects Substantial Similarity Comparison can compare durations types and or other features of Conversational Activities as an initial check before proceeding to further detailed comparisons.

Substantial Similarity Comparison can automatically adjust i.e. increase or decrease the strictness of the rules for determining substantial similarity of Rounds of Conversational Exchange . In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that substantially similar match had not been found. Substantial Similarity Comparison can keep adjusting the strictness of the rules until a substantially similar match is found. All the rules or settings of substantial similarity can be set reset or adjusted by Substantial Similarity Comparison in response to another strictness level determination. For example Substantial Similarity Comparison may attempt to find a match in a certain percentage i.e. 95 etc. of Conversational Activities or portions thereof from the compared Rounds of Conversational Exchange . If the comparison does not provide a substantially similar match Substantial Similarity Comparison may decide to decrease the strictness of the rules to find a substantially similar match. In response Substantial Similarity Comparison may attempt to find fewer matching Conversational Activities or portions thereof than in the previous attempt using stricter rules. If the comparison still does not provide a substantially similar match Substantial Similarity Comparison may determine to further decrease i.e. down to a certain minimum strictness or threshold etc. the strictness by requiring fewer Conversational Activities or portions thereof to match thereby further increasing a chance of finding a substantially similar match. In further aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that multiple substantially similar matches had been found. Substantial Similarity Comparison can keep adjusting the strictness of the rules until a best of the substantially similar matches is found. For example Substantial Similarity Comparison may attempt to find a match in a certain percentage i.e. 70 etc. of Conversational Activities or portions thereof from the compared Rounds of Conversational Exchange . If the comparison provides a number of substantially similar matches Substantial Similarity Comparison may decide to increase the strictness of the rules to decrease the number of substantially similar matches. In response Substantial Similarity Comparison may attempt to find more matching Conversational Activities or portions thereof in addition to the earlier found Conversational Activities or portions thereof to limit the number of substantially similar matches. If the comparison still provides more than one substantially similar match Substantial Similarity Comparison may determine to further increase the strictness by requiring additional Conversational Activities or portions thereof to match thereby further narrowing the number of substantially similar matches until a best substantially similar match is found.

In determining substantial similarity of Conversational Activities or portions thereof Substantial Similarity Comparison may compare Sub stream of Digital Pictures or portions thereof of one Conversational Activity with Sub stream of Digital Pictures or portions thereof of another Conversational Activity . Also in determining substantial similarity of Conversational Activities or portions thereof Substantial Similarity Comparison may compare Sub stream of Digital Sound Samples or portions thereof of one Conversational Activity with Sub stream of Digital Sound Samples or portions thereof of another Conversational Activity . In some embodiments total equivalence is achieved when Sub streams of Digital Pictures or portions thereof and Sub streams of Digital Sound Samples or portions thereof of the compared Conversational Activities match. If total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In some aspects substantial similarity can be achieved when most of the portions i.e. pictures features regions of pixels pixels etc. of Sub streams of Digital Pictures and or portions i.e. words features sound samples etc. of Sub streams of Digital Sound Samples of the compared Conversational Activities match or substantially match. In other aspects substantial similarity can be achieved when at least a threshold number or percentage of portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities match or substantially match. Similarly substantial similarity can be achieved when the number or percentage of matching or substantially matching portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities exceeds a threshold. In further aspects substantial similarity can be achieved when all but a threshold number or percentage of portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities match or substantially match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input. In one example substantial similarity can be achieved when at least 1 2 17 38 124 4299 or any other threshold number of portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities match or substantially match. Similarly substantial similarity can be achieved when the number of matching or substantially matching portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities exceeds 1 2 17 38 124 4299 or any other threshold number. In another example substantial similarity can be achieved when at least 9 23 29 41 63 79 92 or any other percentage of portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities match or substantially match. Similarly substantial similarity can be achieved when the percentage of matching or substantially matching portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities exceeds 9 23 29 41 63 79 92 or any other threshold percentage. In other embodiments weights can be assigned to Sub streams of Digital Pictures or portions thereof and or Sub streams of Digital Sound Samples or portions thereof indicating their importance in the comparison. In one example 1 i.e. 100 etc. can be assigned to Sub streams of Digital Sound Samples or portions thereof and 0 i.e. 0 etc. can be assigned to Sub streams of Digital Pictures or portions thereof indicating that Substantial Similarity Comparison relies solely on comparison of Sub streams of Digital Sound Samples or portions thereof in which case comparison of Sub streams of Digital Pictures or portions thereof can be omitted. In another example 0.8 i.e. 80 etc. can be assigned to Sub streams of Digital Sound Samples or portions thereof and 0.2 i.e. 20 etc. can be assigned to Sub streams of Digital Pictures or portions thereof indicating that Substantial Similarity Comparison relies mostly on comparison of Sub streams of Digital Sound Samples or portions thereof. In a further example 0 i.e. 0 etc. can be assigned to Sub streams of Digital Sound Samples or portions thereof and 1 i.e. 100 etc. can be assigned to Sub streams of Digital Pictures or portions thereof indicating that Substantial Similarity Comparison relies solely on comparison of Sub streams of Digital Pictures or portions thereof in which case comparison of Sub streams of Digital Sound Samples or portions thereof can be omitted. Any weight or importance can be assigned to any of the Sub streams of Digital Sound Samples or portions thereof Sub streams of Digital Pictures or portions thereof and or other elements herein. Similar weights can be utilized for any of the elements described herein. In further embodiments substantial similarity can be achieved taking into account the type and or other features of Conversational Activities . For example any observing Conversational Activity i.e. silent facial expressions silent body movements motionless silence etc. may be found to match another observing Conversational Activity .

Substantial Similarity Comparison can automatically adjust i.e. increase or decrease the strictness of the rules for determining substantial similarity of Conversational Activities . In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that substantially similar match had not been found. Substantial Similarity Comparison can keep adjusting the strictness of the rules until a substantially similar match is found. All the rules or settings of substantial similarity can be set reset or adjusted by Substantial Similarity Comparison in response to another strictness level determination. For example Substantial Similarity Comparison may attempt to find a match in a certain percentage i.e. 88 etc. of the portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities . If the comparison does not provide a substantially similar match Substantial Similarity Comparison may decide to decrease the strictness of the rules to find a substantially similar match. In response Substantial Similarity Comparison may attempt to find fewer matching portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples than in the previous attempt using stricter rules. If the comparison still does not provide a substantially similar match Substantial Similarity Comparison may determine to further decrease the strictness i.e. down to a certain minimum strictness or threshold etc. by requiring fewer portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples to match thereby further increasing a chance of finding a substantially similar match. In further aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that multiple substantially similar matches had been found. Substantial Similarity Comparison can keep adjusting the strictness of the rules until a best of the substantially similar matches is found. For example Substantial Similarity Comparison may attempt to find a match in a certain percentage i.e. 74 etc. of portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples of the compared Conversational Activities . If the comparison provides a number of substantially similar matches Substantial Similarity Comparison may decide to increase the strictness of the rules to decrease the number of substantially similar matches. In response Substantial Similarity Comparison may attempt to find more matching portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples in addition to the earlier found portions to limit the number of substantially similar matches. If the comparison still provides more than one substantially similar match Substantial Similarity Comparison may determine to further increase the strictness by requiring additional portions of Sub streams of Digital Pictures and or portions of Sub streams of Digital Sound Samples to match thereby further narrowing the number of substantially similar matches until a best substantially similar match is found.

In determining substantial similarity of Sub streams of Digital Pictures or portions thereof and or Sub streams of Digital Sound Samples or portions thereof Substantial Similarity Comparison can use various techniques examples of which are described below.

In some embodiments in determining substantial similarity of Sub streams of Digital Sound Samples or portions thereof Substantial Similarity Comparison may compare one or more words recognized from one Sub stream of Digital Sound Samples with one or more words recognized from another Sub stream of Digital Sound Samples . Any features functionalities and embodiments of the previously described Speech Sound Recognizer can be used in such word or speech recognition. In some aspects total equivalence is found when all words recognized from one Sub stream of Digital Sound Samples match all words recognized from another Sub stream of Digital Sound Samples . In other aspects if total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In one example substantial similarity can be achieved when most of the words recognized from the compared Sub streams of Digital Sound Samples match. In another example substantial similarity can be achieved when at least a threshold number i.e. 1 2 4 7 34 etc. or percentage i.e. 33 58 72 99 etc. of words of the compared Sub streams of Digital Sound Samples match. Similarly substantial similarity can be achieved when the number or percentage of matching words of the compared Sub streams of Digital Sound Samples exceeds a threshold number i.e. 1 2 4 7 34 etc. or a threshold percentage i.e. 33 58 72 99 etc. . In a further example substantial similarity can be achieved when all but a threshold number or percentage of words of the compared Sub streams of Digital Sound Samples match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis and or other techniques knowledge or input. In further aspects Substantial Similarity Comparison can utilize the order of words for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to front most words thereby tolerating mismatches in later words. Any order of words can be favored in alternate implementations. In further aspects Substantial Similarity Comparison can utilize the type of words for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to substantive or longer words such as nouns verbs adjectives and or others thereby tolerating mismatches in less substantive or shorter words such as definite and indefinite articles i.e. the a an etc. demonstratives i.e. this that etc. possessive determiners i.e. my your their etc. quantifiers i.e. many few several etc. distributive determiners i.e. each any etc. and or others. In further aspects Substantial Similarity Comparison can utilize the importance i.e. as indicated by importance index later described etc. of words for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to more important words such as the above described substantive longer and or other words thereby tolerating mismatches in less important words such as less substantive shorter and or other words. In further aspects Substantial Similarity Comparison can omit some of the words from the comparison in determining substantial similarity of Sub streams of Digital Sound Samples . In one example less substantive or shorter words can be omitted from comparison. In another example later or rear most words can be omitted from comparison. In general any word can be omitted from comparison. In further aspects Substantial Similarity Comparison can utilize semantic conversion to account for variations of words in determining substantial similarity of Sub streams of Digital Sound Samples . In one example Sub stream of Digital Sound Samples may include a word home . In addition to searching for the exact word in a compared Sub stream of Digital Sound Samples Substantial Similarity Comparison can employ semantic conversion and attempt to match house residence dwelling place or other semantically similar variations of the word with a meaning home . In another example Sub stream of Digital Sound Samples may include a word buy . In addition to searching for the exact word in a compared Sub stream of Digital Sound Samples Substantial Similarity Comparison can employ semantic conversion and attempt to match buying bought or other semantically similar variations of the word with a meaning buy in different tenses. Any other grammatical analysis or transformation can be utilized to cover the full scope of word variations. In some designs semantic conversion can be implemented using a thesaurus or dictionary. In another example semantic conversion can be implemented using a table where each row comprises semantically similar variations of a word. In further aspects Substantial Similarity Comparison can utilize a language model for understanding or interpreting the concepts contained in the words and compare the concepts instead of or in addition to the words in determining substantial similarity of Sub streams of Digital Sound Samples . A language model may also provide context to distinguish among words and phrases that sound similar. Examples of language models include unigram model n gram model neural network language model bag of words model and or others. Any of the techniques for matching of words can similarly be used for matching of concepts.

Substantial Similarity Comparison can automatically adjust i.e. increase or decrease the strictness of the rules for determining substantial similarity of Sub streams of Digital Sound Samples using words. In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that substantially similar match had not been found. Substantial Similarity Comparison can keep adjusting the strictness of the rules until a substantially similar match is found. All the rules or settings of substantial similarity can be set reset or adjusted by Substantial Similarity Comparison in response to another strictness level determination. For example Substantial Similarity Comparison may attempt to find a match in a certain percentage i.e. 95 etc. of the recognized words from the compared Sub streams of Digital Sound Samples . If the comparison does not provide a substantially similar match using such strictness Substantial Similarity Comparison may decide to decrease the strictness of the rules to find a substantially similar match. In response Substantial Similarity Comparison may attempt to find fewer matching words than in the previous attempt using stricter rules. If the comparison still does not provide a substantially similar match Substantial Similarity Comparison may determine to further decrease the strictness i.e. down to a certain minimum strictness or threshold etc. by requiring fewer words to match thereby further increasing a chance of finding a substantially similar match.

Where a reference to a word is used herein it should be understood that a portion of a word or a collection of words can be used instead of or in addition to the word. In one example instead of or in addition to words individual characters i.e. letters digits symbols etc. and or other portions of a word can be compared. In another example instead of or in addition to words phrases sentences and or other collections of words can be compared. As such any operations rules logic and or functions operating on words similarly apply to any portion of a word and or any collection of words. In a further example where one or more front most words are used in the comparison as previously described one or more front most characters and or other portions of a word can be used in an alternate implementation of the comparison involving portions of a word. In a further example where comparison strictness is increased by using one or more additional words in the comparison as previously described additional one or more characters and or other portions of a word can be used in an alternate implementation of the comparison involving portions of a word. In general whole words portions of a word and or collections of words including any operations thereon can be combined to arrive at desired results. Some or all of the above described rules logic and or techniques can be utilized alone or in combination with each other or with other rules logic and or techniques. One of ordinary skill in art will recognize that other techniques known in art for determining similarity of streams of digital sound samples and or other data sequences that would be too voluminous to describe are within the scope of this disclosure.

In other embodiments in determining substantial similarity of Sub streams of Digital Sound Samples or portions thereof Substantial Similarity Comparison can compare one or more features from one Sub stream of Digital Sound Samples with one or more features from another Sub stream of Digital Sound Samples . A feature also referred to as sound feature or other similar reference etc. may include a collection of sound samples of a stream of digital sound samples. For example a feature may include sound samples in time frames of 5 10 15 20 25 30 etc. milliseconds. In general any collection of sound samples covering any time frame can be utilized. Some of the steps or elements in a typical feature oriented system include pre processing feature extraction acoustic modeling language modeling and or others or a combination thereof each of which may include its own sub steps or sub elements depending on the application. Acoustic features can be used for classification of non verbal vocal outbursts such as laughter or sighs whereas linguistic features can be used to transcribe the linguistic message such as words phrases or sentences. Examples of feature extraction techniques include Mel Frequency Cepstral Coefficients Wavelet Based Features Non Negative Matrix Factorization and or others. Once features of the compared Sub streams of Digital Sound Samples are known Substantial Similarity Comparison can compare the features to determine substantial similarity. Some of the previously described comparison techniques for determining substantial similarity of Sub streams of Digital Sound Samples using words can similarly be used for features. In some aspects total equivalence is found when all features from one Sub stream of Digital Sound Samples match al features from another Sub stream of Digital Sound Samples . In other aspects if total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In one example substantial similarity can be achieved when most of the features from the compared Sub streams of Digital Sound Samples match. In another example substantial similarity can be achieved when at least a threshold number i.e. 1 5 17 33 68 114 etc. or percentage i.e. 31 59 82 98 etc. of features from the compared Sub streams of Digital Sound Samples match. Similarly substantial similarity can be achieved when the number or percentage of matching features from the compared Sub streams of Digital Sound Samples exceeds a threshold number i.e. 1 5 17 33 68 114 etc. or a threshold percentage i.e. 31 59 82 98 etc. . In a further example substantial similarity can be achieved when all but a threshold number or percentage of features from the compared Sub streams of Digital Sound Samples match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis and or other techniques knowledge or input. In further aspects Substantial Similarity Comparison can utilize the order of features for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to front most features thereby tolerating mismatches in later features. Any order of features can be favored in alternate implementations. In further aspects Substantial Similarity Comparison can utilize the type of features for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to substantive or longer features thereby tolerating mismatches in less substantive or shorter features. In further aspects Substantial Similarity Comparison can utilize the importance i.e. as indicated by importance index later described etc. of features for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to more important features such the above described substantive longer and or other features thereby tolerating mismatches in less important features such as less substantive shorter and or other features. In further aspects Substantial Similarity Comparison can omit some of the features from the comparison in determining substantial similarity of Sub streams of Digital Sound Samples . In one example less substantive or shorter features can be omitted from comparison. In another example later or rear most features can be omitted from comparison. In general any feature can be omitted from comparison.

The previously described functionality of Substantial Similarity Comparison for automatically adjusting i.e. increasing or decreasing the strictness of the rules for determining substantial similarity of Sub streams of Digital Sound Samples using words can similarly be used with features. In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that substantially similar match had not been found. For example if the comparison does not provide a substantially similar match using certain strictness Substantial Similarity Comparison may decide to decrease the strictness i.e. down to a certain minimum strictness or threshold etc. and attempt to find fewer matching features than in the previous attempt using stricter rules.

Where a reference to a feature is used herein it should be understood that a portion of a feature or a collection of features can be used instead of or in addition to the feature. In one example instead of or in addition to features individual sound samples that constitute the feature can be compared. In another example instead of or in addition to features collections of features can be compared. In a further example levels of features where a feature on one level includes one or more features from another level i.e. prior level etc. can be compared. As such any operations rules logic and or functions operating on features similarly apply to any portion of a feature and or any collection of features. In general whole features portions of a feature and or collections of features including any operations thereon can be combined to arrive at desired results. Some or all of the above described rules logic and or techniques can be utilized alone or in combination with each other or with other rules logic and or techniques. Any of the previously described features functionalities and embodiments of Substantial Similarity Comparison for determining substantial similarity of Sub streams of Digital Sound Samples using words can similarly be used for features. One of ordinary skill in art will recognize that other techniques known in art for determining similarity of streams of digital sound samples and or other data sequences that would too voluminous to describe are within the scope of this disclosure.

In further embodiments in determining substantial similarity of Sub streams of Digital Sound Samples or portions thereof Substantial Similarity Comparison can compare sound samples from one Sub stream of Digital Sound Samples with sound samples from another Sub stream of Digital Sound Samples . In some aspects total equivalence is found when all sound samples from one Sub stream of Digital Sound Samples match all sound samples from another Sub stream of Digital Sound Samples . In other aspects if total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In one example substantial similarity can be achieved when most of the sound samples from the compared Sub streams of Digital Sound Samples match. In another example substantial similarity can be achieved when at least a threshold number i.e. 21 85 154 297 422 699 etc. or percentage i.e. 29 48 69 96 etc. of sound samples from the compared Sub streams of Digital Sound Samples match. Similarly substantial similarity can be achieved when the number or percentage of matching sound samples from the compared Sub streams of Digital Sound Samples exceeds a threshold number i.e. 21 85 154 297 422 699 etc. or a threshold percentage i.e. 29 48 69 96 etc. . In a further example substantial similarity can be achieved when all but a threshold number or percentage of sound samples from the compared Sub streams of Digital Sound Samples match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis and or other techniques knowledge or input. In further aspects Substantial Similarity Comparison can utilize the order of sound samples for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to front most sound samples thereby tolerating mismatches in later sound samples. Any order of sound samples can be favored in alternate implementations. In further aspects Substantial Similarity Comparison can utilize the importance i.e. as indicated by importance index later described etc. of sound samples for determining substantial similarity of Sub streams of Digital Sound Samples . For example substantial similarity can be achieved when matches are found with respect to more important sound samples such the above described front most and or other sound samples thereby tolerating mismatches in less important sound samples such as later and or other sound samples. In further aspects Substantial Similarity Comparison can omit some of the sound samples from the comparison in determining substantial similarity of Sub streams of Digital Sound Samples . In one example later or rear most sound samples can be omitted from comparison. In another example every other sound sample can be omitted from comparison i.e. to reduce processing time etc. . In general any sound sample can be omitted from comparison. In further aspects Substantial Similarity Comparison can generally compare one or more sound samples from one Sub stream of Digital Sound Samples with one or more corresponding i.e. similarly positioned temporally related etc. sound samples from another Sub stream of Digital Sound Samples . In one example a 78sound sample from one Sub stream of Digital Sound Samples can be compared with a 78sound sample from another Sub stream of Digital Sound Samples . In another example the 78sound sample from one Sub stream of Digital Sound Samples can be compared with a number of samples around i.e. preceding and or following the 78sound sample from another Sub stream of Digital Sound Samples . This way flexibility can be implemented in finding a matching sound sample if the samples in the compared Sub streams of Digital Sound Samples are not perfectly aligned. In some designs adjustments can be made in selecting one or more corresponding sound samples. In one example Substantial Similarity Comparison can make adjustments to account for variable lengths of silence periods in the compared Sub streams of Digital Sound Samples . In another example Substantial Similarity Comparison can utilize Dynamic Time Warping DTW and or other techniques for comparing and or aligning temporal sequences i.e. Sub stream of Digital Sound Samples etc. that may vary in time or speed in order to account for different speaking speeds. In further aspects Substantial Similarity Comparison can utilize collections or frames of sound samples in determining substantial similarity of Sub streams of Digital Sound Samples . For instance a frame may include sound samples in time periods of 5 10 15 20 25 30 etc. milliseconds. In general a frame may include any number of sound samples covering any time period. In one example substantial similarity can be achieved when most of the frames of the compared Sub streams of Digital Sound Samples match. In another example substantial similarity can be achieved when at least a threshold number i.e. 37 111 228 433 etc. or percentage i.e. 39 48 68 75 99 etc. of frames of the compared Sub streams of Digital Sound Samples match.

The previously described functionality of Substantial Similarity Comparison for automatically adjusting i.e. increasing or decreasing the strictness of the rules for determining substantial similarity of Sub streams of Digital Sound Samples using words and or features can similarly be used with sound samples. In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that a substantially similar match had not been found. For example if the comparison does not provide a substantially similar match using certain strictness Substantial Similarity Comparison may decide to decrease the strictness i.e. down to a certain minimum strictness or threshold etc. and attempt to find fewer matching sound samples than in the previous attempt using stricter rules.

Where a reference to a sound sample is used herein it should be understood that a collection i.e. frame etc. of sound samples can be used instead of or in addition to the sound sample. For example instead of or in addition to sound samples collections of sound samples can be compared. As such any operations rules logic and or functions operating on sound samples similarly apply to any collection of sound samples. In general sound samples and or collections of sound samples including any operations thereon can be combined to arrive at desired results. Some or all of the above described rules logic and or techniques can be utilized alone or in combination with each other or with other rules logic and or techniques. Any of the previously described features functionalities and embodiments of Substantial Similarity Comparison for determining substantial similarity of Sub streams of Digital Sound Samples using words and or features can similarly be used for sound samples. One of ordinary skill in art will recognize that other techniques known in art for determining similarity of streams of digital sound samples and or other data sequences that would too voluminous to describe are within the scope of this disclosure.

Other aspects or properties of digital sound or sound samples can be taken into account by Substantial Similarity Comparison in sound comparisons. Examples of such aspects or properties include amplitude adjustment sample rate or frequency adjustment noise reduction and or others. In some implementations as digital sound can be captured by various sound sensing equipment from various distances and under various conditions Substantial Similarity Comparison can adjust values or levels of sound samples or otherwise manipulate the samples before or during comparison. Such adjustment may include amplifying or reducing the values of one or more sound samples. For example Substantial Similarity Comparison can amplify all samples of one Sub stream of Digital Sound Samples to make it more comparable to another Sub stream of Digital Sound Samples . Substantial Similarity Comparison can also incrementally adjust the sound samples such as amplifying or reducing the samples by a certain amount in each cycle of comparisons in order to find a substantially similar match at one of the incremental adjustment levels. In other implementations Substantial Similarity Comparison can re sample i.e. up sample or down sample or otherwise transform a digital sound before or during comparison. Such re sampling or transformation may include increasing or decreasing the number of sound samples. For example Substantial Similarity Comparison can increase or decrease the number of samples of a Sub stream of Digital Sound Samples to equate its sample rate or frequency with sample rate or frequency of another Sub stream of Digital Sound Samples . Any publically available custom or other re sampling technique or program can be utilized such as converting a stream of sound samples to an analog signal and re sampling at a new rate calculating the values of the new samples directly from the old samples using interpolation i.e. constant linear polynomial spline etc. or decimation i.e. by an integer or rational factor etc. and or others. In further implementations as digital sound can be captured in various environments Substantial Similarity Comparison can perform denoising of the compared Sub streams of Digital Sound Samples . Noise may include any signal that degrades the quality of sound such as equipment related noise electrical or electromagnetic noise environmental noise and or other noise. Any of the publically available custom or other denoising techniques or programs can be utilized such as Adaptive Wiener Filtering Spectral Subtraction Methods i.e. cepstral mean normalization Spectral Restoration i.e. speech enhancement Harmonic Decomposition Nonnegative Matrix Factorization NMF and or others. Substantial Similarity Comparison can perform any other pre processing or manipulation of digital sound or sound samples before or during comparison.

In any of the comparisons involving digital sound or sound samples Substantial Similarity Comparison can utilize a threshold for acceptable number or percentage difference in determining a match for each compared sound sample. A sound sample can be encoded using various techniques such as binary value i.e. 8 bit 16 bit 24 bit etc. hexadecimal value numerical value and or others. For instance in a 16 bit encoding scheme each sound sample is encoded with a value or level of 0 65536 or its binary equivalent. In one example a threshold for acceptable difference i.e. absolute difference etc. can be set at 1000 for each of the sound samples. Therefore a sample encoded with a value of 30100 matches or is sufficiently similar to a compared sample encoded with a value of 30883 because the difference in the samples faNs within the acceptable difference threshold i.e. 1000 in this example etc. . Furthermore a sample encoded with a value of 30100 does not match or is not sufficiently similar to a compared pixel encoded with a value of 31155 because the difference in the samples falls outside the acceptable difference threshold. Any other number threshold can be used such as 1 8 82 492 1195 5004 13883 33841 57112 and or others. A threshold for acceptable percentage difference can similarly be utilized such as 0.38 4 11 24 31 46 78 and or others. A similar difference determination can be utilized in samples encoded in any other encoding scheme. The aforementioned thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input.

In some designs in determining substantial similarity of Sub streams of Digital Pictures or portions thereof Substantial Similarity Comparison can compare pictures from one Sub stream of Digital Pictures with pictures from another Sub stream of Digital Pictures . Any features functionalities and embodiments of the previously described Picture Recognizer can be used in determining substantial similarity of Sub streams of Digital Pictures . In some aspects Substantial Similarity Comparison can compare pictures from one Sub stream of Digital Pictures with corresponding i.e. similarly positioned temporally related etc. pictures from another Sub stream of Digital Pictures . In one example a 67picture from one Sub stream of Digital Pictures can be compared with a 67picture from another Sub stream of Digital Pictures . In another example the 67picture from one Sub stream of Digital Pictures can be compared with a number of pictures around i.e. preceding and or following the 67picture from another Sub stream of Digital Pictures . This way flexibility can be implemented in finding a substantially similar picture if the pictures in the compared Sub streams of Digital Pictures are not perfectly aligned. In other aspects Substantial Similarity Comparison can utilize Dynamic Time Warping DTW and or other techniques know in art for comparing and or aligning temporal sequences i.e. Sub streams of Digital Pictures etc. that may vary in time or speed. Once the corresponding i.e. similarly positioned temporally related time warped aligned etc. pictures of Sub streams of Digital Pictures are compared and their substantial similarity determined using comparison techniques for individual pictures described below Substantial Similarity Comparison can utilize a threshold for the number or percentage of matching pictures for determining substantial similarity of the compared Sub streams of Digital Pictures . In one example substantial similarity can be achieved when most of the pictures of the compared Sub streams of Digital Pictures match or substantially match. In another example substantial similarity of Sub streams of Digital Pictures can be achieved when at least a threshold number i.e. 28 74 283 322 995 874 8028 etc. or percentage i.e. 29 33 58 72 99 etc. of pictures of the compared Sub streams of Digital Pictures match or substantially match. Similarly substantial similarity can be achieved when the number or percentage of matching or substantially matching pictures of the compared Sub streams of Digital Pictures exceeds a threshold number i.e. 28 74 283 322 995 874 8028 etc. or a threshold percentage i.e. 29 33 58 72 99 etc. . In a further example substantial similarity of Sub streams of Digital Pictures can be achieved when all but a threshold number or percentage of pictures of the compared Sub streams of Digital Pictures match or substantially match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input. In some aspects Substantial Similarity Comparison can omit some of the pictures from the comparison in determining substantial similarity of Sub streams of Digital Pictures .

In some embodiments in determining substantial similarity of individual pictures i.e. pictures from the compared Sub streams of Digital Pictures etc. Substantial Similarity Comparison can compare one or more features of one picture with one or more features of another picture. A feature also referred to as picture feature or other similar reference etc. may include a collection or region of pixels of a picture. Some of the steps or elements in a typical feature oriented system include pre processing feature extraction detection segmentation decision making and or others or a combination thereof each of which may include its own sub steps or sub elements depending on the application. Examples of features that can be used include lines edges ridges corners blobs regions and or others. Examples of feature extraction techniques include Canny Sobe Kayyali Harris Stephens et al SUSAN Level Curve Curvature FAST Laplacian of Gaussian Difference of Gaussians Determinant of Hessian MSER PCBR Grey level Blobs and or others. Once features of the compared pictures are known Substantial Similarity Comparison can compare the features to determine substantial similarity. In some aspects total equivalence is found when all features of one picture match all features of another picture. In other aspects if total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In one example substantial similarity can be achieved when most of the features of the compared pictures match. In another example substantial similarity can be achieved when at least a threshold number i.e. 3 22 47 93 128 431 etc. or percentage i.e. 49 53 68 72 95 etc. of features of the compared pictures match. Similarly substantial similarity can be achieved when the number or percentage of matching features of the compared pictures exceeds a threshold number i.e. 3 22 47 93 128 431 etc. or a threshold percentage i.e. 49 53 68 72 95 etc. . In a further example substantial similarity can be achieved when all but a threshold number or percentage of features of the compared pictures match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis and or other techniques knowledge or input. In further aspects Substantial Similarity Comparison can utilize the type of features for determining substantial similarity of pictures. In one example substantial similarity can be achieved when matches are found with respect to edges thereby tolerating mismatches in blobs. In another example substantial similarity can be achieved when matches are found with respect to more substantive larger and or other features thereby tolerating mismatches in less substantive smaller and or other features. In further aspects Substantial Similarity Comparison can utilize the importance i.e. as indicated by importance index later described etc. of features for determining substantial similarity of pictures. For example substantial similarity can be achieved when matches are found with respect to more important features such as the above described more substantive larger and or other features thereby tolerating mismatches in less important features such as less substantive smaller and or other features. In further aspects Substantial Similarity Comparison can omit some of the features from the comparison in determining substantial similarity of pictures. In one example isolated features can be omitted from comparison. In another example less substantive or smaller features can be omitted from comparison. In general any feature can be omitted from comparison. In further aspects Substantial Similarity Comparison can focus on features in certain regions of interest of the compared pictures. For example substantial similarity can be achieved when matches are found with respect to features in regions comprising the face or parts i.e. mouth eyes etc. thereof thereby tolerating mismatches in features of regions comprising other body parts the background and or other regions. In further aspects Substantial Similarity Comparison can detect or recognize persons or objects in the compared pictures. Any features functionalities and embodiments of the previously described Picture Recognizer can be used in such detection or recognition. Once a person or object is detected in a picture Substantial Similarity Comparison may attempt to detect the person or object in the compared picture. In one example substantial similarity can be achieved when the compared pictures comprise the same person or object. In another example concerning Sub streams of Digital Pictures substantial similarity can be achieved when the compared Sub streams of Digital Pictures comprise a detected person or object in at least a threshold number or percentage of their pictures. In further aspects Substantial Similarity Comparison may include identifying and or analyzing tied and or overlapping features which can then be combined i.e. similar to some process steps in convolutional neural networks etc. and compared to determine substantial similarity of pictures.

Substantial Similarity Comparison can automatically adjust i.e. increase or decrease the strictness of the rules for determining substantial similarity of pictures using features. In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that a substantially similar match had not been found. Substantial Similarity Comparison can keep adjusting the strictness rules until a substantially similar match is found. All the rules or settings of substantial similarity can be set reset or adjusted by Substantial Similarity Comparison in response to another strictness level determination. For example Substantial Similarity Comparison may attempt to find a match in a certain percentage i.e. 95 etc. of features from the compared pictures. If the comparison does not provide a substantially similar match Substantial Similarity Comparison may decide to decrease the strictness of the rules to find a substantially similar match. In response Substantial Similarity Comparison may attempt to find fewer matching features than in the previous attempt using stricter rules. If the comparison still does not provide a substantially similar match Substantial Similarity Comparison may determine to further decrease the strictness i.e. down to a certain minimum strictness or threshold etc. by requiring fewer features to match thereby further increasing a chance of finding a substantially similar match.

Where a reference to a feature is used herein it should be understood that a portion of a feature or a collection of features can be used instead of or in addition to the feature. In one example instead of or in addition to features individual pixels that constitute the feature can be compared. In another example instead of or in addition to features collections of features can be compared. In a further example levels of features where a feature on one level includes one or more features from another level i.e. prior level etc. can be compared. As such any operations rules logic and or functions operating on features similarly apply to any portion of a feature and or any collection of features. In general whole features portions of a feature and or collections of features including any operations thereon can be combined to arrive at desired results. Some or all of the above described rules logic and or techniques can be utilized alone or in combination with each other or with other rules logic and or techniques. One of ordinary skill in art will recognize that other techniques known in art for determining similarity of digital pictures streams thereof and or other data sequences that would be too voluminous to describe are within the scope of this disclosure.

In other embodiments in determining substantial similarity of individual pictures i.e. pictures from the compared Sub streams of Digital Pictures etc. Substantial Similarity Comparison can compare pixels of one picture with pixels of another picture. In some aspects total equivalence is found when all pixels of one picture match all pixels of another picture. In other aspects if total equivalence is not found Substantial Similarity Comparison may attempt to determine substantial similarity. In one example substantial similarity can be achieved when most of the pixels from the compared pictures match. In another example substantial similarity can be achieved when at least a threshold number i.e. 449 2219 92229 442990 1000028 etc. or percentage i.e. 39 45 58 72 92 etc. of pixels from the compared pictures match. Similarly substantial similarity can be achieved when the number or percentage of matching pixels from the compared pictures exceeds a threshold number i.e. 449 2219 92229 442990 1000028 etc. or a threshold percentage i.e. 39 45 58 72 92 etc. . In a further example substantial similarity can be achieved when all but a threshold number or percentage of pixels from the compared pictures match. Such thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis and or other techniques knowledge or input. In further aspects Substantial Similarity Comparison can omit some of the pixels from the comparison in determining substantial similarity of pictures. In one example pixels composing the background or any insignificant content can be omitted from comparison. In general any pixel can be omitted from comparison. In further aspects Substantial Similarity Comparison can utilize collections or regions of pixels in determining substantial similarity of pictures. A region may include any number of pixels. For example substantial similarity can be achieved when at least a threshold number i.e. 1 2 4 9 11 28 etc. or percentage i.e. 19 32 55 62 94 etc. of regions of the compared pictures match or substantially match. Similarly substantial similarity can be achieved when the number or percentage of matching regions of the compared pictures exceeds a threshold number i.e. 1 2 4 9 11 28 etc. or a threshold percentage i.e. 19 32 55 62 94 etc. . In further aspects Substantial Similarity Comparison can focus on pixels in certain regions of interest in determining substantial similarity of pictures. For example substantial similarity can be achieved when matches are found with respect to pixels in regions comprising the face or parts i.e. mouth eyes etc. thereof thereby tolerating mismatches in pixels of regions comprising other body parts the background and or other regions.

The previously described functionality of Substantial Similarity Comparison for automatically adjusting i.e. increasing or decreasing the strictness of the rules for determining substantial similarity of pictures using features can similarly be used with pixels. In some aspects such adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that total equivalence match had not been found. In other aspects an adjustment in strictness can be done by Substantial Similarity Comparison in response to determining that a substantially similar match had not been found. For example if the comparison does not provide a substantially similar match using certain strictness Substantial Similarity Comparison may decide to decrease the strictness i.e. down to a certain minimum strictness or threshold etc. and attempt to find fewer matching pixels than in the previous attempt using stricter rules.

Where a reference to a pixel is used herein it should be understood that a collection i.e. region etc. of pixels can be used instead of or in addition to the pixel. For example instead of or in addition to pixels collections of pixels can be compared. As such any operations rules logic and or functions operating on pixels similarly apply to any collection of pixels. In general pixels and or collections of pixels including any operations thereon can be combined to arrive at desired results. Some or all of the above described rules logic and or techniques can be utilized alone or in combination with each other or with other rules logic and or techniques. Any of the previously described features functionalities and embodiments of Substantial Similarity Comparison for determining substantial similarity of pictures using features can similarly be used for pixels. One of ordinary skill in art will recognize that other techniques known in art for determining similarity of digital pictures streams thereof and or other data sequences that would be too voluminous to describe are within the scope of this disclosure.

Other aspects or properties of digital pictures or pixels can be taken into account by Substantial Similarity Comparison in picture comparisons. Examples of such aspects or properties include color adjustment size adjustment transparency i.e. alpha channel etc. use of mask and or others. In some implementations as digital pictures can be captured by various picture taking equipment in various environments and under various lighting conditions Substantial Similarity Comparison can adjust lighting or color of pixels or otherwise manipulate pixels before or during comparison. Lighting or color adjustment also referred to as gray balance neutral balance white balance etc. may generally include manipulating or rebalancing the intensities of the colors i.e. red green and or blue if RGB color model is used etc. of one or more pixels. For example Substantial Similarity Comparison can adjust lighting or color of all pixels of one picture to make it more comparable to another picture. Substantial Similarity Comparison can also incrementally adjust the pixels such as increasing or decreasing the red green and or blue pixel values by a certain amount in each cycle of comparisons in order to find a substantially similar match at one of the incremental adjustment levels. Any of the publically available custom or other lighting or color adjustment techniques or programs can be utilized such as color filters color balancing color correction and or others. In other implementations Substantial Similarity Comparison can resize or otherwise transform a digital picture before or during comparison. Such resizing or transformation may include increasing or decreasing the number of pixels. For example Substantial Similarity Comparison can increase or decrease the size of a picture proportionally i.e. increase or decrease length and or width keeping aspect ratio constant etc. to equate its size with the size of another picture. Substantial Similarity Comparison can also incrementally resize a picture such as increasing or decreasing the size of the picture proportionally by a certain amount in each cycle of comparisons in order to find a substantially similar match at one of the incremental sizes. Any of the publically available custom or other digital picture resizing techniques or programs can be utilized such as nearest neighbor interpolation bilinear interpolation bicubic interpolation and or others. In further implementations in digital pictures comprising transparency features or functionalities Substantial Similarity Comparison can utilize a threshold for acceptable number or percentage transparency difference similar to the below described threshold for the acceptable color difference. Alternatively transparency can be applied to one or more pixels of a picture and color difference may then be determined between compared pixels taking into account the transparency related color effect. Alternatively transparent pixels can be excluded from comparison. In further implementations certain regions or subsets of pixels can be ignored during comparison using a mask to define the region or subset of pixels excluded from comparison. In general any region or subset of a picture determined to contain no content of interest can be excluded from comparison using a mask. Examples of such regions or subsets include background transparent or partially transparent regions regions comprising insignificant content or any arbitrary region or subset. Substantial Similarity Comparison can perform any other pre processing or manipulation of digital pictures or pixels before or during comparison.

In any of the comparisons involving digital pictures or pixels Substantial Similarity Comparison can utilize a threshold for acceptable number or percentage difference in determining a match for each compared pixel. A pixel in a digital picture can be encoded using various techniques such as RGB i.e. red green blue CMYK i.e. cyan magenta yellow and key black binary value hexadecimal value numerical value and or others. For instance in RGB color scheme each of red green and blue colors is encoded with a value 0 255 or its binary equivalent. In one example a threshold for acceptable difference i.e. absolute difference etc. can be set at 10 for each of the three colors. Therefore a pixel encoded as R130 G240 B50 matches or is sufficiently similar to a compared pixel encoded as R135 G231 B57 because the differences in all three colors fall within the acceptable difference threshold i.e. 10 in this example etc. . Furthermore a pixel encoded as R130 G240 B50 does not match or is not sufficiently similar to a compared pixel encoded as R143 G231 B57 because the difference in red value falls outside the acceptable difference threshold. Any other number threshold can be used such as 1 3 8 15 23 77 132 197 243 and or others. A threshold for acceptable percentage difference can similarly be utilized such as 0.12 2 7 14 23 36 65 and or others. In some aspects a threshold for acceptable number or percentage difference in red green and blue can be set to be different for each color. A similar difference determination can be utilized in pixels encoded in any other color scheme. The aforementioned thresholds can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input.

In some embodiments the previously described Extra Info can be used to enhance any of the aforementioned substantial similarity determinations. Extra Info may include any contextual or other information that can be useful in determining similarity between any compared elements. In one example Extra Info includes time stamp i.e. time information etc. of a Sub stream of Digital Pictures Sub stream of Digital Sound Samples Conversational Activity Round of Conversational Exchange and or other element. In another example Extra Info includes location i.e. location information etc. of the Conversation Participant while engaged in a conversation. In a further example Extra Info includes objects or environments i.e. observed information etc. recognized from Sub stream of Digital Pictures and or Sub stream of Digital Sound Samples . In some aspects in determining substantial similarity of Sub streams of Digital Pictures Substantial Similarity Comparison can compare one or more Extra Info i.e. time stamp location information observed information etc. in addition to or instead of comparing pictures i.e. frames etc. features pixels and or other elements. Extra Info can be set to be less equally or more important i.e. as indicated by importance index later described etc. than pictures features pixels and or other elements in the comparison. In other aspects in determining substantial similarity of Sub streams of Digital Sound Samples Substantial Similarity Comparison can compare one or more Extra Info in addition to or instead of comparing words features sound samples and or other elements. Extra Info can be set to be less equally or more important than words features sound samples and or other elements in the comparison. In further aspects in determining substantial similarity of Conversational Activities Substantial Similarity Comparison can compare one or more Extra Info in addition to or instead of comparing Sub streams of Digital Pictures Sub streams of Digital Sound Samples and or other elements. Extra Info can be set to be less equally or more important than any Sub stream of Digital Pictures Sub stream of Digital Sound Samples and or other elements in the comparison. In further aspects in determining substantial similarity of Rounds of Conversational Exchange Substantial Similarity Comparison can compare one or more Extra Info in addition to or instead of comparing Conversational Activities and or other elements. Extra Info can be set to be less equally or more important than any Conversational Activity and or other elements in the comparison.

In some embodiments an importance index not shown or other importance ranking technique can be used in any of the previously described comparisons or other processing involving elements of different importance. Importance index indicates importance of the element to or with which the index is assigned or associated. For example importance index may indicate importance of Round of Conversational Exchange Conversational Activity Sub stream of Digital Pictures Sub stream of Digital Sound Samples word feature and or other element to or with which the index is assigned or associated. In some aspects importance index on a scale from 0 to 1 can be utilized although any other range can also be utilized. Importance index can be stored in or associated with the element to which the index pertains. Association of importance indexes can be implemented using a table where one column comprises elements and another column comprises their associated importance indexes for example. Importance indexes of various elements can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input. In one example a higher Importance index can be assigned to speaking Conversational Activities . In another example a higher Importance index can be assigned to Extra Info . In a further example a higher importance index can be assigned to front most words recognized from a Sub stream of Digital Sound Samples . Any importance index can be assigned to or associated with any element described herein. Any importance ranking technique can be utilized as or instead of importance index in alternate embodiments.

In some embodiments Substantial Similarity Comparison may generate a similarity index not shown for any compared elements. Similarity index indicates how well an element is matched with another element. For example similarity index indicates how well a Round of Conversational Exchange Conversational Activity Sub stream of Digital Pictures Sub stream of Digital Sound Samples word feature and or other element is matched with a compared element. In some aspects similarity index on a scale from 0 to 1 can be utilized although any other range can also be utilized. Similarity index can be generated by Substantial Similarity Comparison whether substantial similarity between the compared elements is achieved or not. In one example similarity index can be determined for a Sub stream of Digital Sound Samples based on a ratio percentage of matched words and or Extra Info relative to the number of all words and or Extra Info in the Sub stream of Digital Sound Samples . Specifically similarity index of 0.46 is determined if 46 of words and or Extra Info match. In some designs importance i.e. as indicated by importance index etc. of one or more words and or Extra Info can be included in the calculation of a weighted similarity index. Similar determination of similarity index can be implemented with features sound samples and or other elements of Sub stream of Digital Sound Samples . In a further example similarity index can be determined for a Sub stream of Digital Pictures based on a ratio percentage of matched pictures i.e. frames etc. and or Extra Info relative to the number of all pictures i.e. frames etc. and or Extra Info in the Sub stream of Digital Pictures . Specifically similarity index of 0.93 is determined if 93 of pictures i.e. frames etc. and or Extra Info match or substantially match. In some designs importance i.e. as indicated by importance index etc. of one or more pictures i.e. frames etc. and or Extra Info can be included in the calculation of a weighted similarity index. Similar determination of similarity index can be implemented with features pixels and or other elements of Sub stream of Digital Pictures . In another example similarity index can be determined for a Conversational Activity based on similarities or similarity indexes of Sub streams of Digital Pictures Sub streams of Digital Sound Samples and or Extra Info in the compared Conversational Activities . In some designs an average or weighted average of similarities or similarity indexes of Sub streams of Digital Pictures Sub streams of Digital Sound Samples and or Extra Info can be used to calculate a weighted similarity index for a Conversational Activity . In another example similarity index can be determined for a Round of Conversational Exchange based on similarities or similarity indexes of Conversational Activities and or Extra Info in the compared Rounds of Conversational Exchange . In some designs an average or weighted average of similarities or similarity indexes of Conversational Activities and or Extra Info can be used in calculating a weighted similarity index for a Round of Conversational Exchange . Any combination of the aforementioned similarity index determinations or calculations can be utilized in alternate embodiments. Any similarity ranking technique can be utilized to determine or calculate similarity index in alternate embodiments.

Referring to an exemplary embodiment of learning Rounds of Conversational Exchange using Neural Network comprising shortcut Connections is illustrated. In some aspects Rounds of Conversational Exchange in one Layer can be connected with Rounds of Conversational Exchange in any Layer not only in a successive Layer thereby creating shortcuts i.e. shortcut Connections etc. through Neural Network . Creating a shortcut Connection can be implemented by performing Substantial Similarity Comparisons of a Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in any Layer when applying i.e. storing copying etc. the Round of Conversational Exchange from Knowledge Structuring Unit onto Neural Network . Once created shortcut Connections enable a wider variety of Rounds of Conversational Exchange to be considered when selecting a path through Neural Network . In some embodiments Knowledge Structuring Unit creates or generates Rounds of Conversational Exchange and the system applies them onto Neural Network thereby implementing learning of Rounds of Conversational Exchange . The system can perform Substantial Similarity Comparisons of a Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in a corresponding and or other Layers of Neural Network . If a substantially similar Round of Conversational Exchange is not found in the corresponding or other Layers of Neural Network the system may insert i.e. copy store etc. the Round of Conversational Exchange from Knowledge Structuring Unit into the corresponding or another Layer of Neural Network and create a Connection to the inserted Round of Conversational Exchange from a prior Round of Conversational Exchange including assigning an occurrence count to the new Connection calculating a weight of the new Connection and updating any other Connections originating from the prior Round of Conversational Exchange . On the other hand if a substantially similar Round of Conversational Exchange is found in the corresponding or other Layers of Neural Network the system may update occurrence count and weight of Connection to that Round of Conversational Exchange from a prior Round of Conversational Exchange and update any other Connections originating from the prior Round of Conversational Exchange . Any of the previously described and or other techniques for comparing inserting updating and or other operations on Rounds of Conversational Exchange Connections Layers and or other elements can similarly be utilized in Neural Network that comprises shortcut Connections .

Referring to an exemplary embodiment of learning Rounds of Conversational Exchange using Graph is illustrated. In some aspects any Round of Conversational Exchange can be connected with any other Round of Conversational Exchange in Graph . In other aspects any Round of Conversational Exchange can be connected with itself and or any other Round of Conversational Exchange in Graph . In some embodiments Knowledge Structuring Unit creates or generates Rounds of Conversational Exchange and the system applies them onto Graph thereby implementing learning of Rounds of Conversational Exchange . The system can perform Substantial Similarity Comparisons of a Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Graph . If a substantially similar Round of Conversational Exchange is not found in Graph the system may insert i.e. copy store etc. the Round of Conversational Exchange from Knowledge Structuring Unit into Graph and create a Connection to the inserted Round of Conversational Exchange from a prior Round of Conversational Exchange including assigning an occurrence count to the new Connection calculating a weight of the new Connection and updating any other Connections originating from the prior Round of Conversational Exchange . On the other hand if a substantially similar Round of Conversational Exchange is found in Graph the system may update occurrence count and weight of Connection to that Round of Conversational Exchange from a prior Round of Conversational Exchange and update any other Connections originating from the prior Round of Conversational Exchange . Any of the previously described and or other techniques for comparing inserting updating and or other operations on Rounds of Conversational Exchange Connections and or other elements can similarly be utilized in Graph

For example the system can perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Graph . In the case that a substantially similar match is not found the system may insert Round of Conversational Exchange into Graph and copy Round of Conversational Exchange into the inserted Round of Conversational Exchange . The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Graph . In the case that a substantially similar match is found between Round of Conversational Exchange and Round of Conversational Exchange the system may create Connection between Round of Conversational Exchange and Round of Conversational Exchange with occurrence count of 1 and weight of 1. The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Graph . In the case that a substantially similar match is found between Round of Conversational Exchange and Round of Conversational Exchange the system may update occurrence count and weight of Connection between Round of Conversational Exchange and Round of Conversational Exchange and update weights of other outgoing Connections one in this example originating from Round of Conversational Exchange as previously described. The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Graph . In the case that a substantially similar match is not found the system may insert Round of Conversational Exchange into Graph and copy Round of Conversational Exchange into the inserted Round of Conversational Exchange . The system may also create Connection between Round of Conversational Exchange and Round of Conversational Exchange with occurrence count of 1 and weight calculated based on the occurrence count as previously described. The system may also update weights of other outgoing Connections one in this example originating from Round of Conversational Exchange as previously described. The system can then perform Substantial Similarity Comparisons of Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Graph . In the case that a substantially similar match is not found the system may insert Round of Conversational Exchange into Graph and copy Round of Conversational Exchange into the inserted Round of Conversational Exchange . The system may also create Connection between Round of Conversational Exchange and Round of Conversational Exchange with occurrence count of 1 and weight of 1. Applying any additional Rounds of Conversational Exchange from Knowledge Structuring Unit onto Graph follows similar logic or process as the above described.

Referring to an exemplary embodiment of learning Rounds of Conversational Exchange using Collection of Sequences is illustrated. Collection of Sequences comprises the functionality for storing one or more Sequences . Sequence comprises the functionality for storing multiple Rounds of Conversational Exchange and or other elements. Connections can be used to link Rounds of Conversational Exchange or can be optionally omitted in some implementations of Sequence . In some aspects each Sequence in a Collection of Sequences may include Rounds of Conversational Exchange of an entire conversation. For example Knowledge Structuring Unit creates or generates Rounds of Conversational Exchange and the system applies them onto Collection of Sequences thereby implementing learning of Rounds of Conversational Exchange . The system can perform Substantial Similarity Comparisons of Rounds of Conversational Exchange from Knowledge Structuring Unit with corresponding Rounds of Conversational Exchange in Sequences stored in Collection of Sequences to find a Sequence comprising Rounds of Conversational Exchange that are substantially similar to the Rounds of Conversational Exchange from Knowledge Structuring Unit . If Sequence comprising such substantially similar Rounds of Conversational Exchange is not found in Collection of Sequences the system may create a new Sequence comprising the Rounds of Conversational Exchange from Knowledge Structuring Unit and insert i.e. copy store etc. the new Sequence into Collection of Sequences . On the other hand if Sequence comprising substantially similar Rounds of Conversational Exchange is found in Collection of Sequences the system may optionally omit inserting the Rounds of Conversational Exchange from Knowledge Structuring Unit into Collection of Sequences as inserting a similar Sequence may not add much or any additional knowledge. This approach can save storage resources and limit the number of Rounds of Conversational Exchange that may later need to be processed or compared. In other aspects each Sequence in a Collection of Sequences may include Rounds of Conversational Exchange of a part of a conversation. A similar learning process as the above described can be utilized in such implementations. In further aspects one or more long Sequences each including Rounds of Conversational Exchange of multiple conversations can be utilized. In one example Rounds of Conversational Exchange of all conversations can be stored in a single long Sequence in which case Collection of Sequences as a separate element can be omitted. In another example Rounds of Conversational Exchange of multiple conversations can be included in a plurality of long Sequences such as daily weekly monthly yearly or other periodic or other Sequences . Substantial Similarity Comparisons can be performed by traversing the one or more long Sequences to find a match or substantially similar match. For example the system can perform Substantial Similarity Comparisons of Rounds of Conversational Exchange from Knowledge Structuring Unit with corresponding Rounds of Conversational Exchange in subsequences of a long Sequence in incremental or other traversing pattern to find a subsequence comprising Rounds of Conversational Exchange that are substantially similar to the Rounds of Conversational Exchange from Knowledge Structuring Unit . The incremental traversing pattern may start from one end of a long Sequence and move the comparison subsequence up or down one i.e. or any amount etc. incremental Round of Conversational Exchange at a time. Other traversing patterns or methods can be employed such as starting from the middle of the Sequence and subdividing the resulting sub sequences in a recursive pattern or any other traversing pattern or method. If a subsequence comprising substantially similar Rounds of Conversational Exchange is not found in the long Sequence Knowledge Structuring Unit may concatenate or append the Rounds of Conversational Exchange from Knowledge Structuring Unit to the long Sequence . In further aspects a Round of Conversational Exchange in a single Sequence can be connected not only with a next Round of Conversational Exchange in the Sequence but also with any other Round of Conversational Exchange thereby creating alternate routes or shortcuts through the Sequence . Any number of Connections connecting any Rounds of Conversational Exchange can be utilized. For example Knowledge Structuring Unit creates or generates Rounds of Conversational Exchange and the system applies them onto a single Sequence thereby implementing learning of Rounds of Conversational Exchange . The system can perform Substantial Similarity Comparisons of a Round of Conversational Exchange from Knowledge Structuring Unit with Rounds of Conversational Exchange in Sequence . If a substantially similar Round of Conversational Exchange is not found in Sequence the system may concatenate or append the Round of Conversational Exchange from Knowledge Structuring Unit to the Sequence and create i.e. including assigning occurrence count and weight etc. a Connection to that Round of Conversational Exchange from a prior Round of Conversational Exchange . On the other hand if a substantially similar Round of Conversational Exchange is found in Sequence the system may create i.e. including assigning occurrence count and weight etc. a Connection to that Round of Conversational Exchange from a prior Round of Conversational Exchange if the Connection did not exist. Otherwise the system may update i.e. update occurrence count and weight etc. the Connection if the Connection existed. Any of the previously described and or other techniques for comparing inserting updating and or other operations on Rounds of Conversational Exchange Connections and or other elements can similarly be utilized in Sequences .

Any of the previously described arrangements of Rounds of Conversational Exchange such as Neural Network Graph Collection of Sequences Sequence and or others can be used alone or in combination with each other or with other elements in alternate embodiments. In one example a path in Neural Network or Graph may include its own separate sequence of Rounds of Conversational Exchange that are not interconnected with Rounds of Conversational Exchange in other paths. In another example a portion of a path in Neural Network or Graph may include a sequence of Rounds of Conversational Exchange interconnected with Rounds of Conversational Exchange in other paths whereas another portion of the path may include its own separate sequence of Rounds of Conversational Exchange that are not interconnected with Rounds of Conversational Exchange in other paths. Any other combinations or arrangements of Rounds of Conversational Exchange can be implemented.

Referring now to some embodiments of System for Learning AIIMs System for Learning AIIMs can be implemented to learn only speech or sounds of Conversation Participants in which case the elements for processing pictures can be omitted. Such System for Learning AIIMs that learns only speech or sounds can be utilized in any situation where a Picture capturing Device may not be available or where visual input is undesirable. Examples of such situations include situations where people prefer not to be visually recorded situations involving verbal communication systems with no visual input i.e. telephone systems etc. and or others.

Referring to some embodiments of System for Learning AIIMs one or more Conversation Participants may be non human Conversation Participants . A non human Conversation Participant may include any device apparatus or system comprising conversational functionalities such as a humanoid or other robot conversation participant simulated on a computing device and or others. System for Learning AIIMs can learn conversations among a human Conversation Participant and a non human Conversation Participant . As the human Conversation Participant and the non human Conversation Participant exchange verbal and visual expressions or communication in a conversation System for Learning AIIMs may capture and learn these conversational exchanges as previously described. In some aspects a non human Conversation Participant may be configured to guide a conversation with a human Conversation Participant to enhance the learning effectiveness of System for Learning AIIMs . For example the non human Conversation Participant may perform specific conversational activities i.e. ask specific questions make specific facial expressions etc. to cause desired conversational activities i.e. specific statements specific facial expressions etc. to be performed by the human Conversation Participant . This way System for Learning AIIMs can learn or be trained to learn targeted or desirable verbal and visual expressions or communication of the human Conversation Participant in a controlled process. A similar process can be implemented with a human Conversation Participant who is instructed to guide a conversation with another human Conversation Participant .

Referring to the illustration shows an embodiment of a method for learning AIIMs. The method can be used on a computing device or system to enable learning of conversations among two or more conversation participants. The computing device or system may include a dedicated device a user device i.e. User Device etc. a server i.e. Server etc. a host device i.e. Host Device etc. or an embedded element thereof and or others. Method may include any action or operation of any of the disclosed methods such as method and or others. Other additional steps actions or operations can be included as needed or some of the disclosed ones can be optionally omitted or a different combination or order thereof can be implemented in alternate embodiments of method .

At step a stream of digital pictures of a first conversation participant is captured. A stream of digital pictures i.e. Stream of Digital Pictures etc. may include a conversation participant s i.e. Conversation Participant s etc. visual expressions or communication. In some embodiments a stream of digital pictures comprises any type or form of digital motion picture such as MPEG AVI FLV MOV RM SWF WMV DivX and or other digitally encoded motion picture. In other embodiments a stream of digital pictures comprises any type or form of digital pictures such as digital bitmaps JPEG pictures GIF pictures TIFF pictures PDF pictures and or other digitally encoded pictures. In yet other embodiments a stream of digital pictures comprises any computer generated pictures such as views of a 3D game 3D application or CAD CAM application captured or rendered as a stream of digital pictures. In further embodiments a stream of digital pictures comprises any application or process that can generate a stream of digital pictures or other visual content. A stream of digital pictures comprising a conversation participant s visual expressions or communication can be captured by a picture capturing device i.e. Picture capturing Device etc. such as a motion or still picture camera or other picture capturing device. In some aspects a picture capturing device may be part of a device dedicated to implementing AIIM learning functionalities. In other aspects a picture capturing device may be part of a user device that is connected via a network to a server implementing AIIM learning functionalities. In further aspects a picture capturing device may be part of a host device whose embedded element implements AIIM learning functionalities. Picture capturing device may be provided in any other device system or configuration. In some aspects a plurality of picture capturing devices can be utilized each dedicated to capturing visual expressions or communication of a single conversation participant. In other aspects a single picture capturing device may capture visual expressions or communication of a plurality of conversation participants. Capturing comprises any action or operation by or for a Picture capturing Device Stream of Digital Pictures and or other disclosed elements.

At step a stream of digital sound samples of the first conversation participant is captured. A stream of digital sound samples i.e. Stream of Digital Sound Samples etc. may include a conversation participant s verbal expressions or communication. In some embodiments a stream of digital sound samples comprises any type or form of digital sound such as WAV WMA AIFF MP3 RA OGG and or other digitally encoded sound. In other embodiments a stream of digital sound samples comprises any computer generated stream of digital sound samples such as synthesized sound. In further embodiments a stream of digital sound samples comprises any application or process that can generate a stream of digital sound samples or other audio content. In some designs stream of digital sound samples is captured simultaneously with the aforementioned stream of digital pictures and vice versa. Stream of digital sound samples may therefore be associated with or correspond to a stream of digital pictures. A stream of digital sound samples comprising a conversation participant s verbal expressions or communication can be captured by a sound capturing device i.e. Sound capturing Device etc. such as a microphone or other sound capturing device. In some aspects a sound capturing device may be part of a device dedicated to implementing AIIM learning functionalities. In other aspects a sound capturing device may be part of a user device that is connected via a network to a server implementing AIIM learning functionalities. In further aspects a sound capturing device may be part of a host device whose embedded element implements AIIM learning functionalities. Sound capturing device may be provided in any other device system or configuration. In some aspects a plurality of sound capturing devices can be utilized each dedicated to capturing verbal expressions or communication of a single conversation participant. In other aspects a single sound capturing device may capture verbal expressions or communication of a plurality of conversation participants. Capturing comprises any action or operation by or for a Sound capturing Device Stream of Digital Sound Samples and or other disclosed elements.

At step a stream of digital pictures of a second conversation participant is captured. Step may include any action or operation described in Step as applicable. Step may be performed concurrently with Step and or .

At step a stream of digital sound samples of the second conversation participant is captured. Step may include any action or operation described in Step as applicable. Step may be performed concurrently with Step and or .

At step the first conversation participant s first conversational activity is detected from at least one of the stream of digital pictures of the first conversation participant or the stream of digital sound samples of the first conversation participant. Examples of conversational activities include speaking silent facial expressions silent body movements motionless silence absence from the conversation and or others. Detecting conversational activities of a conversation participant may include processing either or both a stream of digital pictures comprising the conversation participant s visual expressions or communication and or a stream of digital sound samples comprising the conversation participant s verbal expressions or communication. A stream of digital pictures may include visual expressions or communication of a single conversation participant or a plurality of conversation participants. Similarly a stream of digital sound samples may include verbal expressions or communication of a single conversation participant or a plurality of conversation participants. In one example a conversation participant s speaking activity can be detected by recognizing the conversation participant s speech in a stream of digital sound samples. Specifically for instance a beginning of a conversation participant s speaking activity can be determined by recognizing the conversation participant s speech in a stream of digital sound samples after a threshold period of silence i.e. no speech or sound etc. . Further an end of a conversation participant s speaking activity can be determined by recognizing a threshold period of silence in a stream of digital sound samples after the conversation participant s speech. In another example a conversation participant s silent facial expressions activity can be detected by recognizing the conversation participant s facial expressions i.e. smiling lifting eyebrows etc. in a stream of digital pictures and by recognizing the conversation participant s silence i.e. no speech or sound etc. in a stream of digital sound samples. In another example a conversation participant s silent body movements activity can be detected by recognizing the conversation participant s body movements i.e. nodding head shaking head shrugging shoulders pointing finger pointing fist etc. in a stream of digital pictures and by recognizing the conversation participant s silence i.e. no speech or sound etc. in a stream of digital sound samples. In a further example a conversation participant s motionless silence activity can be detected by recognizing no or marginal motion i.e. no facial change no body movement etc. of the conversation participant in a stream of digital pictures and by recognizing the conversation participant s silence i.e. no speech or sound etc. in a stream of digital sound samples. For instance marginal motion of a conversation participant may include comparing one picture of a stream of digital pictures with another i.e. subsequent etc. picture of the stream of digital pictures and determining that a number or percentage difference between regions of the two pictures comprising the conversation participant does not exceed a threshold. In a further example a conversation participant s absence from the conversation activity can be detected by recognizing the conversation participant s absence i.e. conversation participant missing from the field of view etc. in a stream of digital pictures and or by recognizing the conversation participant s silence i.e. no speech or sound etc. in a stream of digital sound samples. In some aspects detecting a conversational activity may include comparing collections of sound samples of a stream of digital sound samples with collections of known sound samples. In further aspects detecting a conversational activity may include comparing features of a stream of digital sound samples with features of known sounds. For example acoustic features can be used for classification of non verbal vocal outbursts such as laughter or sighs whereas linguistic features can be used to transcribe the linguistic message such as words phrases or sentences. In other aspects detecting a conversational activity may include comparing regions of pixels from one or more pictures i.e. frames etc. of a stream of digital pictures with collections of pixels comprising known persons objects and or their activities. In other aspects detecting a conversational activity may include comparing features i.e. lines edges ridges corners blobs regions etc. from one or more pictures i.e. frames etc. of a stream of digital pictures with features of known persons objects and or their activities. In one example facial recognition involves identifying and or analyzing facial features such as the relative position size and or shape of the eyes nose cheekbones jaw etc. which may then be used to search for pictures with matching features. In further aspects detecting any new conversational activity may mark an end to a previously detected conversational activity. In some designs detecting a conversational activity of a conversation participant may include recognizing the conversation participant s visual expressions or communication in a part of a conversation. Similarly detecting a conversational activity of a conversation participant may include recognizing the conversation participant s verbal expressions or communication in a part of a conversation. In other designs detecting a conversational activity of a conversation participant may include identifying a sub stream of digital pictures i.e. Sub stream of Digital Pictures etc. in a stream of digital pictures the sub stream of digital pictures comprising the conversation participant s visual expressions or communication in a part of a conversation. Similarly detecting a conversational activity of a conversation participant may include identifying a sub stream of digital sound samples i.e. Sub stream of Digital Sound Samples etc. in a stream of digital sound samples the sub stream of digital sound samples comprising the conversation participant s verbal expressions or communication in a part of a conversation. Detecting a conversational activity of a conversation participant may also include creating or generating a recording or data structure of the conversational activity i.e. Conversational Activity also sometimes referred to simply as conversational activity etc. that comprises one or more sub streams of digital pictures one or more sub streams of digital sound samples and or other data i.e. Extra Info etc. . In further designs detecting a conversational activity of a conversation participant may include extracting or filtering persons and or objects of interest i.e. conversation participant s face etc. from a stream of digital pictures or sub stream of digital pictures. Similarly detecting a conversational activity of a conversation participant may include extracting or filtering speech and or sounds of interest i.e. conversation participant s speech and or sounds etc. from a stream of digital sound samples or sub stream of digital sound samples. Such extractions or filtering can optionally be performed as part of another step or in their own separate steps in alternate embodiments. Detecting comprises any action or operation by or for an Activity Detector Picture Recognizer Speech Sound Recognizer Sub stream of Digital Pictures Sub stream of Digital Sound Samples Conversational Activity and or other disclosed elements.

At step the second conversation participant s first conversational activity is detected from at least one of the stream of digital pictures of the second conversation participant or the stream of digital sound samples of the second conversation participant. Step may include any action or operation described in Step as applicable.

At step a first round of conversational exchange including a recording of the first conversation participants first conversational activity and a recording of the second conversation participant s first conversational activity is generated. A round of conversational exchange i.e. Round of Conversational Exchange etc. may include one or more recordings of one conversation participant s conversational activities i.e. Conversational Activities etc. and one or more recordings of another conversation participant s conversational activities. A round of conversational exchange may also include one or more recordings of additional conversation participant s conversational activities. A recording of one conversation participant s conversational activity may be correlated with a recording of another conversation participant s conversational activity. In effect a round of conversational exchange includes a unit of knowledge of how one conversation participant acted relative to another conversation participant and vice versa in a part of a conversation. In one example the first conversation participant may speak while the second conversation participant observes i.e. silent facial expressions silent body movements motionless silence etc. in a part of a conversation therefore a round of conversational exchange may include a recording of the first conversation participant s speaking conversational activity correlated with a recording of the second conversation participant s silent facial expressions conversational activity. In another example both conversation participants may observe in a part of a conversation therefore a round of conversational exchange may include a recording of the first conversation participant s silent body movements conversational activity correlated with a recording of the second conversation participant s silent facial expressions conversational activity. In a further example both conversation participants may speak in a part of a conversation therefore a round of conversational exchange may include a recording of the first conversation participant s speaking conversational activity correlated with a recording of the second conversation participant s speaking conversational activity. A variety of arrangements of recordings of conversational activities and or other data i.e. Extra Info etc. can be stored in rounds of conversational exchange. In some aspects the timing of recordings of conversational activities of different conversation participants may coincide partially coincide or overlap. In further aspects the number of recordings of conversational activities of one conversation participant may equal or differ from the number of recordings of conversational activities of another conversation participant. In general a round of conversational exchange may include any number types timing and or other properties of recordings of conversational activities of any number of conversation participants arranged in any conceivable combination. Generating comprises any action or operation by or for a Knowledge Structuring Unit and or other disclosed elements.

At step the first round of conversational exchange is stored the first round of conversational exchange being part of a stored plurality of rounds of conversational exchange. Rounds of conversational exchange can be stored in a memory unit or other repository. Once created or generated rounds of conversational exchange can be used in as neurons nodes vertices or other elements in any of the data or knowledge structures arrangements i.e. neural networks graphs sequences etc. used for facilitating learning functionalities herein. Rounds of conversational exchange may be connected interrelated or interlinked into knowledge structures using statistical artificial intelligence machine learning and or other models or techniques. Such interconnected or interrelated rounds of conversational exchange can be used for simulating a conversation with a person i.e. artificially intelligent conversation participant etc. in the person s absence after the person is deceased or in any situation where a simulation of a conversation with the person may be needed. The interconnected rounds of conversational exchange may be stored or organized into a knowledgebase i.e. Knowledgebase etc. . In some embodiments knowledgebase may be or include a neural network i.e. Neural Network etc. . In other embodiments knowledgebase may be or include a graph i.e. Graph etc. . In further embodiments knowledgebase may be or include a collection of sequences i.e. Collection of Sequences etc. . In further embodiments knowledgebase may be or include a sequence i.e. Sequence etc. . In general knowledgebase may be or include any data structure knowledge structure or repository capable of storing the knowledge of one or more conversations and or other data. Storing comprises any action or operation by or for a Knowledgebase Neural Network Graph Collection of Sequences Sequence Node Layer Connection Substantial Similarity Comparison and or other disclosed elements.

Referring to an embodiment of System for Using AIIMs implemented on User Device is illustrated. System for Using AIIMs can also be implemented in any computing device or system instead of or in addition to User Device . In one example System for Using AIIMs can be implemented in a dedicated device that can be owned by someone or given as a present to someone to simulate conversations with his her favorite person. In another example System for Using AIIMs can be embedded into Host Device such as a television device a set top box a disc or other media player i.e. DVD or Blue ray player etc. a gaming device i.e. Microsoft Xbox Sony PlayStation etc. a smartphone i.e. Apple iPhone Samsung Galaxy etc. a mobile computer i.e. tablet or laptop computer etc. a still or motion picture camera and or others.

Referring to an embodiment of internal structure of System for Using AIIMs implemented on User Device is illustrated. System for Using AIIMs comprises interconnected Decision making Unit Knowledgebase and Activity Detector . Some embodiments of Activity Detector may include Picture Recognizer and or Speech Sound Recognizer . System for Using AIIMs may be part of User Device . System for Using AIIMs may utilize User Device s Picture capturing Device Sound capturing Device Display and Sound producing Device to implement its functionalities. Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments.

System for Using AIIMs comprises any hardware programs or a combination thereof. System for Using AIIMs comprises the functionality for simulating a conversation. System for Using AIIMs comprises the functionality for simulating a conversation based on knowledge of one or more conversations stored in Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data structure knowledge structure or repository. System for Using AIIMs comprises the functionality for simulating verbal visual and or other expressions or communication that resemble a person s conversational style or character. System for Using AIIMs comprises the functionality for simulating a conversation with one or more simulated persons. As such System for Using AIIMs enables a simulation of a conversation among User and one or more Artificially Intelligent Conversation Participants also referred to as AI Conversation Participants etc. . System for Using AIIMs can therefore be used to simulate a conversation with a person in the person s absence after the person is deceased or in any situation where a simulation of a conversation with the person may be needed. For example System for Using AIIMs can be used to simulate a conversation with a parent predecessor i.e. grandparent etc. spouse friend historical figure famous person i.e. scientist athlete actor musician politician etc. and or other persons. A person can even talk to an artificially intelligent interactive version of him herself. System for Using AIIMs also comprises other functionalities disclosed herein.

In one example the teaching presented by the disclosure can be implemented in a device or system for using AIIMs. The device or system may include one or more processor circuits. The device or system may further include a memory unit coupled to the one or more processor circuits that stores a plurality of rounds of conversational exchange including a first round of conversational exchange the first round of conversational exchange comprising a recording of a first conversation participant s first conversational activity and a recording of a second conversation participant s first conversational activity. The device or system may further include a picture capturing device coupled to the one or more processor circuits configured to capture a stream of digital pictures of a user. The device or system may further include a sound capturing device coupled to the one or more processor circuits configured to capture a stream of digital sound samples of the user. The one or more processor circuits may be configured to detect the user s first conversational activity from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. The one or more processor circuits may also be configured to compare at least one portion of a recording of the user s first conversational activity with at least one portion of the recording of the first conversation participant s first conversational activity. The one or more processor circuits may also be configured to determine that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. The one or more processor circuits may also be configured to cause a display and a sound producing device to play at least one portion of the recording of the second conversation participant s first conversational activity. Any of the operations of the described elements can be performed repeatedly and or in different orders in alternate embodiments. In some aspects the one or more processor circuits the memory unit the picture capturing device the sound capturing device the display and the sound producing device are part of a single device. In other aspects at least one of the one or more processor circuits or the memory unit are part of a server whereas the picture capturing device the sound capturing device the display and the sound producing device are part of a user device the user device coupled to the server via a network. Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments. The device or system for using AIIMs can also include any actions or operations of any of the disclosed methods such as methods and or later described .

User also referred to simply as user etc. can be a human user. In one example User can be Conversation Participant whose conversations with Conversation Participant have been learned by System for Learning AIIMs and who wishes to simulate a conversation with Conversation Participant via System for Using AIIMs . In another example User can be any person who wishes to simulate a conversation via System for Using AIIMs . In some aspects User can be a non human User . The non human User may include any device apparatus or system comprising conversational functionalities such as a humanoid or other robot user simulated on a computing device and or others.

Display comprises the functionality for displaying visual information and or other functionalities. Examples of a typical Display include a liquid crystal display LCD cathode ray tube CRT display and or other display. In some aspects Display may include a projector a hologram producing device and or other device for displaying visual information. In further aspects Display may be placed on the front of a robot s head to simulate a face. In further aspects instead using Display the learned verbal and or visual information can be transformed into physical movements of a robot s face i.e. mouth eyes etc. and or other robot s body parts thereby simulating a conversation with a physical artificially intelligent conversation participant. Such movements can be implemented by engaging motors or actuators in the robot s face or other robot s body parts.

Sound producing Device comprises the functionality for producing sound and or other functionalities. Examples of Sound producing Device include a built in or an external speaker headphone and or other sound producing device.

Decision making Unit comprises the functionality for determining activities i.e. Conversational Activities etc. of AI Conversation Participant and or other functionalities. Decision making Unit comprises functions rules and or logic to determine or anticipate which Conversational Activity is most suitable or likely to be used or implemented by AI Conversation Participant in a simulated conversation. Similarly Decision making Unit comprises the functionality to determine which Conversational Activity is second most suitable or likely to be used or implemented which Conversational Activity is third most suitable or likely to be used or implemented and so on. Furthermore Decision making Unit comprises the functionality to determine a sequence or order in which Conversational Activities are most suitable or likely to be used or implemented by AI Conversation Participant in a simulated conversation. In some aspects Decision making Unit may determine Conversational Activities of AI Conversation Participant by performing Substantial Similarity Comparisons of Conversational Activities from Activity Detector with Conversational Activities from Rounds of Conversational Exchange stored in Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data structure knowledge structure or repository. A Round of Conversational Exchange includes a unit of knowledge i.e. correlated Conversational Activities etc. of how one Conversation Participant acted relative to another Conversation Participant and vice versa in a part of a conversation as previously described. When Conversational Activities with similar content structure and or other properties are detected involving User in the future the learned Conversational Activities of one or more Conversation Participants stored in Rounds of Conversational Exchange can be determined or anticipated by Decision making Unit thereby simulating a conversation with one or more AI Conversation Participants .

In some embodiments Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity or portion thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data structure knowledge structure or repository also referred to as Knowledgebase etc. . In some implementations where Rounds of Conversational Exchange similar to the one shown in are used if a substantially or otherwise similar Conversational Activity or portion thereof is found in a Round of Conversational Exchange from Knowledgebase a concurrent Conversational Activity or portion thereof of AI Conversation Participant can be anticipated in a correlated Conversational Activity or portion thereof from the Round of Conversational Exchange . Also subsequent Conversational Activity or portion thereof of AI Conversation Participant can be anticipated in a Conversational Activity or portion thereof from Round of Conversational Exchange connected with the prior Round of Conversational Exchange . In some implementations where Rounds of Conversational Exchange similar to the one shown in are used if a substantially or otherwise similar Conversational Activity or portion thereof is found in a Round of Conversational Exchange from Knowledgebase a concurrent Conversational Activity or portion thereof of AI Conversation Participant can be anticipated in a correlated Conversational Activity or portion thereof from the Round of Conversational Exchange . Also subsequent Conversational Activity or portion thereof of AI Conversation Participant can be anticipated in a subsequent Conversational Activity or portion thereof from the Round of Conversational Exchange .

Decision making Unit can utilize various elements and or techniques for selecting a path of Rounds of Conversational Exchange or Conversational Activities therein through Neural Network for example. Although these elements and or techniques are described using Neural Network below they can similarly be used in any Knowledgebase Graph Collection of Sequences Sequence and or other data structure or repository.

In some embodiments Decision making Unit can utilize similarity index in selecting Rounds of Conversational Exchange or Conversational Activities therein in a path through Neural Network . Similarity index may indicate how well a Conversational Activity or portion thereof is matched with another Conversational Activity or portion thereof as previously described. Substantial Similarity Comparison can be used to generate a similarity index whether substantial or other similarity of the compared Conversational Activities or portions thereof is achieved or not as previously described. In one example Decision making Unit may select a Round of Conversational Exchange comprising Conversational Activity with the highest similarity index even if Connection pointing to that Round of Conversational Exchange has less than the highest weight. Therefore similarity index or other such element or parameter can override or disregard the weight of a Connection or other element. In another example Decision making Unit may select a Round of Conversational Exchange comprising Conversational Activity whose similarity index is higher than or equal to a weight of Connection pointing to that Round of Conversational Exchange . In a further example Decision making Unit may select a Round of Conversational Exchange comprising Conversational Activity whose similarity index is lower than or equal to a weight of Connection pointing to that Round of Conversational Exchange . Therefore similarity index can be set to be more less or equally important than a weight of a Connection . In some aspects a minimum similarity index or threshold can be set for a Conversational Activity . In other aspects different minimum similarity indexes or thresholds can be set for different Conversational Activities . Minimum similarity indexes or thresholds can also be set for any other elements such as Rounds of Conversational Exchange Layers and or other elements. For example a higher minimum similarity index or threshold can be set for lower numbered Layers and decreased for the remaining Layers . Any other settings of a minimum similarity index can be utilized in alternate embodiments.

In other embodiments Decision making Unit can utilize Connections in selecting Rounds of Conversational Exchange or Conversational Activities therein in a path through Neural Network . Decision making Unit can take into account weights of Connections among the interconnected Rounds of Conversational Exchange in choosing from which Round of Conversational Exchange to compare a Conversational Activity first second third and so on. Specifically for instance Decision making Unit can perform Substantial Similarity Comparison with Conversational Activity from Round of Conversational Exchange pointed to by the highest weight Connection first Conversational Activity from Round of Conversational Exchange pointed to by the second highest weight Connection second and so on. In some aspects Decision making Unit can stop performing Substantial Similarity Comparisons in a particular Layer as soon as it finds a substantially similar Conversational Activity from an interconnected Round of Conversational Exchange . In other aspects Decision making Unit may only follow the highest weight Connection to arrive at a Round of Conversational Exchange comprising Conversational Activity to be compared thereby disregarding Connections with less than the highest weight.

In further embodiments Decision making Unit can utilize a bias to adjust similarity index weight of a Connection and or other element or parameter used in selecting Rounds of Conversational Exchange or Conversational Activities therein in a path through Neural Network . In one example Decision making Unit may select a Round of Conversational Exchange comprising Conversational Activity whose similarity index multiplied by or adjusted for a bias is higher than or equal to a weight of Connection pointing to that Round of Conversational Exchange . In another example Decision making Unit may select a Round of Conversational Exchange comprising Conversational Activity whose similarity index multiplied by or adjusted for a bias is lower than or equal to a weight of Connection pointing to that Round of Conversational Exchange . In a further example bias can be used to resolve deadlock situations where similarity index is equal to a weight of a Connection . In some aspects bias can be expressed in percentages such as 0.3 percent 1.2 percent 25.7 percent 79.8 percent 99.9 percent 100.1 percent 155.4 percent 298.6 percent 1105.5 percent and so on. For example a bias below 100 percent decreases an element or parameter to which it is applied a bias equal to 100 percent does not change the element or parameter to which it is applied and a bias higher than 100 percent increases the element or parameter to which it is applied. In general any amount of bias can be utilized. Bias can be applied to one or more of a weight of a Connection similarity index any other element or parameter and or al or any combination of them. Also different biases can be applied to each of a weight of a Connection similarity index or any other element or parameter. For example 30 percent bias can be applied to similarity index and 15 percent bias can be applied to a weight of a Connection . Also different biases can be applied to various Layers of Neural Network and or other disclosed elements. Bias can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input.

Any other element and or technique can be utilized in selecting Rounds of Conversational Exchange or Conversational Activities therein in a path through Neural Network

Referring to an embodiment of internal structure of System for Using AIIMs implemented as a network service is illustrated. System for Using AIIMs comprises interconnected Decision making Unit Knowledgebase and Activity Detector . Some embodiments of Activity Detector may include Picture Recognizer and or Speech Sound Recognizer . System for Using AIIMs or any element thereof may reside or operate on Server which is accessible by User Device over Network . User Device comprises Picture capturing Device Sound capturing Device Display and Sound producing Device . Other additional elements can be included as needed or some of the disclosed ones can be excluded or a combination thereof can be utilized in alternate embodiments.

In some embodiments System for Using AIIMs operating on Server can use knowledge i.e. Knowledgebase etc. of conversations among Conversation Participants learned by System for Learning AIIMs that itself may be operating on Server as previously described. Conversation Participant or any other user can utilize System for Using AIIMs operating on Server to simulate a conversation with AI Conversation Participant on his her User Device via Network . As such System for Using AIIMs implemented as a network service may be available to members of the network service i.e. membership or subscription based network service etc. or to all the world s Users i.e. freely available network service etc. who wish to simulate conversations.

Referring to an exemplary embodiment of selecting a path of Rounds of Conversational Exchange or Conversational Activities therein through Neural Network is illustrated. Neural Network may include knowledge i.e. interconnected Rounds of Conversational Exchange etc. of one or more conversations between Conversation Participants and . In this example Round of Conversational Exchange comprises a Conversational Activity of Conversation Participant correlated with a Conversational Activity of Conversation Participant similar to the one shown in . User may be the same person as Conversation Participant or any other person. The conversation is simulated with AI Conversation Participant who uses knowledge of Conversation Participant stored in Neural Network to resemble Conversation Participant . Substantial Similarity Comparison can be used to determine substantial similarity of the compared Conversational Activities or portions thereof. Such substantial similarity if achieved may be used primarily for selecting a path through Neural Network whereas weight of any Connection and or other elements may be used secondarily for example. Optional ancillary Substantial Similarity Comparisons can be selectively performed where applicable i.e. with speaking Conversational Activities or portions thereof etc. to enhance decision making i.e. narrow down choices etc. as later described. As the simulated conversation progresses Decision making Unit can receive User s Conversational Activities or portions thereof from Activity Detector .

For example Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Layer or any other one or more Layers etc. . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with the highest similarity. Decision making Unit can play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. motionless silence etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. silent facial expressions etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in corresponding Layer interconnected with Round of Conversational Exchange . Conversation Participant s Conversational Activities or portions thereof from multiple Rounds of Conversational Exchange may be found substantially similar. To enhance decision making i.e. narrow down choices etc. Decision making Unit can also perform ancillary Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from the Rounds of Conversational Exchange comprising substantially similar Conversation Participant s Conversational Activities or portions thereof. Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found at least partially similar. Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. during User s Conversational Activity i.e. silent facial expressions etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in corresponding Layer interconnected with Round of Conversational Exchange . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar. Decision making Unit may follow Connection disregarding its less than highest weight and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. motionless silence etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. silent body movements etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in corresponding Layer interconnected with Round of Conversational Exchange . None of Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Layer interconnected with Round of Conversational Exchange may be found substantially similar. Decision making Unit may follow the highest weight Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. during User s Conversational Activity i.e. silent body movements etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when a determination is made that an initial similarity has not been reached as later described. Since Connection is the only connection from Round of Conversational Exchange Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Decision making Unit can implement similar logic or process for any additional Conversational Activities from Activity Detector and so on.

In some aspects Decision making Unit may perform the aforementioned ancillary Substantial Similarity Comparisons to narrow down the choice of Rounds of Conversational Exchange comprising Conversational Activities found to be substantially or otherwise similar by the main Substantial Similarity Comparisons thereby enabling a more accurate decision making. For example Decision making Unit can perform ancillary Substantial Similarity Comparisons of User s prior Conversational Activities i.e. speaking etc. or portions thereof from Activity Detector with Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange currently being processed as described in the example above. Ancillary Substantial Similarity Comparisons may be especially useful in comparing speaking Conversational Activities as Conversation Participants may repeat some of each other s words in subsequent speaking Conversational Activities . In other aspects Decision making Unit may perform ancillary Substantial Similarity Comparisons to look forward and identify subsequent similar Conversational Activities even before User s current Conversational Activity is fully received. For example while performing main Substantial Similarity Comparisons of Conversational Activities in a current Layer Decision making Unit may perform ancillary Substantial Similarity Comparisons of Conversational Activities or portions thereof from Rounds of Conversational Exchange in a subsequent Layer interconnected with Round of Conversational Exchange currently being processed. Ancillary Substantial Similarity Comparisons can be performed concurrently with main Substantial Similarity Comparisons . Concurrent and or forward looking ancillary Substantial Similarity Comparisons may enhance real time performance of AI Conversation Participant in a simulated conversation. Ancillary Substantial Similarity Comparisons can also be omitted.

The foregoing exemplary embodiment provides an example of utilizing a combination of Substantial Similarity Comparisons ancillary comparisons weights of Connections and or other elements and or techniques. It should be understood that any of these elements and or techniques can be omitted used in a different combination or used in combination with other elements and or techniques in which case the path of Rounds of Conversational Exchange or Conversational Activities therein through Neural Network would be affected accordingly. In one example Substantial Similarity Comparisons can be omitted in which case weights of Connections can be primarily utilized for deciding which Rounds of Conversational Exchange or Conversational Activities therein to select in a path through Neural Network . In another example weights of Connections can be omitted in which case Substantial Similarity Comparisons can be primarily utilized for deciding which Rounds of Conversational Exchange or Conversational Activities therein to select in a path through Neural Network . These elements and or techniques can similarly be utilized in Graph Collections of Sequences Sequence and or other data structures or repositories. Any of the previously described arrangements of Conversational Activities in a Round of Conversational Exchange and or other elements can be utilized as well. One of ordinary skill in art will understand that this exemplary embodiment is described merely as an example of a variety of possible implementations and that while all of its variations are too voluminous to describe they are within the scope of this disclosure.

Referring to an exemplary embodiment of selecting a path of Rounds of Conversational Exchange or Conversational Activities therein through Neural Network is illustrated. Neural Network may include knowledge i.e. interconnected Rounds of Conversational Exchange etc. of one or more conversations between Conversation Participants and . In this example Round of Conversational Exchange comprises a Conversational Activity of Conversation Participant correlated with Conversational Activity of Conversation Participant and a subsequent Conversational Activity of Conversation Participant correlated with a subsequent Conversational Activity of Conversation Participant similar to the one shown in . User may be the same person as Conversation Participant or any other person. The conversation is simulated with AI Conversation Participant who uses knowledge of Conversation Participant stored in Neural Network to resemble Conversation Participant . Substantial Similarity Comparison can be used to determine substantial similarity of the compared Conversational Activities or portions thereof. Such substantial similarity if achieved may be used primarily for selecting a path through Neural Network whereas weight of any Connection and or other elements may be used secondarily for example. In this example as the simulated conversation progresses Decision making Unit can be setup to receive User s speaking Conversational Activities or portions thereof from Activity Detector while other Conversational Activities from Activity Detector may be omitted.

For example Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in Layer or any other one or more Layers etc. . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent facial expressions etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in corresponding Layer interconnected with Round of Conversational Exchange . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with highest similarity. Decision making Unit may follow Connection disregarding its less than highest weight and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Since Connection is the only connection from Round of Conversational Exchange Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. motionless silence etc. during User s Conversational Activity i.e. speaking etc. . Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in corresponding Layer interconnected with Round of Conversational Exchange . None of Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from Rounds of Conversational Exchange in Layer interconnected with Round of Conversational Exchange may be found substantially similar. Decision making Unit may follow the highest weight Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent facial expressions etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when a determination is made that an initial similarity has not been reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in corresponding Layer interconnected with Round of Conversational Exchange . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with highest similarity. Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can implement similar logic or process for any additional Conversational Activities from Activity Detector and so on.

In both of the above described and or other exemplary embodiments any time that substantial similarity or other similarity threshold is not achieved in any of the compared Conversational Activities or portions thereof instead of following the highest weight Connection or the only Connection Decision making Unit can decide to look for a substantially or otherwise similar Conversational Activity or portion thereof elsewhere in Neural Network such as in any Layer subsequent to a current Layer in the first Layer in the entire Neural Network and or others.

In both of the above described and or other exemplary embodiments as the simulated conversation progresses a history i.e. sequence etc. of User s Conversational Activities or portions thereof becomes available which can then be collectively compared with Conversational Activities or portions thereof from Rounds of Conversational Exchange in paths of Neural Network . Collectively comparing Conversational Activities or portions thereof may enable Decision making Unit to more accurately determine or anticipate AI Conversation Participant s activities in the simulated conversation. For example Decision making Unit can perform collective Substantial Similarity Comparisons of a history of User s Conversational Activities or portions thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in one or more paths of Neural Network . As additional User s Conversational Activities or portions thereof from Activity Detector become available Decision making Unit can use a longer history of User s Conversational Activities or portions thereof to compare with corresponding Conversational Activities or portions thereof from Rounds of Conversational Exchange in paths of Neural Network . In each cycle of comparisons Decision making Unit may choose the most similar of the compared paths and switch to a more suitable path based on such collective similarity determinations.

The foregoing exemplary embodiment provides an example of utilizing a combination of Substantial Similarity Comparisons weights of Connections and or other elements and or techniques. It should be understood that any of these elements and or techniques can be omitted used in a different combination or used in combination with other elements and or techniques in which case the path of Rounds of Conversational Exchange or Conversational Activities therein through Neural Network would be affected accordingly. Also any of the elements and or techniques utilized in other examples or embodiments described herein such as ancillary comparisons concurrent comparisons various arrangements of Conversational Activities in a Round of Conversational Exchange and or others can similarly be utilized in this exemplary embodiment. One of ordinary skill in art will understand that this exemplary embodiment is described merely as an example of a variety of possible implementations and that while all of its variations are too voluminous to describe they are within the scope of this disclosure.

Referring to an exemplary embodiment of selecting a path of Rounds of Conversational Exchange or Conversational Activities therein through Graph is illustrated. Graph may include knowledge i.e. interconnected Rounds of Conversational Exchange etc. of one or more conversations between Conversation Participants and . In this example Round of Conversational Exchange comprises a Conversational Activity of Conversation Participant correlated with a Conversational Activity of Conversation Participant similar to the one shown in . User may be the same person as Conversation Participant or any other person. The conversation is simulated with AI Conversation Participant who uses knowledge of Conversation Participant stored in Graph to resemble Conversation Participant . Substantial Similarity Comparison can be used to determine substantial similarity of the compared Conversational Activities or portions thereof. Such substantial similarity if achieved may be used primarily for selecting a path through Graph whereas weight of any Connection and or other elements may be used secondarily for example. Optional ancillary Substantial Similarity Comparisons can be selectively performed where applicable i.e. with speaking Conversational Activities or portions thereof etc. to enhance decision making i.e. narrow down choices etc. as later described. As the simulated conversation progresses Decision making Unit can receive User s Conversational Activities or portions thereof from Activity Detector .

For example Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with the highest similarity. Decision making Unit can play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. motionless silence etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar. Decision making Unit may follow Connection disregarding its less than highest weight and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. during User s Conversational Activity i.e. motionless silence etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . None of Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange interconnected with Round of Conversational Exchange may be found substantially similar. Decision making Unit may follow the highest weight Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent facial expressions etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when a determination is made that an initial simdarity has not been reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. silent facial expressions etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . Conversation Participant s Conversational Activities or portions thereof from multiple Rounds of Conversational Exchange may be found substantially similar. To enhance decision making i.e. narrow down choices etc. Decision making Unit can also perform ancillary Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from the Rounds of Conversational Exchange in Graph comprising substantially similar Conversation Participant s Conversational Activities or portions thereof. Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found at least partially similar. Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. during User s Conversational Activity i.e. silent facial expressions etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar. Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. motionless silence etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can implement similar logic or process for any additional Conversational Activities from Activity Detector and so on.

The foregoing exemplary embodiment provides an example of utilizing a combination of Substantial Similarity Comparisons ancillary comparisons weights of Connections and or other elements and or techniques. It should be understood that any of these elements and or techniques can be omitted used in a different combination or used in combination with other elements and or techniques in which case the path of Rounds of Conversational Exchange or Conversational Activities therein through Graph would be affected accordingly. Also any of the elements and or techniques utilized in other examples or embodiments described herein such as concurrent comparisons various arrangements of Conversational Activities in a Round of Conversational Exchange and or others can similarly be utilized in this exemplary embodiment. One of ordinary skill in art will understand that this exemplary embodiment is described merely as an example of a variety of possible implementations and that while all of its variations are too voluminous to describe they are within the scope of this disclosure.

Referring to an exemplary embodiment of selecting a path of Rounds of Conversational Exchange or Conversational Activities therein through Graph is illustrated. Graph may include knowledge i.e. interconnected Rounds of Conversational Exchange etc. of one or more conversations between Conversation Participants and . In this example Round of Conversational Exchange comprises a Conversational Activity of Conversation Participant correlated with Conversational Activity of Conversation Participant and a subsequent Conversational Activity of Conversation Participant correlated with a subsequent Conversational Activity of Conversation Participant similar to the one shown in . User may be the same person as Conversation Participant or any other person. The conversation is simulated with AI Conversation Participant who uses knowledge of Conversation Participant stored in Graph to resemble Conversation Participant . Substantial Similarity Comparison can be used to determine substantial similarity of the compared Conversational Activities or portions thereof. Such substantial similarity if achieved may be used primarily for selecting a path through Graph whereas weight of any Connection and or other elements may be used secondarily for example. In this example as the simulated conversation progresses Decision making Unit can be setup to receive User s speaking Conversational Activities or portions thereof from Activity Detector while other Conversational Activities from Activity Detector may be omitted.

For example Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent facial expressions etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with highest similarity. Decision making Unit may follow Connection disregarding its less than highest weight and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange may be found substantially similar with highest similarity. Decision making Unit may follow Connection disregarding its less than highest weight and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. motionless silence etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Since Connection is the only outgoing connection from Round of Conversational Exchange Decision making Unit may follow Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can then perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities i.e. speaking etc. or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections . None of the Conversational Activities or portions thereof from one or more Rounds of Conversational Exchange in Graph interconnected with Round of Conversational Exchange by outgoing Connections may be found substantially similar. Decision making Unit may follow the highest weight Connection and play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. motionless silence etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the correlated Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when a determination is made that an initial similarity has not been reached as later described. Decision making Unit may also play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a subsequent Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. subsequent to User s Conversational Activity i.e. speaking etc. . Decision making Unit can implement similar logic or process for any additional Conversational Activities from Activity Detector and so on.

In both of the above described and or other exemplary embodiments any time that substantial similarity or other similarity threshold is not achieved in any of the compared Conversational Activities or portions thereof instead of following the highest weight Connection or the only Connection Decision making Unit can decide to look for a substantially or otherwise similar Conversational Activity or portion thereof elsewhere in Graph

In both of the above described and or other exemplary embodiments as the simulated conversation progresses a history i.e. sequence etc. of User s Conversational Activities or portions thereof becomes available which can then be collectively compared with Conversational Activities or portions thereof from Rounds of Conversational Exchange in paths of Graph . Collectively comparing Conversational Activities or portions thereof may enable Decision making Unit to more accurately determine or anticipate AI Conversation Participant s activities in the simulated conversation. For example Decision making Unit can perform collective Substantial Similarity Comparisons of a history of User s Conversational Activities or portions thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in one or more paths of Graph . As additional User s Conversational Activities or portions thereof from Activity Detector become available Decision making Unit can use a longer history of User s Conversational Activities or portions thereof to compare with corresponding Conversational Activities or portions thereof from Rounds of Conversational Exchange in paths of Graph . In each cycle of comparisons Decision making Unit may choose the most similar of the compared paths and switch to a more suitable path based on such collective similarity determinations.

The foregoing exemplary embodiment provides an example of utilizing a combination of Substantial Similarity Comparisons weights of Connections and or other elements and or techniques. It should be understood that any of these elements and or techniques can be omitted used in a different combination or used in combination with other elements and or techniques in which case the path of Rounds of Conversational Exchange or Conversational Activities therein through Graph would be affected accordingly. Also any of the elements and or techniques utilized in other examples or embodiments described herein such as ancillary comparisons concurrent comparisons various arrangements of Conversational Activities in a Round of Conversational Exchange and or others can similarly be utilized in this exemplary embodiment. One of ordinary skill in art will understand that this exemplary embodiment is described merely as an example of a variety of possible implementations and that while all of its variations are too voluminous to describe they are within the scope of this disclosure.

Referring to an exemplary embodiment of selecting a Sequence of Rounds of Conversational Exchange or Conversational Activities therein in Collection of Sequences is illustrated. Collection of Sequences may include knowledge i.e. Sequences of Rounds of Conversational Exchange etc. of one or more conversations between Conversation Participants and . A Sequence may include Rounds of Conversational Exchange of one or more conversations or parts thereof. In this example Round of Conversational Exchange comprises a Conversational Activity of Conversation Participant correlated with a Conversational Activity of Conversation Participant similar to the one shown in . User may be the same person as Conversation Participant or any other person. The conversation is simulated with AI Conversation Participant who uses knowledge of Conversation Participant stored in Collection of Sequences to resemble Conversation Participant . Collective substantial similarity of the compared Conversational Activities or portions thereof if achieved may be used primarily for selecting a Sequence in Collection of Sequences . As the simulated conversation progresses Decision making Unit can receive User s Conversational Activities or portions thereof from Activity Detector .

For example Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity i.e. speaking etc. or portion thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from corresponding Rounds of Conversational Exchange in Collection of Sequences . Conversation Participant s Conversational Activity or portion thereof from Round of Conversational Exchange in Sequence may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent body movements etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform collective Substantial Similarity Comparisons of User s Conversational Activities or portions thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from corresponding Rounds of Conversational Exchange in Collection of Sequences . Conversation Participant s Conversational Activities or portions thereof from Rounds of Conversational Exchange in Sequence may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. during User s Conversational Activity i.e. motionless silence etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform collective Substantial Similarity Comparisons of User s Conversational Activities or portions thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from corresponding Rounds of Conversational Exchange in Collection of Sequences . Conversation Participant s Conversational Activities or portions thereof from Rounds of Conversational Exchange in Sequence may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent facial expressions etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform collective Substantial Similarity Comparisons of User s Conversational Activities or portions thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from corresponding Rounds of Conversational Exchange in Collection of Sequences . Conversation Participant s Conversational Activities or portions thereof from Rounds of Conversational Exchange in Sequence may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. speaking etc. during User s Conversational Activity i.e. silent facial expressions etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can then perform collective Substantial Similarity Comparisons of User s Conversational Activities or portions thereof from Activity Detector with Conversation Participant s Conversational Activities or portions thereof from corresponding Rounds of Conversational Exchange in Collection of Sequences . Conversation Participant s Conversational Activities or portions thereof from Rounds of Conversational Exchange in Sequence may be found substantially similar with highest similarity. Decision making Unit may play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange thereby simulating AI Conversation Participant s activity i.e. silent facial expressions etc. during User s Conversational Activity i.e. speaking etc. . Playing Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of Conversation Participant s Conversational Activity from Round of Conversational Exchange can start at any time during Substantial Similarity Comparisons such as when an initial similarity is reached as later described. Decision making Unit can implement similar logic or process for any additional Conversational Activities from Activity Detector and so on.

In some embodiments various elements and or techniques can be utilized in the aforementioned collective substantial similarity determinations. In some aspects collective substantial similarity of the compared Conversational Activities can be determined based on similarities or similarity indexes of the individually compared Conversational Activities . For example an average or weighted average of similarities or similarity indexes of individually compared Conversational Activities can be used to determine collective similarity of the compared Conversational Activities . For instance to affect the weighting of the collective similarity a higher weight or importance i.e. importance index etc. can be assigned to the similarities or similarity indexes of the current and or recent Conversational Activities and decreased for Conversational Activities in the past. In another instance a higher weight or importance i.e. importance index etc. can be assigned to the similarities or similarity indexes of speaking Conversational Activities while lower weight or importance i.e. importance index etc. can be assigned to the similarities or similarity indexes of observing Conversational Activities i.e. silent facial expressions silent body movements motionless silence etc. . Any other higher or lower importance assignment can be implemented. In other aspects collective substantial similarity of the compared Conversational Activities can be determined based on similarities or similarity indexes of Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples in the compared Conversational Activities . For example an average or weighted average of similarities or similarity indexes of some or all Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples of the compared Conversational Activities can be used to determine collective similarity of the compared Conversational Activities . For instance to affect the weighting of the collective similarity a higher weight or importance i.e. importance index etc. can be assigned to the similarities or similarity indexes of Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples in the current and or recent Conversational Activities and decreased for Conversational Activities in the past. In another instance a higher weight or importance i.e. importance index etc. can be assigned to similarities or similarity indexes of Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples in speaking Conversational Activities while lower weight or importance i.e. importance index etc. can be assigned to similarities or similarity indexes of Sub streams of Digital Pictures and or Sub streams of Digital Sound Samples in observing Conversational Activities i.e. silent facial expressions silent body movements motionless silence etc. . Any other higher or lower importance assignment can be implemented. In further aspects collective substantial similarity of the compared Conversational Activities can be determined based on similarity of some or all words features sound samples and or other elements of Sub stream of Digital Sound Samples in the compared Conversational Activities . In further aspects collective substantial similarity of the compared Conversational Activities can be determined based on similarity of some or all pictures i.e. frames etc. features regions pixels and or other elements of Sub stream of Digital Pictures in the compared Conversational Activities . A threshold for collective substantial similarity can be utilized with any of the aforementioned elements and or techniques. For example collective substantial similarity of the compared Conversational Activities can be achieved if collective similarity of their elements exceeds a threshold. Such threshold can be defined by a user by AIIM system administrator or automatically by the system based on experience testing inquiry analysis synthesis or other techniques knowledge or input. Any combination of the previously described collective substantial similarity determinations or calculations can be utilized in alternate embodiments. Any other elements and or techniques can be utilized to determine or calculate collective substantial similarity in alternate embodiments. Similar elements and or techniques as the aforementioned can be used for collective similarity determinations of other compared elements such as Rounds of Conversational Exchange and or others. Collective similarity determinations may include any features functionalities and embodiments of Substantial Similarity Comparison .

In other embodiments Collection of Sequences may enable User to manually choose a particular conversation or part thereof contained in a Sequence . For example Collection of Sequences can be shown via a graphical user interface GUI and User may select a particular Sequence to implement. The system can simulate a conversation or part thereof contained in the selected Sequence and enable User to recall his her memories of that particular conversation.

The foregoing exemplary embodiment provides an example of utilizing collective substantial similarity determinations and or other elements or techniques. It should be understood that any of these elements and or techniques can be omitted used in a different combination or used in combination with other elements and or techniques in which case the choice of Sequence in Collection of Sequences would be affected accordingly. Also any of the elements and or techniques utilized in other examples or embodiments described herein such as individual Substantial Similarity Comparisons ancillary comparisons concurrent comparisons various arrangements of Conversational Activities in a Round of Conversational Exchange and or others can similarly be utilized in this exemplary embodiment. One of ordinary skill in art will understand that this exemplary embodiment is described merely as an example of a variety of possible implementations and that while all of its variations are too voluminous to describe they are within the scope of this disclosure.

Referring to an exemplary embodiment of selecting Rounds of Conversational Exchange or Conversational Activities therein in a single Sequence is illustrated. The single Sequence may include knowledge i.e. Rounds of Conversational Exchange etc. of one or more conversations or portions thereof between Conversation Participants and . In this example Round of Conversational Exchange comprises a Conversational Activity of Conversation Participant correlated with a Conversational Activity of Conversation Participant similar to the one shown in . User may be the same person as Conversation Participant or any other person. The conversation is simulated with AI Conversation Participant who uses knowledge of Conversation Participant stored in single Sequence to resemble Conversation Participant . Individual or collective substantial similarity of the compared Conversational Activities or portions thereof if achieved may be used primarily for selecting Rounds of Conversational Exchange in Sequence . As the simulated conversation progresses Decision making Unit can receive User s Conversational Activities or portions thereof from Activity Detector .

In some aspects Decision making Unit can perform the previously described individual Substantial Similarity Comparisons of User s Conversational Activities or portions thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in Sequence . Such individual Substantial Similarity Comparisons can be performed by traversing Sequence . For example Decision making Unit can perform individual Substantial Similarity Comparisons of User s Conversational Activity or portion thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange of Sequence in incremental or other traversing pattern. The incremental traversing may start from one end of Sequence and move the comparison up or down one i.e. or any amount etc. incremental Conversational Activity at a time. Other traversing patterns or methods can be employed such as starting from the middle of the Sequence and subdividing the resulting sub sequences in a recursive pattern or any other traversing pattern or method.

In other aspects Decision making Unit can perform the previously described collective Substantial Similarity Comparisons of a history i.e. a sequence itself etc. of User s Conversational Activities or portions thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in subsequences of Sequence . Such collective Substantial Similarity Comparisons can be performed by traversing Sequence . For example Decision making Unit can perform collective Substantial Similarity Comparisons of a history of User s Conversational Activities or portions thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in subsequences of Sequence in the previously described incremental recursive or other traversing pattern. As additional User s Conversational Activities or portions thereof from Activity Detector become available Decision making Unit can use a longer history of User s Conversational Activities or portions thereof to compare with Conversational Activities or portions thereof from Rounds of Conversational Exchange in subsequences of Sequence . In each cycle of comparisons Decision making Unit may choose the most similar of the compared subsequences and switch to a more suitable subsequence based on such collective similarity determinations.

In some designs a Round of Conversational Exchange can be connected not only with a next Round of Conversational Exchange in Sequence but also with any other Round of Conversational Exchange thereby creating alternate routes or shortcuts through Sequence . Any number of Connections connecting any Rounds of Conversational Exchange in Sequence can be utilized. In such implementations Decision making Unit can perform Substantial Similarity Comparisons of User s Conversational Activity or portion thereof from Activity Detector with Conversational Activities or portions thereof from Rounds of Conversational Exchange in Sequence interconnected with prior Round of Conversational Exchange by outgoing Connections for example. Decision making Unit can then follow a Connection based on similarity determinations as previously described. In alternate designs Connections can be optionally omitted from a Sequence that does not include shortcuts.

In both of the above described and or other exemplary embodiments any time that substantial similarity or other similarity threshold is not achieved in one or more of the compared Conversational Activities or portions thereof Decision making Unit can decide to look for one or more substantially or otherwise similar Conversational Activities or portions thereof in any one or more Sequences .

The foregoing exemplary embodiment provides an example of utilizing individual or collective substantial similarity determinations and or other elements or techniques. It should be understood that any of these elements and or techniques can be omitted used in a different combination or used in combination with other elements and or techniques in which case the choice of Sequence or Rounds of Conversational Exchange or Conversational Activities therein in a Sequence would be affected accordingly. Also any of the elements and or techniques utilized in other examples or embodiments described herein such as ancillary comparisons concurrent comparisons various arrangements of Conversational Activities in a Round of Conversational Exchange and or others can similarly be utilized in this exemplary embodiment. One of ordinary skill in art will understand that this exemplary embodiment is described merely as an example of a variety of possible implementations and that while all of its variations are too voluminous to describe they are within the scope of this disclosure.

Referring now to the aforementioned initial similarity determinations in some embodiments it may be desirable to implement an AI Conversation Participant s activity soon or immediately after User starts his her corresponding Conversational Activity . Decision making Unit does not need to wait to receive an entire User s Conversational Activity in order to compare it with Conversational Activities from Rounds of Conversation Exchange stored in Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data structure knowledge structure or repository. Instead Decision making Unit can perform Substantial Similarity Comparisons of portions of Conversational Activities to determine initial similarity at any time while User performs his her Conversational Activity . Such portions of Conversational Activities may include pictures i.e. frames etc. features regions pixels or other elements of Sub streams of Digital Pictures and or words features sound samples or other elements of Sub streams of Digital Sound Samples included in the Conversational Activities . For example to determine initial similarity Decision making Unit can utilize incoming words as they are received from User in real time. Specifically in this example as initial one or more words come from User Decision making Unit can perform Substantial Similarity Comparisons of these words with words of Conversation Participant s Conversational Activities from one or more Rounds of Conversational Exchange stored in Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data structure knowledge structure or repository. If a threshold for initial similarity is not achieved Substantial Similarity Comparison can use an additional word coming from User to determine initial similarity. If a threshold for initial similarity is still not achieved Substantial Similarity Comparison can use additional incoming words thereby further increasing the number of words used in the initial similarity determination until initial similarity is achieved. Similar logic or process for determining initial similarity can be implemented with incoming sound features incoming sound samples incoming pictures i.e. frames etc. incoming regions of pixels incoming picture features incoming pixels and or other elements of Sub stream of Digital Sound Samples and or Sub stream of Digital Pictures . At any point when initial similarity is determined for any of the compared Conversational Activities or portions thereof Decision making Unit can play Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of a Conversational Activity correlated with the initially similar Conversational Activity thereby simulating AI Conversation Participant s activity during User s corresponding Conversational Activity . Therefore initial similarity determination enables quickly determining a best guess of Conversational Activity to use for simulating AI Conversation Participant . Decision making Unit can switch from an initially similar Conversational Activity to a better Conversational Activity if a better initial substantial or other similarity is determined. For example after initial similarity is determined for a Conversational Activity Decision making Unit can continue performing Substantial Similarity Comparisons in attempt to find even better initial or other similarity in other Conversational Activities and if found Decision making Unit can switch to the more similar Conversational Activity for simulating AI Conversation Participant . In some aspects Decision making Unit may adjust the length of Sub stream of Digital Pictures and Sub stream of Digital Sound Samples of the initially similar Conversational Activity to synchronize their playing with User s Conversational Activity . In other aspects if initial similarity is not achieved after a threshold number of attempts or threshold period of time Decision making Unit can use weights of Connections and or alternative elements or techniques in selecting Conversational Activity for simulating AI Conversation Participant .

Referring to some embodiments of System for Using AIIMs in cases where Decision making Unit does not find a substantially or otherwise acceptably similar Conversational Activity in any of the considered Rounds of Conversational Exchange stored in Knowledgebase Neural Network Graph Collection of Sequences Sequence and or other data structure knowledge structure or repository Decision making Unit can utilize various techniques instead of or in addition to Connections in selecting a conversational path. In some aspects Decision making Unit can play a message such as I did not understand that what was that I do not recall that or other message that offers User a chance to alter the path of conversation by inputting another Conversational Activity . The redirecting message itself can be learned by System for Learning AIIMs through the previously described learning process of conversations. One or more such learned redirecting messages can be stored in a special repository dedicated to conversation redirecting functionalities. A redirecting message may include one or more Conversational Activities and or other elements. In one example System for Learning AIIMs may include a list of redirecting phrases or messages to look for such as what was that I did not understand that I don t recall that or other messages which when detected in a conversation may be learned by System for Learning AIIMs and stored in the special repository comprising redirecting messages. In another example the system may ask Conversation Participant to speak redirecting messages which System for Learning AIIMs may learn and store in the special repository. Such training session may be performed during system configuration or at any time convenient for the user. In other aspects instead of offering User a chance to alter the path of conversation as aforementioned Decision making Unit itself may attempt to redirect the conversation by playing a redirecting message such as let s talk about let me tell you you know or other message after which Decision making Unit can direct User into an alternate conversational path i.e. follow highest weight or other Connection etc. . Such redirecting message can be learned automatically or through training as previously described. In further aspects Decision making Unit can present User with a map or other representation of conversational paths comprising Rounds of Conversational Exchange and or Conversational Activities where User can manually choose which conversational path to pursue. User can choose by clicking on paths or on individual Rounds of Conversational Exchange and or Conversational Activities in a graphical or other interface.

Referring to some embodiments of System for Using AIIMs in cases where Decision making Unit runs out of conversational path Decision making Unit can utilize various techniques in redirecting to a new conversational path. In some aspects Decision making Unit can play a message such as what else would you like to talk about what else interests you how about we talk about something else or other message that offers User a chance to continue the simulated conversation by inputting another Conversational Activity . In other aspects instead of offering User a chance to continue the simulated conversation in a path that User wants Decision making Unit itself may attempt to redirect the conversation by playing a redirecting message such as let s talk about let me tell you you know or other message after which Decision making Unit can direct User into a new conversational path as previously described. Any of the aforementioned redirecting messages can be learned automatically or through training as previously described. In further aspects Decision making Unit can present User with a map or other representation of conversational paths comprising Rounds of Conversational Exchange and or Conversational Activities where User can manually choose which new conversational path to pursue as previously described.

Referring to some embodiments of System for Using AIIMs in cases where Decision making Unit needs to use Connections i.e. if substantially or otherwise similar Conversational Activity is not found etc. in a conversational path Decision making Unit can at times decide to follow a random Connection instead of following the highest weight Connection . Following a random Connection may avoid a potential issue of one or more Connections becoming so frequent and dominant that they would not allow alternative paths to be considered or selected.

Referring to some embodiments of System for Using AIIMs in cases where User starts speaking while AI Conversation Participant speaks i.e. performs a speaking activity etc. AI Conversation Participant s speaking can be interrupted and the process of finding a Conversational Activity that is substantially or otherwise similar to User s speaking Conversational Activity and implementing a correlated Conversational Activity by AI Conversation Participant can restart as previously described. The system can therefore give priority to User over AI Conversation Participant in a simulated conversation.

Referring to some embodiments of System for Using AIIMs Decision making Unit can use various transitioning techniques to switch from one Conversational Activity to another. For example such transitioning can be implemented when one AI Conversation Participant s Conversational Activity ends and another starts. Transitioning among Conversational Activities enables a simulated conversation to be perceived as smooth or uninterrupted thereby enhancing User experience. Transitioning mostly relates to visual appearance of AI Conversation Participant in a simulated conversation although sound transitioning can also be implemented. Transitioning may include a seamless visual blending of AI Conversation Participant in the last picture of a preceding Sub stream of Digital Pictures and AI Conversation Participant in the first picture of a subsequent Sub stream of Digital Pictures . In some aspects transitioning includes moving centering aligning resizing and or otherwise transforming AI Conversation Participant s figure i.e. face upper body etc. or picture within which AI Conversation Participant s figure resides. In one example AI Conversation Participant s figure can be centered on screen throughout the simulated conversation to smooth the positioning aspect of the transition. Any other moving or aligning can similarly be implemented. In another example AI Conversation Participant s figure can be resized to a certain size throughout the simulated conversation to smooth the size aspect of the transition. The size can be defined by a user by AIIM system administrator or automatically by the system. Any of the previously described resizing and or other transforming can be utilized in the transitioning. In other aspects transitioning includes lighting or color adjustments of AI Conversation Participant s figure or picture within which AI Conversation Participant s figure resides. In one example a certain level or balance of lighting or color can be maintained for AI Conversation Participant s figure throughout the simulated conversation to smooth the lighting or color aspect of the transition. In another example AI Conversation Participant s figure s lighting or color can be adjusted to better resemble AI Conversation Participant s figure in a preceding Sub stream of Digital Pictures . Any of the previously described lighting or color adjustments can be utilized in the transitioning. In further aspects transitioning includes a cut dissolve and or other motion picture editing techniques suitable for transitioning between motion pictures. In one example a cut can be used to switch instantly from one Sub stream of Digital Pictures to another without any picture processing. In another example a dissolve i.e. cross dissolve etc. can be used to gradually transition from one Sub stream of Digital Pictures to another. In other aspects transitioning includes morphing and or other transformations of AI Conversation Participant s figure or picture within which AI Conversation Participant s figure resides. Morphing may involve the steps of warping and cross dissolving in some implementations. Specifically morphing comprises defining corresponding points on two pictures and distorting one picture into the other as they cross dissolve. Defining the corresponding points on the pictures can be performed automatically using picture or facial recognition techniques that can detect corners blobs and or other points of interest on a picture as previously described. Any features functionalities and embodiments of Picture Recognizer can be used in transitioning or morphing. In one example one AI Conversation Participant s figure can be morphed into another by detecting significant points such as the contour of the nose locations of eyes corners of the mouth and or other facial or body points on both AI Conversation Participant s figures. Morphing can then distort the first AI Conversation Participant s figure into the shape of the second AI Conversation Participant s figure while cross dissolving the two AI Conversation Participant s figures. Beier and Neely and or other algorithm can be used to compute the transformation of image coordinates required for the distortion or warping. Other morphing or warping techniques can be used such as mesh grid based warping feature based morphing and or others. One of ordinary skill in art will understand that the aforementioned transitioning techniques are described merely as examples of a variety of possible implementations and that while all possible transitioning techniques are too voluminous to describe other transitioning techniques known in art are within the scope of this disclosure.

Referring to some embodiments of System for Using AIIMs Decision making Unit can use various bridging techniques to fill a gap between Conversational Activities . For example such bridging can be implemented when a next AI Conversation Participant s Conversational Activity is not yet known or missing. Bridging among Conversational Activities enables a simulated conversation to be perceived as smooth or uninterrupted thereby enhancing User experience. Bridging mostly relates to visual appearance of AI Conversation Participant in a simulated conversation although sound bridging can also be implemented. Bridging may include generating or creating intermediate pictures i.e. frames etc. between two Sub streams of Digital Pictures to give the appearance that AI Conversation Participant in a preceding Sub stream of Digital Pictures evolves smoothly into AI Conversation Participant in a subsequent Sub stream of Digital Pictures . Any features functionalities and embodiments of the previously described transitioning can be used in bridging. In some aspects bridging includes interpolation inbetweening extrapolation and or other picture or frame generation technique. In one example interpolation and or inbetweening can be used to generate intermediate pictures i.e. frames etc. between the last picture of a preceding Sub stream of Digital Pictures and the first picture of a subsequent Sub stream of Digital Pictures . In other aspects bridging includes playing or replaying one or more Sub streams of Digital Pictures or portions thereof. In one example a simple way to bridge between Sub streams of Digital Pictures is to repeatedly replay or freeze the last picture i.e. frame etc. of a preceding Sub stream of Digital Pictures until a subsequent Sub stream of Digital Pictures is known. This approach can be used in any implementation but may provide realistic bridging for short duration gaps. In another example a portion i.e. certain number of rearmost pictures etc. of a preceding Sub stream of Digital Pictures can be repeatedly replayed until a subsequent Sub stream of Digital Pictures is known. In a further example the entire preceding Sub stream of Digital Pictures can be repeatedly replayed until a subsequent Sub stream of Digital Pictures is known. In a further example any one or more Sub streams of Digital Pictures or portions i.e. certain number of pictures etc. thereof can be played or repeatedly replayed until a subsequent Sub stream of Digital Pictures is known. In such implementations one or more Sub streams of Digital Pictures from a similar Conversational Activity may be best suited to play or replay. For instance if bridging is needed between an observing Conversational Activity i.e. silent facial expressions silent body movements motionless silence etc. and a speaking Conversational Activity a Sub stream of Digital Pictures from another observing Conversational Activity preferably of the same type can be played or replayed until the speaking Conversational Activity is known. One of ordinary skill in art will understand that the aforementioned bridging techniques are described merely as examples of a variety of possible implementations and that while all possible bridging techniques are too voluminous to describe other bridging techniques known in art are within the scope of this disclosure.

Referring to the illustration shows an embodiment of a method for using AIIMs. The method can be used on a computing device or system to enable simulating a conversation with an artificially intelligent conversation participant. The computing device or system may include a user device i.e. User Device etc. a server i.e. Server etc. a dedicated device a host device i.e. Host Device etc. or an embedded element thereof and or others. Method may include any action or operation of any of the disclosed methods such as method and or others. Other additional steps actions or operations can be included as needed or some of the disclosed ones can be optionally omitted or a different combination or order thereof can be implemented in alternate embodiments of method .

At step a stored plurality of rounds of conversational exchange including a first round of conversational exchange are accessed the first round of conversational exchange comprising a recording of a first conversation participants first conversational activity and a recording of a second conversation participant s first conversational activity. The stored plurality of rounds of conversational exchange comprise any features functionalities and embodiments of the stored plurality of rounds of conversational exchange described in steps and or of method as applicable.

At step a stream of digital pictures of a user is captured. Step may include any action or operation described in step of method as applicable.

At step a stream of digital sound samples of the user is captured. Step may include any action or operation described in step of method as applicable.

At step the user s first conversational activity is detected from at least one of the stream of digital pictures of the user or the stream of digital sound samples of the user. Step may include any action or operation described in step of method as applicable.

At step at least one portion of a recording of the user s first conversational activity are compared with at least one portion of the recording of the first conversation participants first conversational activity. A portion of a recording of a conversational activity i.e. Conversational Activity etc. may include sub stream of digital sound samples i.e. Sub stream of Digital Sound Samples etc. or portion i.e. word feature sound sample etc. thereof. A portion of a recording of a conversational activity may include sub stream of digital pictures i.e. Sub stream of Digital Pictures etc. or portion i.e. picture feature region of pixels pixel etc. thereof. In some embodiments the comparing may include comparing sub stream of digital sound samples or portions thereof of one recording of conversational activity with sub stream of digital sound samples or portions thereof of another recording of conversational activity. In some aspects the comparing may include comparing one or more words recognized from one sub stream of digital sound samples with one or more words recognized from another sub stream of digital sound samples. In other aspects the comparing may include comparing one or more features i.e. sound features etc. from one sub stream of digital sound samples with one or more sound features i.e. sound features etc. from another sub stream of digital sound samples. In further aspects the comparing may include comparing sound samples from one sub stream of digital sound samples with sound samples from another sub stream of digital sound samples. In further aspects Dynamic Time Warping DTW and or other adjustments or techniques can be utilized for comparing and or aligning temporal sequences i.e. sub streams of digital sound samples etc. that may vary in time or speed. Comparing may also include other aspects or properties of digital sound or sound samples examples of which comprise amplitude adjustment sample rate or frequency adjustment noise reduction and or others. In other embodiments the comparing may include comparing sub stream of digital pictures or portions thereof of one recording of conversational activity with sub stream of digital pictures or portions thereof of another recording of conversational activity. In some designs Dynamic Time Warping DTW and or other adjustments or techniques can be utilized for comparison and or aligning temporal sequences i.e. sub streams of digital pictures etc. that may vary in time or speed. In some aspects the comparing may include comparing pictures from one sub stream of digital pictures with pictures from another sub stream of digital pictures. In some aspects comparing of individual pictures i.e. pictures from the sub streams of digital pictures etc. may include comparing one or more features i.e. picture features etc. of one picture with one or more features i.e. picture features etc. of another picture. In other aspects comparing of individual pictures may include comparing regions of pixels of one picture with regions of pixels of another picture. In further aspects comparing of individual pictures may include comparing pixels of one picture with pixels of another picture. Comparing may also include other aspects or properties of digital pictures or pixels examples of which comprise color adjustment size adjustment transparency i.e. alpha channel etc. use of a mask and or others. Any combination of the aforementioned and or other elements or techniques can be utilized in alternate embodiments of the comparing. Comparing comprises any action or operation by or for a Decision making Unit Substantial Similarity Comparison and or other disclosed elements.

At step a determination is made that a similarity between at least one portion of the recording of the user s first conversational activity and at least one portion of the recording of the first conversation participant s first conversational activity exceeds a similarity threshold. In some embodiments the determination may include determining that the number or percentage of matching or substantially matching portions of sub streams of digital pictures and or portions of sub streams of digital sound samples of the compared recordings of conversational activities exceeds a threshold number or threshold percentage. In some aspects weight can be assigned to sub streams of digital pictures or portions thereof and or sub streams of digital sound samples or portions thereof indicating their importance in the similarity determination. In other embodiments the determination may include determining that the number or percentage of matching words recognized from the sub streams of digital sound samples exceeds a threshold number i.e. 1 2 4 7 etc. or a threshold percentage i.e. 33 58 72 99 etc. . In some aspects the order of words the type of words the importance of words semantic variations of words concepts of words and or other elements and or techniques relating to words can be utilized for determining similarity using words. In further aspects some of the words can be omitted in determining similarity using words. Where a reference to a word is used herein it should be understood that a portion of a word or a collection of words can be used instead of or in addition to the word. In further embodiments the determination may include determining that the number or percentage of matching features from the sub streams of digital sound samples exceeds a threshold number i.e. 1 5 17 33 68 114 etc. or a threshold percentage i.e. 31 59 82 98 etc. . In some aspects the order of features the type of features the importance of features and or other elements or techniques relating to features can be utilized for determining similarity using features. In other aspects some of the features can be omitted in determining similarity using features. Where a reference to a feature is used herein it should be understood that a portion of a feature or a colection of features can be used instead of or in addition to the feature. In further embodiments the determination may include determining that the number or percentage of matching sound samples from the sub streams of digital sound samples exceeds a threshold number i.e. 21 85 154 297 422 699 etc. or a threshold percentage i.e. 29 48 69 96 etc. . In some aspects the order of sound samples the importance of sound samples and or other elements or techniques relating to sound samples can be utilized for determining similarity using sound samples. In further aspects some of the sound samples can be omitted in determining similarity using sound samples. Where a reference to a sound sample is used herein it should be understood that a collection i.e. frame etc. of sound samples can be used instead of or in addition to the sound sample. In further embodiments the determination may include determining that the number or percentage of matching or substantially matching pictures of the sub streams of digital pictures exceeds a threshold number i.e. 28 74 283 322 995 874 etc. or a threshold percentage i.e. 29 33 58 72 99 etc. . In some aspects the order of pictures and or other elements or techniques relating to pictures can be utilized for determining similarity using pictures. In further aspects some of the pictures can be omitted in determining similarity using pictures. In further embodiments the determination may include determining that the number or percentage of matching features from individual pictures exceeds a threshold number i.e. 3 22 47 93 128 431 etc. or a threshold percentage i.e. 49 53 68 72 95 etc. . In some aspects the type of features the importance of features and or other elements or techniques relating to features can be utilized for determining similarity using features. In further aspects some of the features can be omitted in determining similarity using features. In further aspects similarity determination can focus on features in certain regions of interest from the individual pictures. In further aspects detection or recognition of persons or objects using features in the pictures can be utilized for determining similarity. Where a reference to a feature is used herein it should be understood that a portion of a feature or a collection of features can be used instead of or in addition to the feature. In further embodiments the determination may include determining that the number or percentage of matching pixels from individual pictures exceeds a threshold number i.e. 449 2219 92229 442990 1000028 etc. or a threshold percentage i.e. 39 45 58 72 92 etc. . In some aspects some of the pixels can be omitted in determining similarity using pixels. In further aspects similarity determination can focus on pixels in certain regions of interest from the individual pictures. Where a reference to a pixel is used herein it should be understood that a collection i.e. region etc. of pixels can be used instead of or in addition to the pixel. Any combination of the aforementioned and or other elements or techniques can be utilized in alternate embodiments. Determining comprises any action or operation by or for a Decision making Unit Substantial Similarity Comparison and or other disclosed elements.

At step at least one portion of the recording of the second conversation participant s first conversational activity is played. Playing a recording of conversational activity or portion thereof may include playing sub stream of digital pictures or portion thereof and or sub stream of digital sound samples or portion thereof included in the recording of conversational activity. The playing may be performed concurrently with the user s current i.e. first etc. conversational activity. A played sub stream of digital pictures or portion thereof may include a conversation participant s i.e. second conversation participant s etc. visual expressions or communication. Similarly a played sub stream of digital sound samples may include a conversation participant s i.e. second conversation participant s etc. verbal expressions or communication. In one example the second conversation participant s observing conversational activity i.e. silent facial expressions silent body movements motionless silence etc. or portion thereof can be played to simulate artificially intelligent conversation participant s i.e. AI Conversation Participant s etc. observing while user speaks i.e. performs a speaking conversational activity etc. . In another example the second conversation participant s speaking conversational activity or portion thereof can be played to simulate artificially intelligent conversation participant s speaking while user observes i.e. performs an observing conversational activity i.e. silent facial expressions silent body movements motionless silence etc. etc. . In some aspects playing can be interrupted if the user starts speaking and the process can redirect to step . In some embodiments the playing may include transitioning from one sub stream of digital pictures to another i.e. subsequent etc. sub stream of digital pictures to enable a simulated conversation to be perceived as smooth or uninterrupted thereby enhancing user experience. Such transitioning may include manipulating one or more pictures or content thereof of a preceding sub stream of digital pictures and one or more pictures or content thereof of a subsequent sub stream of digital pictures. In some aspects transitioning includes moving centering aligning resizing and or otherwise transforming one or more pictures or content thereof of a sub stream of digital pictures. In other aspects transitioning includes lighting or color adjustment of one or more pictures or content thereof of a sub stream of digital pictures. In further aspects transitioning includes a cut dissolve and or other motion picture editing techniques between sub streams of digital pictures. In further aspects transitioning includes morphing and or other transformations of one or more pictures or content thereof of sub streams of digital pictures. In other embodiments the playing may include bridging between one sub stream of digital pictures and another i.e. subsequent etc. sub stream of digital pictures to enable a simulated conversation to be perceived as smooth or uninterrupted thereby enhancing user experience. Such bridging may include any of the aforementioned transitioning techniques generating additional or intermediate pictures playing or replaying pictures and or other techniques. In some aspects bridging includes interpolation inbetweening extrapolation and or other picture or frame generation techniques. In other aspects bridging includes playing or replaying one or more pictures of a sub stream of digital pictures. Playing comprises any action or operation by or for a Decision making Unit Display Sound producing Device and or other disclosed elements.

It must be noted that as used herein and in the appended claims the singular forms a an and the include plural referents unless the context clearly dictates otherwise.

A number of embodiments have been described herein. While this disclosure contains many specific implementation details these should not be construed as limitations on the scope of any inventions or of what may be claimed but rather as descriptions of features specific to particular embodiments. It should be understood that various modifications can be made without departing from the spirit and scope of the invention. The logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other or additional steps elements or connections can be included or some of the steps elements or connections can be eliminated or a combination thereof can be utilized in the described flows illustrations or descriptions. Further the various aspects of the disclosed devices apparatuses systems and or methods can be combined in whole or in part with each other to produce additional implementations. Moreover separation of various components in the embodiments described herein should not be understood as requiring such separation in all embodiments and it should be understood that the described components can generally be integrated together in a single program or product or packaged into multiple programs or products. Accordingly other embodiments are within the scope of the following claims.

