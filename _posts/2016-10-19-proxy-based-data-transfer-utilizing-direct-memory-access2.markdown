---

title: Proxy based data transfer utilizing direct memory access
abstract: A method for transferring data utilizing direct memory access. The method includes a computer processor establishing a networking connection, using a proxy, between at least a first computing entity and a second computing entity. The method further includes determining a shared memory space for the established networking connection between at least the first computing entity and the second computing entity. The method further includes allocating the shared memory space from heap memory. The method further includes transmitting data over the established networking connection between at least the first computing entity and the second computing entity utilizing a direct memory access protocol and the allocated shared memory space.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09632974&OS=09632974&RS=09632974
owner: International Business Machines Corporation
number: 09632974
owner_city: Armonk
owner_country: US
publication_date: 20161019
---
The present invention relates generally to the field of data transfer within virtualized computing environments and more particularly to data transfer within a computing node by directly utilizing shared system memory.

In system virtualization each computing entity behaves as if it were a separate computer information and data are transferred e.g. communicated utilizing computer networking. Some virtualized systems permit a VM to support multitenancy of a runtime environment or a shared container in a cloud computing application. In some virtualized systems each tenant e.g. application may be treated as a computing entity. In computer networking the transport layer provides end to end communication services for applications within a layered architecture of network components and protocols. The transport layer provides convenient services such as application programming interfaces APIs connection oriented data stream support reliability flow control socket creation socket closing data transmission and multiplexing. Computing entities within a virtualized system and entities external to a virtualized system may utilize a proxy server or a proxy application to process communications between computing entities.

Communication between applications within the same virtualized system progresses through a networking software stack associated with a first application and another networking software stack for the second application. Alternatively a modification of an infrastructure such as a Virtual Machine Communication Interface VMCI protocol provides fast e.g. low latency and efficient e.g. high bandwidth communication between a virtual machine and the host operating system and between two or more virtual machines and or applications executing on the same host i.e. the same physical real computer .

Aspects of an embodiment of the present invention disclose a method computer program product and computing system for communicating data utilizing direct memory access. In an embodiment the method includes one or more processors establishing a networking connection using a proxy between at least a first computing entity and a second computing entity. The method further includes one or more processors determining a shared memory space for the established networking connection between at least the first computing entity and the second computing entity. The method further includes one or more processors allocating the shared memory space from heap memory. The method further includes one or more processors transmitting data over the established networking connection between at least the first computing entity and the second computing entity utilizing a direct memory access protocol and the allocated shared memory space.

Embodiments of the present invention recognize that transferring data and information within a computing node e.g. a physical machine of a virtualized computing environment may slow due to various implementations of network communication. For example communications between processes e.g. applications within a virtualized computing environment can invoke a network application programming interface API defined within a runtime library of a programming language. An API can interact with the operating system kernel which in turn accesses the transmission control protocol TCP stack. A similar series of events occurs for a second application that transfers data with the first application since the data transfer is treated as network communication. Multi layered overhead e.g. increased latency additional system resources utilized etc. may occur among computing entities herein identified as entities. Entities may include software applications processes VMs tenant applications threads tasks etc. executing within the same computing node. Additional overhead may be added if a computing entity utilizes a proxy server or proxy application to facilitate the network communication to another computing entity. A proxy acts as an intermediary for requests from clients e.g. computers servers programs etc. seeking resources from other servers e.g. computers programs entities etc. .

Embodiments of the present invention recognize that various virtualized computing environments provide shortcuts e.g. VMCI protocol VMCI API etc for communicating between VMs of the same computing node. Embodiments of the present invention also recognize that various operating systems provide shortcuts for the networking sockets within one OS. A network socket is an endpoint of an inter process communication across a computer network. One such shortcut permits the data information e.g. object that is exchanged to be copied from user mode to kernel mode. Embodiments of the present invention recognize that applications written utilizing various software languages and software development kits SDKs may utilize remote direct memory access RDMA operations that support zero copy networking to transfer data directly to or from application memory. A RDMA operation eliminates the need to copy data between application memory and the data buffers of an OS executing on the same physical computing node. However when computing entities utilize a proxy application or proxy server to communicate RDMA operations occur between each computing entity and the proxy as opposed to RDMA operations occurring between the communicating computing entities.

Some embodiments of the present invention utilize TCP and a socket registry. Various embodiments of the present invention utilize a proxy to establish communications e.g. network communications between computing entities such as a socket secure SOCKS session. Computing entities may utilize a proxy for but not limited to access control content control e.g. filtering load balancing security e.g. encryption certificates encapsulation etc. Some embodiments of the present invention create utilize and update tables herein identified as global socket registry tables that identify the endpoints e.g. host name identity information Internet protocol IP address process number port number etc. of a communication path of the computing entities. Some embodiments of the present invention may utilize user datagram protocol UDP or another networking protocol. Embodiments of the present invention initiate modifications within a transport layer e.g. TCP UDP and do not affect authentication mechanisms related to SOCKS5.

Embodiments of the present invention allocate memory e.g. blocks buffers etc. from a heap e.g. unallocated memory of a computing node such that one or more direct memory access methods e.g. RDMA a fast path communication solution etc. may be utilized to communicate data between computing entities. Embodiments of the present invention enable a proxy application or proxy server herein identified as a proxy to manage a read pointer and a write pointer pair RWP corresponding to data communicated via direct memory access e.g. transfer .

One embodiment of the present invention may allocate a shared memory buffer for each instance of data communicated between computing entities. Some embodiments of the present invention may utilize a proxy that enables data communications among a plurality of entities e.g. shared socket connections . Other embodiments of the present invention may combine RWP pairs such that multiple read pointer and write pointers manage the data communication within a shared memory buffer. In one example one entity may write data to a shared memory buffer and no subsequent data is written to the shared memory buffer until each reading entity consumes the written data. The number and locations e.g. within a VM within a LPAR between LPARs of entities that communicate data via a proxy may be affected by the architecture of a computing node a hypervisor e.g. firmware software of the computing node and or communication functions incorporated within the computing node.

An embodiment of the present invention may allocate a block of memory comprised of a plurality of buffers to a message sharing pool. In such an embodiment each socket connection pair e.g. clientproxyserver is assigned an unused messaging buffer of a message sharing pool. In addition a proxy manages a read pointer assigned to the designated buffer that allows writing new messages e.g. data into the designated buffer and read out of the designated buffer as the data consumed. Subsequently when a socket connection pair disconnects the buffer e.g. shared memory may be returned to a message sharing pool. Various embodiments of the present invention may utilize a plurality of read pointer and write pointer pairs to communicate e.g. transmit data e.g. multiple data records via a buffer within the allocated shared memory. Some embodiments of the present invention may dynamically allocate the size of a buffer within a messaging pool based on the size of the data and or dictates associated with communicating entities.

Some embodiments of the present invention modify the implementation codes e.g. APIs methods etc. for network communications for a supported language runtime environment networking e.g. socket creation port identification etc. to support shared heap space for data transfer e.g. a fast path communication solution . Utilizing modified implementation codes e.g. alternative API and the shared heap space for data exchange embodiments of the present invention may accelerate communications between applications and reduce method invocations. Other embodiments of the present invention may detect a native networking API invocation and redirect a network communication method to initiate an alternative API. An embodiment of the present invention may affect the creation of sockets via the network connection management code of the SDK. Such an embodiment of the present invention may dictate modifying the source code of an application and or recompiling an application to utilize the fast path communication solution.

Other embodiments of the present invention may be implemented at the application layer as opposed to the transport layer. For example a hypertext transfer protocol HTTP proxy and or a HTTP over secure socket layer HTTPS proxy may be utilized by computing entities that engage in web based and or Internet like transfers of data. Alternative embodiments of the present invention may be based on various web protocols that utilize TCP as a basis for network connections.

Various embodiments of the present invention determine whether the computing entities e.g. applications that communicate data execute within the same physical computing node and if so transfer the data e.g. object via direct memory access e.g. reducing latency . However the dynamic nature of memory management within a virtualized computing environment may move data or reclaim reallocate memory space. For example some programming languages may rely on a system function such as a garbage collector to free e.g. de allocate shared memory after data is communicated e.g. consumed . Other programming languages may explicitly release shared memory utilized to communicate data when a call a method a function etc. finalizes e.g. completes terminates etc. . Some embodiments of the present invention may incorporate additional controls to protect the address locations and memory space allocated for the transfer of data until an application has consumed the data e.g. an object and flagged the data as dead e.g. unneeded consumed released . For example an object may be designated as dead when the object is not utilized by an executing application the object is not referenced by an executing application and the object is finalized by a method function within an executing application. In another example consumed data may remain in a protected condition in shared memory when the data is cached for reuse.

The present invention will now be described in detail with reference to the Figures. is a functional block diagram illustrating a networked computing environment which includes computing node i.e. a virtualized computing system network and client in accordance with the present invention.

Client may be a laptop computer a tablet computer a netbook computer a personal computer PC a desktop computer a personal digital assistant PDA a smart phone a wearable device e.g. digital eyeglasses smart glasses smart watches personal fitness devices personal safety devices or any programmable computer system known in the art. In certain embodiments computing node and client represents a computer system utilizing clustered computers and components e.g. database server computers application server computers etc. that act as a single pool of seamless resources when accessed through network as is common in data centers and with cloud computing applications. In general computing node and client are representative of any programmable electronic device or combination of programmable electronic devices capable of executing machine readable program instructions and communicating with users of computing node and client via network . Computing node and client may include components as depicted and described in further detail with respect to in accordance with embodiments of the present invention.

In an embodiment computing node is divided into multiple partitions that include logical partitions LPARs and . In an illustrated example for computing node LPAR LPAR and LPAR each run an independent operating environment such as an operating system OS . In some embodiments LPAR includes VM VM and VM executing a shared OS. LPAR includes VM executing another OS. LPAR includes VM executing another OS capable of multitenancy of applications. In this instance VM includes app app and app e.g. multitenancy . In other embodiments LPAR LPAR and LPAR may include a different number of provisioned VMs. In further embodiments LPAR LPAR and LPAR may include other operating environments and combinations of operating environments. In various embodiments computing node is a node of a clustered computer system such as a cloud computer.

Communications to and from network are routed through shared Ethernet adapter SEA to virtual adapters and on respective LPARs and in accordance with an embodiment of the present invention. In an embodiment SEA is comprised of one or more network cards controlled by hypervisor . In another embodiment SEA is associated with an LPAR not shown executing an OS such as a virtual input output I O server VIOS . In an alternative embodiment physical network adapters are allocated to LPARs and .

Hypervisor forms LPAR LPAR and LPAR from the physical resources e.g. hardware of computing node . The physical hardware of computing node is comprised of processors disk network cards and or memory which may be allocated e.g. provisioned to LPAR LPAR and LPAR . Hypervisor performs standard operating system functions and manages communication between LPAR LPAR and LPAR via an internal network. In one embodiment communications within computing node are provided by a virtual local area network VLAN . In some embodiments computing node may utilize other technologies such as VMCI or virtual network interface cards VNIC to enhance the communications with virtual adapters and or to replace virtual adapters and .

Communication module is associated with hypervisor and includes look up tables to track various communication protocols port numbers and socket addresses utilized to communicate between various computing entities. In some embodiments communication module includes look up tables e.g. global socket registry tables identifying the real TCP connections utilized by the communication solution between the computing entities. In an alternative embodiment communication module includes tables that are associated with a fast path communication solution and are identified herein as SharedVM FPath SharedOS FPath and SharedHypervisor FPath. SharedVM FPath communication solution utilizes one or more shared memory buffers allocated from heap associated with a VM. SharedOS FPath communication solution utilizes one or more shared memory buffers allocated from heap associated with a LPAR. SharedHypervisor FPath communication solutions utilizes one or more shared memory buffers allocated from heap associated with computing node .

In addition communication module may interact with a memory management function not shown that manages e.g. allocates protects releases etc. memory associated with the heap and or shared memory buffers. In some embodiments communication module creates provisions and manages proxies executing within computing node that enable entities to communicate data.

In some embodiments computing node communicates through network to client other computing nodes not shown within networked computing environment other virtualized computing environments not shown and other computers not shown . Network can be for example a local area network LAN a telecommunications network a wireless local area network WLAN a wide area network WAN such as the Internet or any combination of the previous and can include wired wireless or fiber optic connections. In general network can be any combination of connections and protocols that will support communications between processors and computing node in accordance with embodiments of the present invention. In another embodiment network operates locally via wired wireless or optical connections and can be any combination of connections and protocols e.g. NFC laser infrared etc. . In some embodiments a physical computer such as computing node is identified by a media access control address MAC address which is a unique identifier assigned to network interfaces for communications on the physical network segment.

DMA proxy module includes API library fast path communication program and data management program . DMA proxy module may include shareable memory functions not shown and communication functions not shown that respond to embodiments of the present invention to generate the interaction that produce the fast path communication solution e.g. a path . For example communication controls such as send receive locks ensure correct read write sequencing of shared heap memory space which is shared by different threads. In some embodiments DMA proxy module includes one or more global socket registry tables such as a SharedVM FPath table a SharedOS FPath table and a SharedHypervisor FPath table.

API library includes one or more networking APIs coded for each runtime environment and or software language to enable a direct memory transfer e.g. zero copy operation of data among computing entities in accordance with embodiments of the present invention. In one embodiment API library includes modified networking APIs that enable direct memory transfer of data that subsequently replace native networking APIs such as networking APIs related to a runtime environment. In other embodiment API library includes modified networking APIs that are invoked in response to a function call of a software application.

Fast path communication program identifies computing entities executing within networked computing environment that are engaged in data communication e.g. data transmission date exchange . In one embodiment fast path communication program identifies a location e.g. computing node client for each computing entity engaged in data communication. In addition fast path communication program determines which computing entities utilize a proxy to enable communications. In another embodiment fast path communication program determines a direct memory transfer protocol that is utilized by computing entities that communicate data. In various embodiments fast path communication program allocates heap memory that is utilized for direct memory transfer of data between computing entities. In some embodiments fast path communication program interfaces with data management program . In other embodiments multiple instances of fast path communication program and data management program execute to enable direct memory transfer of data among a plurality of computing entities that communicate.

Data management program determines information that is associated with communicated data such as a size of the data one or more controls related to the data a status of the data etc. In one embodiment data management program may also apply one or more controls to communicated data that prevents a memory management function of computing node from affecting a shared memory that stores the communicated data. In another embodiment data management program determines a status for communicated data and manages the shared memory based at least in part on the status of the communicated data. In various embodiments data management program manages shared memory utilized to communicate data between entities by interfacing with a proxy where the proxy manages a read pointer and a write pointer pair that corresponds to the data that is communicated between entities.

In some embodiments multiple instances of data management program execute concurrently. In an embodiment one instance of data management program executes and interfaces with one instance of fast path communication program . In another embodiment a plurality of instances of data management program execute and interface with an instance of fast path communication program . In an embodiment an instance of data management program manages one or more read pointer and write pointer pairs of a proxy. In another embodiment data management program manages a plurality of read pointer and write pointer pairs associated with two or more proxies.

In step fast path communication program identifies entities that engaged in data communication. In an embodiment fast path communication program identifies two or more entities that are engaged in data communication within networked computing environment . In one embodiment fast path communication program identifies a corresponding location of each computing entity that engages in data communication. In one scenario fast path communication program identifies a location for a communicating entity based on information within one or more table related to a global sockets registry. In another scenario fast path communication program identifies a location for a communicating entity based on information obtained by communication module .

In some embodiments fast path communication program identifies which communicating entities exist within computing node . In other embodiments fast path communication program identifies which communicating entities e.g. client communicate with computing node via network . In various embodiments fast path communication program additionally identifies relationships among entities such as entities that communicate within a shared VM e.g. app and app entities that communicate within a shared OS e.g. LPAR such as VMs and and entities that communicate within computing node e.g. between different LPARs such as VM and app . In addition fast path communication program may determine which entities that communicate data utilize a proxy to communicate the data.

In decision step fast path communication program determined whether the entities utilize a proxy to communicate. In response to determining that entities utilize a proxy to communicate Yes branch decision step fast path communication program updates a global sockets registry table for entities that communicate utilizing a proxy step .

In step fast path communication program updates a global sockets registry table for entities that communicate utilizing a proxy. In one embodiment fast path communication program updates a global sockets registry table to include a flag that identifies entities that utilize a proxy to communicate within computing node . In another embodiment fast path communication program updates a global sockets registry table to include a different flag that identifies entities that utilize a proxy to communicate via network .

Referring to decision step responsive to determining that entities do not utilize a proxy to communicate No branch decision step fast path communication program updates a global sockets registry table for entities that communicate utilizing a proxy step .

In step fast path communication program updates a global sockets registry table for entities that communicate within a shared computing device. In an embodiment fast path communication program updates a global sockets registry table based on the identification of entities that communicate within computing node .

In step fast path communication program determines a direct memory transfer protocol to communicate data between entities. In one embodiment fast path communication program determines a direct memory transfer protocol for entities that communicate utilizing a proxy. In an example fast path communication program may determine based on one or more global socket registry tables that one of the following direct memory transfer protocols is utilized SharedVM FPath a SharedOS FPath and a SharedHypervisor FPath. In one scenario fast path communication program selects SharedVM FPath as the direct memory transfer protocol for entities that communicate within a shared VM such as app and app . In another scenario fast path communication program selects SharedOS FPath as the direct memory transfer protocol for entities that communicate within a shared OS such as VM and VM .

In another scenario fast path communication program selects SharedHypervisor FPath as the direct memory transfer protocol for entities that communicate within a shared physical machine e.g. computing node . In one example fast path communication program selects SharedHypervisor FPath to communicate data between entities e.g. VM and VM within LPARs e.g. LPAR and LPAR executing different operating systems such as AIX and LINUX . In another example fast path communication program selects SharedHypervisor FPath to communicate data between entities e.g. VM and VM within different LPARs e.g. LPAR and LPAR executing a different instance of the same OS.

In an alternative embodiment fast path communication program determines a direct memory transfer protocol for entities that communicate data without utilizing a proxy based on information within one or more global socket registry tables.

In step fast path communication program allocates memory for communicated data. In some embodiments fast path communication program allocates memory for a shared memory buffer based on information e.g. size associated with the identified data referring to step . In another embodiment fast path communication program allocates memory for communicated data based on one or more constraints and or dictates. In one scenario fast path communication program determines that one or more parameters are associated with a LPAR that constrains a minimum size of heap memory. For example LPAR is provisioned with 12 GB of memory and the maximum memory utilization of VM VM and VM is 11.5 GB. Fast path communication program is constrained to allocate a total 0.5 GB of memory among one or more shared memory buffers. In another example referring to proxy utilizes a messaging pool to communicate data. Proxy may dictate that each messaging pool buffer MPB is 200 MB. If proxy creates three MPBs then fast path communication program allocates a minimum of 600 MB of memory to shared memory SM buffer .

In one embodiment if fast path communication program selects a SharedVM FPath communication protocol then fast path communication program allocates memory from heap associated with a VM e.g. VM . In another embodiment if fast path communication program selects a SharedOS FPath communication protocol then fast path communication program allocates memory from heap associated with a LPAR e.g. LPAR . In another embodiment if fast path communication program selects a SharedHypervisor FPath communication protocol then fast path communication program allocates memory from heap associated with computing node .

In a further embodiment fast path communication program may allocate more memory to a SM buffer than is utilized for the communication of a data. In one scenario fast path communication program allocates a larger SM buffer to enable substantially concurrent bidirectional communication of data between entities. In another scenario fast path communication program allocates a larger SM buffer to enable entities that communicate via proxies and entities that communicate directly to utilize a SM buffer.

In step fast path communication program communicates data utilizing a direct memory transfer protocol. In one embodiment fast path communication program communicates data between entities utilizing a direct memory transfer protocol such as SharedVM FPath a SharedOS FPath and a SharedHypervisor FPath. In another embodiment fast path communication program interfaces with data management program to enable one or more aspects of a direct memory transfer protocol. In one example fast path communication program may communicate data larger than the memory that is allocated to a SM buffer. Referring to step fast path communication program may interface with data management program to manage the shared memory that communicates the data between entities. In another example fast path communication program may interface with data management program to determine that data is cached within a SM buffer. The cached data may be directly read from the SM buffer as opposed to communicating a networking request for the data to the other entity and the other entity communicating the data to the SM buffer.

In step data management program identifies data that is communicated. In one embodiment data management program identifies data that is communicated between two entities. In one scenario data management program identifies the data that is communicated between two entities that utilize a proxy to establish a networked connection. In another scenario data management program identifies the data that is communicated between two entities that do not utilize a proxy to establish a networked connection. In some embodiments data management program determines information related to communicated data such as a size of the data whether the data is cached whether the data is shared e.g. multiple receiving entities a priority assigned to the data and whether the proxy affected e.g. encrypted certified etc. the data. In another embodiment data management program may determine that the identified data is utilized by two or more entities. In an example referring to VM may perform a database query and obtain a result. Subsequently VM and VM include applications that process some or all of the obtained result. In this example a proxy managing the communication among VM VM and VM may utilize two or more RWPs to communicate the result via a shared memory buffer from VM to VM and VM respectively.

In step data management program determines controls related to data communicated via direct memory transfer. In one embodiment data management program determines that communicated data is cached. In one scenario data management program may set a flag in the header of the communicated data indicating that the data is cached and is not deleted and or memory addresses released when the data is communicated e.g. used consumed . In another scenario data management program communicates with a memory management program of hypervisor that the data within a SM memory is cached. In another embodiment data management program communicates with a memory management program e.g. a garbage collector of hypervisor that the data within a SM memory buffer is protected e.g. not moved to another memory address memory is not released .

In some embodiments data management program obtains controls related to data communication from a proxy managing the networking and or data communication e.g. transfer between entities. For example data management program may determine in step that the communicated data is segmented e.g. larger than an allocated shared memory buffer and that read write locks are utilized for data communication. In another example data management program may determine that two or more entities may access the same data within a shared memory buffer concurrently. Therefore data management program determines that the shared memory buffer is protected until the two or more entities each obtain the shared data. In other embodiments data management program obtains controls related to data communication from an entity e.g. software API etc. engaged in communicating the data. In an example data management program may determine that app is coded in a language that explicitly releases memory when data communication is complete.

In step data management program determines a status for a communicated data. In one embodiment data management program determines whether the communication of the data between entities is successful. In some embodiments data management program determines status information related to an in progress communication of data. In an example data management program may determine a completion percentage for a data communication. In another example data management program may determine which blocks of memory that comprise a shared memory buffer are communicated and which blocks of memory remain in a queue. In other embodiments data management program may pause at step until a status changes for a communicated data. In another embodiment data management program determines whether data is flagged as cached. In an alternative embodiment data management program determines a status based on analyzing exception information received from a proxy and or a communicating entity.

In step data management program manages shared memory utilized to communicate data. In one embodiment data management program may manage communication controls such as send receive locks ensure correct read write sequencing of shared heap memory space which is shared by communicating entities. In another embodiment data management program releases shared memory when data is consumed and or a network connection terminates. In an alternative embodiment data management program may utilize information associated with the data step to determine which buffer contains data of a lower priority. In case of an emergency e.g. constrained shared memory a memory buffer associated with a lower priority data and or process may be purged and the memory buffer assigned to higher priority data. In one scenario data management program interfaces with fast path communication program to resume communication of the low priority data based on the state of a corresponding RWP from a point in time of the emergency buffer purge. In another scenario data management program interfaces with fast path communication program to update a global socket registry table associated with the low priority data such that the low priority data is communicated via a standard network e.g. TCP communication path.

LPAR includes VM and VM each previously discussed in and in one embodiment fast path communication program allocates referring to step SM .

LPAR includes app app and app each previously discussed in and in one embodiment fast path communication program referring to step allocates SM .

In one embodiment proxy and or proxy are proxy servers. In another embodiment proxy and or proxy are software constructs e.g. applications functions services etc. . In some embodiments proxy and or proxy are initiated by hypervisor communication module and or another system function of computing node each previously discussed in when entities e.g. VM and VM communicate data.

Network connections NC such as NC NC NC and NC are depicted by compound lines i.e. two lines . In one embodiment proxy establishes and manages NC and NC . In another embodiment proxy establishes and manages NC and a portion of NC connected to network . In some embodiments proxy may be associated with SEA previously discussed in . In other embodiments proxy utilizes SEA to communicate with network .

In another embodiment proxy establishes and manages network connections NC among app app and app of VM . In another embodiment proxy establishes and manages a network connection NC between VM and VM of LPAR . In a further embodiment proxy establishes and manages a network connection comprised of NC and NC which enables communication of data between VM and app . In some embodiments a proxy e.g. proxy may establish and manage a plurality of network connections. The number and locations e.g. within a VM within an LPAR between LPARs of entities that may communicate data via a proxy may be affected by the architecture of a computing node e.g. computing node a hypervisor of the computing node and or communication functions incorporated within the computing node. Similar considerations and or constraints e.g. the architecture of a computing node may affect e.g. limit the number the SM buffers that are managed by a proxy.

Proxy establishes and manages a network connection NC that enables client to communicate among app app and app via network . Client exists external to computing node therefore proxy cannot establish a shared memory buffer that enables client to utilize a direct memory transfer protocol with any or all of app app and app .

In one embodiment proxy enables communication among app app and or app via a fast path communication solution e.g. SharedVM FPath utilizing SM . SM includes messaging pool buffer MPB MPB and MPB . Proxy manages MPB via read write pointers pair RWP . Proxy manages MPB via RWP . In one example app communicates data to app via MPB . In another example app communicates data to app via MPB . In the current depiction MPB is allocated however MPB is unused. In some embodiments proxy may manage a plurality of read pointers and write pointers pairs associated with RWP and or RWP . In an example a plurality of read pointers and write pointers pairs enables a buffer e.g. MPB that communicates two or more data between app and app . In one scenario data communication may be unidirectional such as from app to app . In another scenario data communication may be bidirectional such as a first data is communicated e.g. transferred from app to app app processes the communicated data and app communicates e.g. returns the results of the processed data to app . In various embodiments proxy may manage a plurality of read pointers and write pointers which are shared by multiple servers that are associated with a memory buffer and where the plurality of read pointers and write pointers are not constrained to exist as pairs.

In another embodiment proxy establishes and manages NC between VM and VM of LPAR . In an example fast path communication program allocates SM SM may communicate data between VM and VM via a fast path communication solution e.g. SharedOS FPath . Proxy manages RWP for SM which communicates data between VM and VM .

In a further embodiment proxy establishes and manages NC and NC that enables communication between VM of LPAR and app of VM executing within LPAR . In an example fast path communication program allocates SM which communicates data between VM and app via a fast path communication solution e.g. SharedHypervisor FPath . Proxy manages RWP for SM SM communicates data between VM of LPAR and app of VM executing within LPAR .

Memory and persistent storage are computer readable storage media. In this embodiment memory includes random access memory RAM . In general memory can include any suitable volatile or non volatile computer readable storage media. Cache is a fast memory that enhances the performance of processor s by holding recently accessed data and data near recently accessed data from memory . With respect to computing node memory includes at least in part designated memory e.g. physical hardware depicted in to be shared among LPARs.

Program instructions and data used to practice embodiments of the present invention may be stored in persistent storage and in memory for execution by one or more of the respective processor s via cache . In an embodiment persistent storage includes a magnetic hard disk drive. Alternatively or in addition to a magnetic hard disk drive persistent storage can include a solid state hard drive a semiconductor storage device read only memory ROM erasable programmable read only memory EPROM flash memory or any other computer readable storage media that is capable of storing program instructions or digital information. With respect to computing node persistent storage includes at least in part disks e.g. physical hardware depicted in to be shared among LPARs.

The media used by persistent storage may also be removable. For example a removable hard drive may be used for persistent storage . Other examples include optical and magnetic disks thumb drives and smart cards that are inserted into a drive for transfer onto another computer readable storage medium that is also part of persistent storage . Software and data are stored in persistent storage for access and or execution by one or more of the respective processor s via cache and one or more memories of memory . With respect to computing node software and data includes hypervisor communication module DMA proxy module app app and app . In addition DMA proxy module includes fast path communication program and data management program DMA proxy module may also include shareable memory functions and communication functions. With respect to communication module software and data may also include a garbage collector one or more memory management functions not shown one or more look up tables and one or more global socket registry tables.

Communications unit in these examples provides for communications with other data processing systems or devices including resources of computing node processors and client . In these examples communications unit includes one or more network interface cards. Communications unit may provide communications through the use of either or both physical and wireless communications links. With respect to computing node hypervisor software and data and program instructions and data used to practice embodiments of the present invention may be downloaded to persistent storage through communications unit . With respect to computing node communications unit includes at least in part one or more network cards e.g. physical hardware shared Ethernet adapter SEA and virtual adapters and depicted in to be shared among LPARs.

I O interface s allows for input and output of data with other devices that may be connected to each computer system. For example I O interface may provide a connection to external devices such as a keyboard keypad a touch screen and or some other suitable input device. External devices can also include portable computer readable storage media such as for example thumb drives portable optical or magnetic disks and memory cards. Software and data used to practice embodiments of the present invention can be stored on such portable computer readable storage media and can be loaded onto persistent storage via I O interface s . I O interface s also connect to display device .

Display device provides a mechanism to display data to a user and may be for example a computer monitor. Display device can also function as a touch screen such as the display of a tablet computer or a smartphone.

It is understood in advance that although this disclosure discusses system virtualization implementation of the teachings recited herein are not limited to a virtualized computing environment. Rather the embodiments of the present invention are capable of being implemented in conjunction with any type of clustered computing environment now known e.g. cloud computing or later developed.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code instructions embodied thereon.

The programs described herein are identified based upon the application for which they are implemented in a specific embodiment of the invention. However it should be appreciated that any particular program nomenclature herein is used merely for convenience and thus the invention should not be limited to use solely in any specific application identified and or implied by such nomenclature.

The present invention may be a system a method and or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.

The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present invention may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data configuration data for integrated circuitry or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Smalltalk C or the like and procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present invention.

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the blocks may occur out of the order noted in the Figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

The descriptions of the various embodiments of the present invention have been presented for purposes of illustration but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The terminology used herein was chosen to best explain the principles of the embodiment the practical application or technical improvement over technologies found in the marketplace or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.

