---

title: User interface interaction for transparent head-mounted displays
abstract: Embodiments of the present invention are directed toward enabling a user to quickly interact with a graphical user interface (GUI) displayed by the HMD. Utilizing techniques provided herein, a hand or other object can be used to select visual elements displayed by the HMD. The visual elements can be located within larger active regions, allowing the user to more easily select a visual element by selecting the active region in which the visual element is disposed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547374&OS=09547374&RS=09547374
owner: QUALCOMM INCORPORATED
number: 09547374
owner_city: San Diego
owner_country: US
publication_date: 20160526
---
The present application is a continuation of co pending commonly assigned U.S. Non Provisional application Ser. No. 13 800 329 filed Mar. 13 2013 entitled USER INTERFACE INTERACTION FOR TRANSPARENT HEAD MOUNTED DISPLAYS which claims the benefit and priority under 35 U.S.C. 119 e of commonly assigned U.S. Provisional App. No. 61 659 914 filed Jun. 14 2012 entitled USER INTERFACE INTERACTION FOR TRANSPARENT HEAD MOUNTED DISPLAYS. Both these patent applications are incorporated by reference herein in their entirety for all purposes.

A see through head mounted display HMD can provide transparent display area within a user s field of view in which a user can view both physical objects in the user s surroundings and virtual objects on the display. Some HMDs can provide augmented reality functionality by overlaying physical objects viewed by a user with digital content such as text pictures and or video associated with the physical objects or associated with the user s location and or context for example. Input to such HMDs are typically limited to buttons a touch pad or other simple input devices. These input devices can be bulky and inefficient.

Embodiments of the present invention are directed toward enabling a user to quickly interact with a graphical user interface GUI displayed by a head mounted display HMD using a hand and or other objects. Utilizing techniques provided herein a hand or other object can be used to select visual elements displayed by the HMD. The visual elements can be located within larger active regions allowing the user to more easily select a visual element by selecting the active region in which the visual element is disposed.

An example method of providing a user interface in an HMD according to the disclosure includes causing a first visual element to be displayed in a transparent display area of the HMD. The first visual element is selectable by a user of the HMD. The method also includes defining a first active region of the transparent display area larger than the first visual element such that the first visual element is disposed within a portion of the first active region and tracking a physical object that appears from a perspective of the user within the transparent display area. The method further includes determining at least a portion of the physical object appears from the user s perspective within the first active region indicating a selection of the first visual element and manipulating with a processing unit the user interface based on the selection.

An example HMD according to the disclosure includes a display having a transparent display area a sensor configured to provide data regarding objects viewable through the transparent display area by a user of the HMD a processing unit communicatively coupled with the display and the sensor. The processing unit is configured to perform functions including causing the display to show a first visual element in the transparent display area where the first visual element is selectable by the user and defining a first active region of the transparent display area such that the first visual element is disposed within a portion of the first active region. The processing unit is further configured to use the data from the sensor to track a physical object that appears from a user s perspective within the transparent display area. Moreover the processing unit is also configured to determine at least a portion of the physical object appears from the user s perspective within the first active region indicating a selection of the first visual element and manipulate one or more elements displayed on the display based on the selection.

An example computer readable storage medium according to the disclosure is encoded with instructions for causing an HMD to perform functions including causing a first visual element to be displayed in a transparent display area of the HMD where the first visual element being selectable by a user of the HMD and defining a first active region of the transparent display area larger than the first visual element such that the first visual element is disposed within a portion of the first active region. The instructions also cause an HMD to track a physical object that appears from a user s perspective within the transparent display area and determine at least a portion of the physical object appears from the user s perspective within the first active region indicating a selection of the first visual element. The instructions further cause an HMD to manipulate one or more elements displayed in the transparent display area based on the selection.

An example apparatus according to the disclosure includes means for causing a first visual element to be displayed in a transparent display area of an HMD where the first visual element is selectable by a user of the HMD. The apparatus also includes means for defining a first active region of the transparent display area such that the first visual element is disposed within a portion of the first active region and means for tracking a physical object that appears from a user s perspective within the transparent display area. The apparatus further includes means for determining at least a portion of the physical object appears from the user s perspective within the first active region indicating a selection of the first visual element and means for manipulating with a processing unit one or more elements based on the selection.

Items and or techniques described herein may provide one or more of the following capabilities as well as other capabilities not mentioned. Techniques can provide for easily navigating a GUI by providing visual elements within large active regions. Furthermore a selected visual element and corresponding active region may be replaced with additional visual elements and corresponding active regions providing for example a submenu for further interaction. These and other embodiments along with many of its advantages and features are described in more detail in conjunction with the text below and the attached figures.

The following description is provided with reference to the drawings where like reference numerals are used to refer to like elements throughout. While various details of one or more techniques are described herein other techniques are also possible. In some instances structures and devices are shown in block diagram form in order to facilitate describing various techniques.

Embodiments of the present invention are directed toward enabling user interaction with a head mounted display HMD . HMDs are devices that can display information to a user wearing the device. Furthermore because these devices are worn on a user s head HMDs can have capabilities that are unachievable in other displays. For example HMD s can provide a transparent display area within a user s field of view in which a user can view both physical objects and virtual objects on the display. Some embodiments can provide augmented reality functionality by overlaying physical objects viewed by a user with digital content such as text pictures and or video associated with the physical objects or associated with the user s location and or context for example. Embodiments of the present invention further enable a physical object seen and or manipulated by a user to quickly select items in and or perform other interactions with a user interface e.g. a graphical user interface or GUI displayed by the HMD.

Although examples provided herein discuss a single physical object e.g. a hand finger etc. used to interact with an HMD the techniques provided herein can easily allow a user to interact with a user interface displayed by an HMD with two or more physical objects. Further although embodiments are described herein with respect to a HMD those of skill in the art will appreciate that other displays or other forms of heads up displays may be utilized. For example embodiments described herein may be implemented with respect to one or more contact lenses that a user may wear and or may be implemented in another form of display through which a user may perceive a field of view.

At least a portion of the displays is transparent providing a transparent display area that enables a user to view not only visual elements shown on the displays but also physical objects in the user s surroundings. The level of transparency of the visual elements shown on the displays may vary depending on the desired functionality of the displays settings of a GUI shown on the displays and or a software application executed by the HMD e.g. a video a map an Internet browser etc. . Although embodiments shown in illustrate displays positioned in a eyeglasses like frame other technologies capable of providing a transparent display area e.g. a retinal projector contact lens es or other optical system can be utilized in other embodiments.

The outward facing camera s can capture images of the user s surroundings including the user s hand and or other objects that can be controlled by the user to provide input to the HMD . As indicated in the cameras can be RGB red green blue and or RGBD red green blue plus depth cameras but other embodiments may include other types of cameras and or sensors that provide images and or other information to a processing unit that enables the HMD to track a physical object in front of the user. In one embodiment for example an HMD can utilize a single RGB camera to track a physical object s vertical and horizontal position. Other embodiments can employ multiple cameras and or a camera capable of determining depth e.g. time of flight ranging camera to track a physical object s depth as well. The camera s can have a field of view that enables the HMD to track a physical object that appears within the transparent display area from the perspective of the user. Embodiments may switch to a low power mode when the physical object is not within the transparent display area from the perspective of the user . In some embodiments the camera s can have a field of view that is broader than the transparent display area to allow the HMD to begin executing and or scale up object tracking algorithms when the HMD determines the physical object is approaching the transparent display area from the perspective of the user .

The HMD can execute tracking algorithms that receive images from the camera s and track certain recognized objects. Furthermore the camera s and tracking algorithms can be calibrated to determine the location of a tracking point on a physical object as it appears in the transparent display area from the perspective of the user . In the view shown in this tracking point corresponds to the fingertip of the user s hand . The manipulation of the physical object e.g. user s hand and corresponding movement of the tracking point can enable the user to interact with a GUI and or other visual elements shown in the transparent display area . It can be noted that the tracking point may or may not be highlighted in the transparent display area depending on desired functionality.

Any of a variety of objects can be used to allow the user to interact with the GUI such as the user s hand s a specialized apparatus and or any other object recognizable by the HMD. User settings of the HMD may allow a user to designate the object s to use for GUI interaction. As explained in more detail below object tracking can utilize any of a variety of visual tracking methods which may involve determining various traits of the object such as the shape color and or movement. In some embodiments depth filtering can be utilized to help ensure an object tracked for GUI interaction is controlled by the user. Thus the HMD will allow GUI interaction after the HMD determines the object is within a threshold distance from the HMD. Means for making this determination can include a processing unit coupled with sensors capable of determining distances such as camera s e.g. stereoscopic cameras a depth camera etc. proximity sensors and the like.

In the HMD recognizes a physical object for GUI interaction and establishes a tracking point . As discussed previously the tracking point may or may not be highlighted or otherwise indicated in the transparent display area . The physical object can be any physical object e.g. a body part a pointing device etc. that can be recognized and tracked by the HMD . Some embodiments may allow a user to designate which physical object can be used to interact with the HMD s GUI.

In some embodiments the HMD may not establish a tracking point and or evoke the GUI of the HMD for user interaction e.g. display a first set of visual elements and or options until a triggering event has occurred. This can help prevent unintended user interaction with the GUI. In some embodiments the triggering event could be that the object engages in a certain pose such as a hand with a pointer finger extended as shown in . In other embodiments the triggering event could be that a recognized physical object appears in the transparent display area for at least a threshold amount of time. Some embodiments could require both. For example an HMD could activate a GUI for user interaction after determining that the user s hand is has a pointer finger extended in an engagement pose and has been located within a field of view of transparent display area from the user s perspective for at least 2 seconds. Other embodiments may activate a GUI after periods of time greater or less than 2 seconds depending on desired functionality. In some embodiments the triggering event includes the user s hand and or finger being located in a particular or predetermined portion of the display area for example near an edge or in particular corner of the display area . Various other triggering events can be utilized.

In the GUI of the HMD displays a set of visual elements corresponding to options from which the user can select. Each visual element can be associated with and or surrounded by a respective active region . In some embodiments the elements and or the region may be displayed in a virtual plane with respect to the user. The active regions allow a user to select an option corresponding to a visual element by moving the tracking point to the respective active region . Depending on the placement of the visual elements and the desired size and shape of the active regions one or more borders of an active region may extend to an edge of the transparent display area . Further it may be easier for the user to move his hand back and forth in order to designate an element to select rather than having to place his hand or finger in a particular location closer to the bottom or top of the display area . Further such selection areas may often be used even when the user is walking or the user s hand is not steady because the precision required to select a region may be less than the precision required to select an element in some embodiments. In some embodiments identifying the user s hand as being in any portion of the region as opposed to touching a particular element may use less processing power and or latency and or images with lower resolution. In some embodiments the visual elements are not displayed at an approximately consistent vertical height as illustrated in . For example one or more of the elements may vary in vertical placement while still being located within a respective region that is substantially columnar.

Although active regions may take on various shapes and sizes columns can be particularly helpful in facilitating easy GUI interaction. This is because when the transparent display area is partitioned into columns visual elements can be located at a vertically central location of the transparent display area that allows the user to easily see the visual elements . Additionally because the user s hand will likely enter the transparent display area from the bottom edge column shaped active regions can facilitate selection because they can extend downward toward the bottom edge of the transparent display area . In some configurations the column shaped active regions can extend all the way to the bottom edge.

The way active areas are shown can vary depending on the embodiment which may be determined by one or more user settings. For example some embodiments may allow all of the active regions to be shown at once. That is the active region for each visual element is shown e.g. highlighted having a visible border etc. . In other embodiments a particular active region may not be shown until the tracking point enters that region e.g. as shown in . In some embodiments the regions may not be shown at all but are used to determine a user selection.

As an example of a GUI in which a visual element may be replaced with multiple visual elements a top level menu can include a number of visual elements as shown in representing different functions of the HMD such as multimedia Internet search navigation web browser and the like where each visual element is positioned within a column shaped active region . If the user selects the web browser visual element a web browser application may be evoked. However if the user selects the multimedia visual element the column shaped active region in which the multimedia visual element was located may be split into multiple active regions each with its own visual element representing a type of multimedia such as videos and music. Moreover each of these additional visual elements and respective active regions may be split up even further into types of videos or music for example genres playlists movies TV shows etc. in a similar fashion such that there are multiple levels of submenus. Because each submenu may require an additional level of accuracy the amount of submenu levels may depend on the options chosen and or the desired functionality. For example because movies are expected to require more attention from the user than music movies may have more levels of submenus which may require more accuracy to select than music. Furthermore levels of submenus may switch between vertical and horizontal alignments. For example if the user selects visual element and there are submenu items associated with then those submenu items may again be distributed horizontally similar to and so on.

Of course the functionality illustrated in are provided as an example of how a user may quickly navigate through menu and submenu options without removing the unselected visual elements from the display. Alternatively selection of a visual element may evoke a new set of visual elements representative of a submenu which replaces the original menu altogether.

Although active regions have been illustrated herein as columns of the transparent display area embodiments are not so limited. illustrate how active regions can vary depending on the layout of the visual elements . for example illustrate how active regions can be located in upper lower left and right quadrants of the transparent display area . Selection of a visual element may be similar to embodiments described previously. Other embodiments may have more visual elements or fewer visual elements which may be positioned differently e.g. in a grid circle or other fashion . In such embodiments active regions are positioned to encompass the visual elements correspondingly. Furthermore embodiments may include spaces between active regions that may not correspond with any visual element or cause a selection to be made which may make visual element selection easier.

In addition to tracking the location of a tracking point an HMD may further be configured to track the motion of a tracking point . Such functionality can enable a user to utilize certain motions such as swipes flips and the like. This can enable a user to navigate through menu items and or other content with even more efficiency than by navigation through position tracking alone. illustrate such an embodiment.

In a user can move one or more visual elements by selecting a visual element and swiping in a certain direction for example with the entire hand or with just one or more fingers in which case the HMD causes the visual element to move correspondingly in the swiped direction as shown in . Optionally the user may simply swipe anywhere in the active region corresponding to a visual element . As illustrated the other visual elements may move correspondingly as well allowing the user to scroll through multiple visual elements easily. Although show the user moving all visual elements one position to the left i.e. moving visual elements 1 5 to the left one position to show visual elements 2 6 swiping may result in greater movement of the visual elements depending on the speed motion and or other characteristics of the swipe. Furthermore swiping may be done in any area of the screen in some embodiments which may affect functionality. For example a user may swipe below the displayed visual elements to indicate to the HMD that the user would like to scroll through the visual elements rather than select one of them. In embodiments where the user places a finger on a visual element or active region and then swipes the time for holding a finger still to activate a swipe scroll may be less than the time for holding the finger still to select the visual element .

These GUI interaction techniques can be extended to augmented reality scenarios enabling a user to interact with a piece of augmented reality content related to a physical object. Such an augmented reality scenario can occur when an HMD identifies a physical location object surface texture image etc. using a camera and or other sensors. The HMD can then analyze the physical location object surface texture image etc. and compare it with a local or online database in an augmented reality and or visual search application. The analysis can return digital content that the HMD displays such that it appears from the perspective of the user in physical proximity to the identified location object surface texture image etc. In some embodiments the digital content appears to be anchored to or displayed on the identified object surface etc. The digital content can have interactive elements surrounded by or including active regions of the screen enabling a user to interact with the displayed digital content. For example digital content can include a button that says press me for more information. In some embodiments an active region may be defined around the button so as to extend in a region beyond the button and or to extend substantially to an edge of the display of the HMD from the button similar to certain of the embodiments described above. In some embodiments the button and or active region appears to be anchored to or displayed on the object surface etc. which the active region and or button is associated with.

The HMD can track the user s hand or other object enabling the user to interact with any interactive element s of the digital content. For example the user can select the interactive element in one or more of various ways such as by moving the user s finger position such that it overlaps with an interactive element from the perspective of the user engaging in a selection pose performing a predetermined movement and or keeping a finger or other object in the element s proximity for a threshold amount of time. The HMD after recognizing selection of the interactive element can alter the rendering of the digital content and or launch a second set of digital content. Depending on desired functionality the second set of digital content also may be anchored to the identified physical location object surface texture image etc. from the perspective of the user .

This augmented reality functionality can be used in countless scenarios to enable the user to learn more about the user s surroundings. For example if an HMD recognizes a movie poster within the field of view of the user a virtual play trailer button can appear in the HMD s display which can be activated with a finger press. In another example a user can spin a virtual 3D model augmented onto a magazine cover with a flick of the hand. In yet another example a user can look at a billboard and be shown media playback aligned with the physical billboard. Countless other scenarios are contemplated. Thus in addition to navigating a 2D GUI anchored to the transparent display techniques provided herein can be expanded to other augmented reality scenarios enabling interactions with elements of digital content that can be anchored from the perspective of the user to the physical world. Because these interactive elements are bound to physical objects in the user s surroundings corresponding active region regions on the HMD s display may move and scale relative to the physical object s position in relation to the user s point of view.

At block visual element is displayed in a transparent display area of the HMD. As indicated above the transparent display area can be at least a portion of a display and or display means of an HMD configured to allow a user to view visual elements shown on the display as well as physical object in the user s surroundings. In some embodiments such as those utilizing a retinal projector the transparent display area can be an area within the field of view of a user that may not necessarily correspond to the area on a physical display or screen. The visual element can be for example a visual element displayed as part of a GUI of the HMD. Means for performing the functionality of block can include an operating system software application processor display and or other means as described below with respect to .

Here it can be noted that before the visual element is displayed in the transparent display area of the HMD the HMD and or device communicatively coupled therewith may determine a set of selectable visual elements to display from for example a predefined user menu obtained from an application database application programming interface API and the like. The HMD may also determine the layout of the visual elements e.g. utilizing columns quadrants etc. which may be based on any of a variety of factors such as application type number of visual elements user settings and the like. In some embodiments a layout may be determined based on one or more user setting e.g. a preference or setting of the user for columns rather than quadrants and then the set of visual items arranged on the display such that the determined layout may be utilized. In other embodiments the layout may be automatically determined based on the type display properties or quantity of visual elements in the set. In some embodiments the layout may be automatically determined based on a field of view of the user for example to maximize a visibility of the visual elements as displayed to the user.

At block an active region within the transparent display area is defined such that the visual element is disposed within a portion of the active region. The active region can be for example a region surrounding a user selectable visual element shown on a transparent display area. The active region itself may or may not be highlighted and or otherwise indicated in the transparent display area. Defining the active region may be performed for example by a processing unit memory and or other computing means which can be part of a computing system incorporated into and or communicatively coupled with the HMD. The active regions may be defined in any of a plurality of configurations. For example a display may be subdivided for example as shown in into a plurality of regions that substantially occupy an entire display or field of view in some embodiments. The active regions in these embodiments may be substantially uniform in shape and or size or may vary. The active regions may be defined as extending from an icon or other display element for example substantially to the edge of a screen for example as shown in . Means for performing the functionality of block can include an operating system software application processor display and or other means as described below with respect to .

As discussed previously the active regions may be columns rows or other shapes. The active regions may be characterized as regions including an area where an object of interest is not being displayed for example the active regions may encompass areas where the visual elements are not being displayed . In such embodiments the active regions may be defined with respect to an icon or other visual element or may be separately defined. In embodiments where the active regions are defined with respect to an icon or other visual element the active regions may be defined to encompass an area which is substantially larger than the icon or visual element. For example the active region may be defined to be twice as large as the icon or display element or 3 5 or 10 times as large or any other size or may be displayed so as to be larger than a threshold size when viewed by the user regardless of how large the icon or visual element is. Moreover in some embodiments active regions may be associated with interactive elements digital content and or physical objects in an augmented reality or other software application. Such active regions may change in size shape and location within the display area as for example a user moves in relation to a physical object. Other configurations of the active regions may also be implemented and or defined. Further an area in which to display a visual element within a determined or defined active region may vary. For example if columnar active regions are being used the HMD may determine a vertical location within each column to display a respective visual element or may determine to change a size or shape of an visual element within an active region.

At block a physical object that appears within the display area from the user s perspective is tracked. Means for tracking can be performed by a camera sensor and or other components configured to capturing image and or position measurements communicatively connected with a processing unit memory and or other computing means configured to determine a position based on the image and or position measurements. Such means are described in additional detail below with respect to .

As indicated above components for tracking the physical object can be calibrated with components for displaying visual elements in the transparent display area enabling the HMD to determine what the user sees. Correspondingly at block the method includes determining that at least a portion of the physical object appears within the active region of the display area from the user s perspective. Here again means for making the determination can include a processing unit memory and or other computing means as described in regard to and may employ tracking means described below. In one embodiment an extremum or other end of a range may be detected as the tracking point . In one embodiment an active region is selected if a tracked object occupies a majority or large portion of the region. In for example the user s finger and or hand occupies a substantial portion of the active region such as at least the bottom half of the active region . In some embodiments the active regions may be configured to substantially mirror a shape of an object used to select items or otherwise control the GUI for example the active regions may comprise columns when an elongated control object or selection object such as a finger is used .

Tracking means may engage any of a variety of tracking algorithms. Certain tracking algorithms may simply track a single point e.g. a coordinate on the transparent display area and or region associated with the object and or a location on the object e.g. a fingertip . More sophisticated tracking algorithms may track other features such as the object s shape e.g. to recognize an engagement pose distance e.g. to determine whether a user presses a button and more. Embodiments may be able to track multiple and or different physical objects for GUI interaction. Furthermore embodiments are not limited to any one method. In some embodiments the HMD may be configured to accept inputs from only certain objects. For example the HMD may be configured to detect whether an object is a finger or a hand subsequently accept inputs from the finger or hand and reject inputs from other objects. Additionally or alternatively an HMD may include eye tracking or gaze tracking that can be used in combination with finger and or hand tracking. This can increase confidence that the user is trying to select an object or region because the user s gaze will probably be looking at that region confirming the selection. Optionally in scenarios where it is likely the user is looking at his or her finger the user s gaze may also be tracked to increase confidence of the tracked position of the finger.

At block a user interface is manipulated based on the determination made at block . Here again means for manipulating the user interface can include a processing unit memory and or other computing means coupled to a display showing the user interface e.g. a GUI .

It should be appreciated that the specific steps illustrated in provide an example of a method of enabling user interaction with an HMD. Alternative embodiments may include alterations to the embodiments shown. For example alternative embodiments may include defining the active region within the transparent display area at a different point during the method . Yet other embodiments may include performing actions to calibrate the physical object tracking components with the display components of the HMD. Furthermore additional features may be added removed or combined depending on the particular applications. One of ordinary skill in the art would recognize many variations modifications and alternatives.

The computer system is shown comprising hardware elements that can be electrically coupled via a bus or may otherwise be in communication as appropriate . The hardware elements may include a processing unit such as processor s which can include without limitation one or more general purpose processors one or more special purpose processors such as digital signal processors graphics acceleration processors and or the like and or other processing means which as stated above can be utilized to perform various steps such as those described in relation to . Hardware elements may also include one or more input devices which can include without limitation one or more camera sensors e.g. light heat ultrasound RF and or other sensors capable of providing data for tracking an object and or other tracking devices which can be included and or otherwise utilized together with the processor s as tracking means to track a physical object as described herein. Other devices such as a touch pad keyboard microphone and or the like may also be included. One or more output devices are also included. These output devices can include one or more transparent display devices and or other display means as well as speakers and or other devices with may be utilized to perform one or more steps as described in relation to . For example when the system is implemented in an HMD the output device may include a transparent or semi transparent display. In embodiments where the device is implemented in a phone or other device controlling the HMD the output device may comprise a touchscreen or may be omitted.

The computer system may further include and or be in communication with one or more non transitory storage devices which can comprise without limitation local and or network accessible storage and or can include without limitation a disk drive a drive array an optical storage device a solid state storage device such as a random access memory RAM and or a read only memory ROM which can be programmable flash updateable and or the like. Such storage devices may be configured to implement any appropriate data stores including without limitation various file systems database structures and or the like.

The computer system might also include a communications subsystem which can include without limitation a modem a network card wireless or wired an infrared communication device a wireless communication device and or a chipset such as a Bluetooth device an 1002.11 device a WiFi device a WiMax device cellular communication facilities etc. and or the like. The communications subsystem may include one or more input and or output communication interfaces to permit data to be exchanged with a network other computer systems and or any other electrical devices peripherals. In many embodiments the computer system will further comprise a working memory which can include a RAM or ROM device as described above.

The computer system also can comprise software elements shown as being currently located within the working memory including an operating system device drivers executable libraries and or other code such as one or more application s which may comprise computer programs provided by various embodiments and or may be designed to implement methods and or configure systems provided by other embodiments as described herein. Merely by way of example a portion of one or more procedures described with respect to the method s discussed above such as the method described in relation to might be implemented as code and or instructions executable by a computer and or a processing unit within a computer in an aspect then such code and or instructions can be used to configure and or adapt a general purpose computer or other device to perform one or more operations in accordance with the described methods.

A set of these instructions and or code might be stored on a non transitory computer readable storage medium such as the storage device s described above. In some cases the storage medium might be incorporated within a computer system such as computer system . In other embodiments the storage medium might be separate from a computer system e.g. a removable medium such as an optical disc and or provided in an installation package such that the storage medium can be used to program configure and or adapt a general purpose computer with the instructions code stored thereon. These instructions might take the form of executable code which is executable by the computer system and or might take the form of source and or installable code which upon compilation and or installation on the computer system e.g. using any of a variety of generally available compilers installation programs compression decompression utilities etc. then takes the form of executable code.

It will be apparent to those skilled in the art that substantial variations may be made in accordance with specific requirements. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets etc. or both. Further connection to other computing devices such as network input output devices may be employed.

As mentioned above in one aspect some embodiments may employ a computer system such as the computer system to perform methods in accordance with various embodiments of the invention. According to a set of embodiments some or all of the procedures of such methods are performed by the computer system in response to processor executing one or more sequences of one or more instructions which might be incorporated into the operating system and or other code such as an application program contained in the working memory . Such instructions may be read into the working memory from another computer readable medium such as one or more of the storage device s . Merely by way of example execution of the sequences of instructions contained in the working memory might cause the processor s to perform one or more procedures of the methods described herein. Additionally or alternatively portions of the methods described herein may be executed through specialized hardware.

The terms machine readable medium and computer readable medium as used herein refer to any medium that participates in providing data that causes a machine to operate in a specific fashion. In an embodiment implemented using the computer system various computer readable media might be involved in providing instructions code to processor s for execution and or might be used to store and or carry such instructions code. In many implementations a computer readable medium is a physical and or tangible storage medium. Such a medium may take the form of a non volatile media or volatile media. Non volatile media include for example optical and or magnetic disks such as the storage device s . Volatile media include without limitation dynamic memory such as the working memory .

Common forms of physical and or tangible computer readable media include for example a floppy disk a flexible disk hard disk magnetic tape or any other magnetic medium a CD ROM any other optical medium any other physical medium with patterns of holes a RAM a PROM EPROM a FLASH EPROM any other memory chip or cartridge or any other medium from which a computer can read instructions and or code.

Various forms of computer readable media may be involved in carrying one or more sequences of one or more instructions to the processor s for execution. Merely by way of example the instructions may initially be carried on a magnetic disk and or optical disc of a remote computer. A remote computer might load the instructions into its dynamic memory and send the instructions as signals over a transmission medium to be received and or executed by the computer system .

The communications subsystem and or components thereof generally will receive signals and the bus then might carry the signals and or the data instructions etc. carried by the signals to the working memory from which the processor s retrieves and executes the instructions. The instructions received by the working memory may optionally be stored on a non transitory storage device either before or after execution by the processor s .

The methods systems and devices discussed above are examples. Various configurations may omit substitute or add various procedures or components as appropriate. For instance in alternative configurations the methods may be performed in an order different from that described and or various stages may be added omitted and or combined. Also features described with respect to certain configurations may be combined in various other configurations. Different aspects and elements of the configurations may be combined in a similar manner. Also technology evolves and thus many of the elements are examples and do not limit the scope of the disclosure or claims.

Specific details are given in the description to provide a thorough understanding of example configurations including implementations . However configurations may be practiced without these specific details. For example well known circuits processes algorithms structures and techniques have been shown without unnecessary detail in order to avoid obscuring the configurations. This description provides example configurations only and does not limit the scope applicability or configurations of the claims. Rather the preceding description of the configurations will provide those skilled in the art with an enabling description for implementing described techniques. Various changes may be made in the function and arrangement of elements without departing from the spirit or scope of the disclosure.

Also configurations may be described as a process which is depicted as a flow diagram or block diagram. Although each may describe the operations as a sequential process many of the operations can be performed in parallel or concurrently. In addition the order of the operations may be rearranged. A process may have additional steps not included in the figure. Furthermore examples of the methods may be implemented by hardware software firmware middleware microcode hardware description languages or any combination thereof. When implemented in software firmware middleware or microcode the program code or code segments to perform the necessary tasks may be stored in a non transitory computer readable medium such as a storage medium. Processors may perform the described tasks.

Having described several example configurations various modifications alternative constructions and equivalents may be used without departing from the spirit of the disclosure. For example the above elements may be components of a larger system wherein other rules may take precedence over or otherwise modify the application of the invention. Also a number of steps may be undertaken before during or after the above elements are considered. Accordingly the above description does not bound the scope of the claims.

