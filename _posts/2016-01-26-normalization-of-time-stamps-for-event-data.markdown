---

title: Normalization of time stamps for event data
abstract: Methods and apparatus consistent with the invention provide the ability to organize, index, search, and present time series data based on searches. Time series data are sequences of time stamped records occurring in one or more usually continuous streams, representing some type of activity. In one embodiment, time series data is organized into discrete events with normalized time stamps and the events are indexed by time and keyword. A search is received and relevant event information is retrieved based in whole or in part on the time indexing mechanism, keyword indexing mechanism, or statistical indices calculated at the time of the search.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09514175&OS=09514175&RS=09514175
owner: Splunk Inc.
number: 09514175
owner_city: San Francisco
owner_country: US
publication_date: 20160126
---
The present application is a continuation of U.S. patent application Ser. No. 14 929 248 filed on Oct. 30 2015 which is a continuation of Ser. No. 14 611 170 filed on Jan. 30 2015 which is a continuation of U.S. patent application Ser. No. 13 353 135 filed on Jan. 18 2012 issued as U.S. Pat. No. 9 002 854 on Apr. 7 2015 which is a continuation of U.S. patent application Ser. No. 11 868 370 filed Oct. 5 2007 issued as U.S. Pat. No. 8 112 425 on Feb. 7 2012 which claims priority under 35 U.S.C. 119 e to U.S. Provisional Patent Application No. 60 828 283 filed Oct. 5 2006. The subject matter of all of the foregoing is incorporated herein by reference in its entirety.

This invention relates generally to information organization search and retrieval and more particularly to time series data organization search and retrieval.

Time series data are sequences of time stamped records occurring in one or more usually continuous streams representing some type of activity made up of discrete events. Examples include information processing logs market transactions and sensor data from real time monitors supply chains military operation networks or security systems . The ability to index search and present relevant search results is important to understanding and working with systems emitting large quantities of time series data.

Existing large scale search engines e.g. Google and Yahoo web search are designed to address the needs of less time sensitive types of data and are built on the assumption that only one state of the data needs to be stored in the index repository for example URLs in a Web search index records in a customer database or documents as part of a file system. Searches for information generally retrieve only a single copy of information based on keyword search terms a collection of URLs from a Website indexed a few days ago customer records from close of business yesterday or a specific version of a document.

In contrast consider an example of time series data from a typical information processing environment shown in . Firewalls routers web servers application servers and databases constantly generate streams of data in the form of events occurring perhaps hundreds or thousands of times per second. Here historical data value and the patterns of data behavior over time are generally as important as current data values. Existing search solutions generally have little notion of time based indexing searching or relevancy in the presentation of results and don t meet the needs of time series data.

Compared to full text search engines which organize their indices so that retrieving documents with the highest relevance scores is most efficient an engine for searching time series data preferably would organize the index so that access to various time ranges including less recent time ranges is efficient. For example unlike for many modem search engines there may be significantly less benefit for a time series search engine to cache the top 1000 results for a particular keyword.

On the other hand given the repetitive nature of time series data opportunities for efficiency of index construction and search optimization are available. However indexing time series data is further complicated because the data can be collected from multiple different sources asynchronously and out of order. Streams of data from one source may be seconds old and data from another source may be interleaved with other sources or may be days weeks or months older than other sources. Moreover data source times may not be in sync with each other requiring adjustments in time offsets post indexing. Furthermore time stamps can have an almost unlimited number of formats making identification and interpretation difficult. Time stamps within the data can be hard to locate with no standard for location format or temporal granularity e.g. day hour minute second sub second .

Searching time series data typically involves the ability to restrict search results efficiently to specified time windows and other time based metadata such as frequency distribution of inter arrival time and total number of occurrences or class of result. Keyword based searching is generally secondary in importance but can be powerful when combined with time based search mechanisms. Searching time series data requires a whole new way to express searches. Search engines today allow users to search by the most frequently occurring terms or keywords within the data and generally have little notion of time based searching. Given the large volume and repetitive characteristics of time series data users often need to start by narrowing the set of potential search results using time based search mechanisms and then through examination of the results choose one or more keywords to add to their search parameters. Timeframes and time based metadata like frequency distribution and likelihood of occurrence are especially important when searching time series data but difficult to achieve with current search engine approaches. Try to find for example all stories referring to the Space Shuttle between the hours of 10 AM and 11 AM on May 10 2005 or the average number of Space Shuttle stories per hour the same day with a Web based search engine of news sites. With a focus on when data happens time based search mechanisms and queries can be useful for searching time series data.

Some existing limited applications of time based search exist in specific small scale domains. For example e mail search is available today in many mainstream email programs and web based email services. However searches are limited to simple time functions like before after or time ranges the data sets are generally small scale and highly structured from a single domain and the real time indexing mechanisms are append only usually requiring the rebuilding of the entire index to interleave new data.

Also unique to the cyclicality of time series data is the challenge of presenting useful results. Traditional search engines typically present results ranked by popularity and commonality. Contrary to this for time series data the ability to focus on data patterns and infrequently occurring or unusual results may be important. To be useful time series search results preferably would have the ability to be organized and presented by time based patterns and behaviors. Users need the ability to see results at multiple levels of granularity e.g. seconds minutes hours days and distribution e.g. unexpected or least frequently occurring and to view summary information reflecting patterns and behaviors across the result set. Existing search engines on the other hand generally return text results sorted by key word density usage statistics or links to or from documents and Web pages in attempts to display the most popular results first.

In one class of time series search engine it would be desirable for the engine to index and allow for the searching of data in real time. Any delay between the time data is collected and the time it is available to be searched is to be minimized. Enabling real time operation against large frequently changing data sets can be difficult with traditional large scale search engines that optimize for small search response times at the expense of rapid data availability. For example Web and document search engines typically start with a seed and crawl to collect data until a certain amount of time elapses or a collection size is reached. A snapshot of the collection is saved and an index is built optimized and stored. Frequently accessed indices are then loaded into a caching mechanism to optimize search response time. This process can take hours or even days to complete depending on the size of the data set and density of the index. Contrast this with a real time time series indexing mechanism designed to minimize the time between when data is collected and when the data is available to be searched. The ability to insert delete and reorganize indices on the fly as data is collected without rebuilding the index structure is essential to indexing time series data and providing real time search results for this class of time series search engines.

Other software that is focused on time series e.g. log event analyzers such as Sawmill or Google s Sawzall can provide real time analysis capabilities but are not search engines per se because they do not provide for ad hoc searches. Reports must be defined and built in advance of any analysis. Additionally no general keyword based or time based search mechanisms are available. Other streaming data research projects including the Stanford Streams project and products from companies like StreamBase Systems can also produce analysis and alerting of streaming data but do not provide any persistence of data indexing time based or keyword based searching.

There exists therefore a need to develop other techniques for indexing searching and presenting search results from time series data.

Methods and apparatus consistent with the invention address these and other needs by allowing for the indexing searching and retrieval of time series data using a time series search engine TSSE . In one implementation one aspect of TSSEs is the use of time as a primary mechanism for indexing searching and or presentation of search results. A time series search language TSSL specific to time based search mechanisms is used to express searches in human readable form and results are presented using relevancy algorithms specific to time series data. Search expression and results presentation are based on key concepts important to searching time series data including but not limited to time windows frequency distribution patterns of occurrences and related time series data points from multiple disparate sources.

In one aspect of the invention multiple sources of time series data are organized and indexed for searching and results are presented upon user or machine initiated searches. In another aspect a time series search engine TSSE includes four parts 1 a time stamp process 2 an indexing process 3 a search process and 4 a results presentation process.

In one aspect of the invention a computer implemented method for time searching data includes the following steps. Time series data streams are received. One example of time series data streams includes server logs and other types of machine data i.e. data generated by machines . The time series data streams are time stamped to create time stamped events. The time stamped events are time indexed to create time bucketed indices which are used to fulfill search requests. Time series search request are executed at least in part by searching the time bucketed indices.

In certain implementations time stamping the time series data streams includes aggregating the time series data streams into events and time stamping the events. For example the events may be classified by domain and then time stamped according to their domain. In one approach for events that are classified in a domain with a known time stamp format the time stamp is extracted from the event. However for events that are not classified in a domain with a known time stamp format the time stamp is interpolated.

In another aspect of the invention time bucketed indices are created by assigning the time stamped events to time buckets according to their time stamps. Different bucket policies can be used. For example the time buckets may all have the same time duration or may have different time durations. In addition time buckets may be instantiated using a lazy allocation policy. The time stamped events may also be segmented and the segments used to determine time bucket indices. Various forms of indexing including hot indexing warm indexing and speculative indexing may also be used.

The creation of time bucket indices facilitates the execution of time series searches. In one approach a time series search request is divided into different sub searches for the affected time buckets with each sub search executed across the corresponding time bucket index.

Other aspects of the invention include software computer systems and other devices corresponding to the methods described above and applications for all of the foregoing.

The figures depict embodiments of the present invention for purposes of illustration only. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles of the invention described herein.

Aspects of the invention will be described with respect to the first picture in the information processing environment but the invention can also be used with other time series data environments and applications including the other environments shown in .

The arrival of time series data streams at the TSSE can be effected by having the TSSE gather them directly or by having a user supplied script collect preprocess and deliver them to a default TSSE collection point. This architecture preferably tolerates data arriving late and temporally out of order. Currently most sources of time series data are not designed for sophisticated processing of the data so the TSSE typically will collect or be fed raw time series data that are close to their native form. The TSSE can be situated in different locations so long as it has access to the time series data. For example one copy of the TSSE can be run on a single central computer or multiple copies can be configured in a peer to peer set up with each copy working on the same time series data streams or different time series data streams.

For example a user might want to locate all the events from a particular web server and a particular application server occurring within the last hour and which contain a specific IP address. In addition the search process may choose to initiate the creation of meta events at search time to handle time based and statistical summary indices useful in searching through repetitive temporal data. For example meta events may represent averages means or counts of actual events or more sophisticated pattern based behavior. In this case a user might want to search to find all the events occurring with a frequency of three per minute.

Upon completion the search process hands results from the selected indices to the presentation process which merges result sets ranks results and feeds the results to an API or user interface for presentation.

Process shown in of an exemplary implementation of a TSSE is to acquire streaming time series data identify individual events within the stream and assign time stamps to each event. An example time stamp process block diagram is shown in and includes several steps including event aggregation domain identification time extraction and time interpolation . Time series data streams are received as input to the time stamp process and then processed into individual time stamped events .

Step in the time stamp process of aggregates the streaming time series data into individual events 3 I 5. In our information processing example web server time series data streams may have a single line per event and be easy to identify. However an application server time series data stream may contain single events with a large number of lines making identification of individual events within the stream difficult.

In one implementation event aggregation uses feature extraction e.g. leading punctuation significant words white space and breaking characters and machine learning algorithms to determine where the event boundaries are. is a diagram of an event aggregation process suitable for use with the time stamp process of .

Given the repetitive yet dynamic nature of the time series data in our information processing example which data will be referred to as machine data or MD an effective aggregation process such as shown in preferably will learn about data formats and structure automatically. In one implementation learning is separated into different domains based on the source of MD . Domains can be general system types such as log files message bus traffic and network management data or specific types such as output of a given application or technology Sendmail logging data Oracle database audit data and J2EE messaging.

In this example event aggregation process the domain for a given source of MD is identified so that domain specific organization methods can be applied. Domains are determined through a learning process. The learning process uses collections of MD from well known domains as input and creates a source signature for each domain. In one implementation source signatures are generated from representative samples of MD by creating a hash table mapping punctuation characters to their frequency. While tokens and token values can change in MD collection in this particular implementation the signature generated by the frequency of punctuation is quite stable and reliable within a specific domain. Other implementations could use functions of the punctuation and tokens such as the frequencies of the first punctuation character on a line or the first capitalized term on a line. Given that source signatures can be large and hard to read signatures can have a corresponding label in the form of a number or text that can be machine generated or human assigned. For example the source signature for an Apache web server log might be programmatically assigned the label 205 or a user can assign the label Apache Server Log .

In one embodiment clustering is used to classify collected MD into domains according to their source signatures . As collections of MD are encountered each collection s signature is matched to the set of known source signatures by performing a nearest neighbor search. If the distance of the closest matching signature is within a threshold the closest matching signature s domain is assumed to be the domain of the source. If no best match can be found a new source signature can be created from the sample signature and a new source domain created. Alternatively a default source domain can be used. In one implementation the distance between two signatures is calculated by iterating over the union of attributes of the two signatures with the total signature distance being the average of distances for each attribute. For each attribute A the value of A on Signature I and Signature2 VI and V2 are compared and a distance is calculated. The distance for attribute A is the square of VI V2 IDF where IDF is the log N I IAI where N is the number of signatures and IAI is the number of signatures with attribute A.

Some MD sources are non textual or binary and cannot be easily processed unless a known process is available to convert the binary MD into textual form. To classify a source as textual or binary a sample MD collection is analyzed. Textual MD can also have embedded binary MD such as a memory dump and the classification preferably identifies it as such. In one implementation the textual binary classification works as follows. The sample is a set of lines of data where a line is defined as the data between new lines i.e. n carriage returns i.e. r or their combination i.e. r n . For each line if the line s length is larger than some large threshold such as 2 k characters or if the line contains a character with an ASCII value of zero 0 a count of Binary looking lines is incremented. Otherwise if the line s length is shorter than a length that one would expect most text lines to be below such as 256 characters a count of Text looking lines is incremented. If the number of Text looking lines is twice as numerous as the Binary looking lines other ratios can be used depending on the context the source is classified as text. Otherwise the source is classified as binary.

When the source signature for a collection of MD has been identified the corresponding aggregation rules are applied to the MD collection. Aggregation rules describe the manner in which MD from a particular domain is organized into event data by identifying the boundaries of events within a collection of MD for example how to locate a discrete event by finding its beginning and ending. In one implementation the method of aggregation learns without prior knowledge by grouping together multiple lines from a sample of MD . Often MD contains events that are anywhere from one to hundreds of lines long that are somehow logically grouped together.

The MD collection may be known a priori or may be classified as single line type i.e. containing only single line events or multi line type i.e. possibly containing multi line events prior to performing aggregation. For those MD collections that are classified as single line type aggregation is simple single line type MD collections are broken on each line as a separate event. Multi line type MD collections are processed for aggregation. In one implementation a MD collection is classified as a multi line type if 1 there is a large percentage of lines that start with spaces or are blank e.g. if more than 5 of the lines start with spaces or are blank or 2 there are too many varieties of punctuation characters in the first N punctuation characters. For example if the set of the first three punctuation characters found on each line has more than five patterns e.g. .. the collection might be classified as multi line.

Another aspect of aggregation methods is the ability to learn and codify into rules what constitutes a break between lines and therefore the boundary between events by analyzing a sample of MD. For example in one implementation an aggregation method compares every two line pair looking for statistically similar structures e.g. use of white space indentation and time stamps to quickly learn which two belong together and which two are independent. In one implementation aggregation works as follows. For each line first check if the line starts with a time stamp. If so then break. Typically lines starting with a time stamp are the start of a new event. For lines that do not start with a time stamp combine the current line with the prior line to see how often the pair of lines occurs one before the other as a percentage of total pairs in the MD sample. Line signatures are used in place of lines where a line signature is a more stable version of a line immune to simple numeric and textual changes. In this implementation signatures can be created by converting a line into a string that is the concatenation of leading white space any punctuation on the line and the first word on the line. The line 10 29 03 Host 191.168.0.1 rebooting normally is converted to .. Host. 

Now this current line signature can be concatenated with the previous line signature i.e. signature1 combined with signature2 and used as a combined key into a table of break rules. The break rule table maps the combined key to a break rule which determines whether there should be a break or not between the two lines i.e. whether they are part of different events or not . Break rules can have confidence levels and a more confident rule can override a less confident rule. Break rules can be created automatically by analyzing the co occurrence data of the two lines and what percent of the time their signatures occur adjacently. If the two line signatures highly co occur a new rule would recommend no break between them. Alternatively if they rarely co occur a new rule would recommend a break between them. For example if line signature A is followed by line signature B greater than 20 of the time A is seen then a break rule might be created to recommend no break between them. Rules can also be created based on the raw number of line signatures that follow proceed another line signature. For example if a line signature is followed by say ten different line signatures create a rule that recommends a break between them. If there is no break rule in the break rule table the default behavior is to break and assume the two lines are from different events. Processing proceeds by processing each two line pair updating line signature and co occurrence statistics and applying and learning corresponding break rules. At regular intervals the break rule table is written out to the hard disk or permanent storage.

Once the incoming time series stream has been aggregated into individual events the events and their event data are input into a time stamp identification step which determines whether or not the time series event data contains tokens that indicate a match to one of a collection of known time stamp formats. If so the event is considered to have a time stamp from a known domain and extraction is performed. Otherwise interpolation is performed.

If a known domain has been identified for an event the event is taken as input to a time stamp extraction step where the time stamp from the raw event data is extracted and passed with the event to the indexing process . In an exemplary implementation this timestamp extraction occurs by iterating over potential time stamp format patterns from a dynamically ordered list in order to extract a time to be recorded as the number of seconds that have passed since the Unix epoch 0 seconds 0 minutes 0 hour Jan. 1 1970 coordinated universal time not including leap seconds. Additionally the implementation takes into account time zone information and normalizes the times to a common offset. To increase performance the ordering of this list is determined using a move to front algorithm wherein whenever a match is found the matching pattern is moved to the beginning of the list. In such an implementation the most frequently occurring patterns are checked earliest and most often improving performance. The move to front lists may be maintained either for all time series data sources together on a per source basis to take advantage of the fact that the formats in a single source are likely to be similar or in some other arrangement.

In the case where the event did not contain a time stamp from a known domain then a timestamp is assigned to the event based on its context. In one implementation the time stamp is linearly interpolated from the time stamps of the immediately preceding and immediately following events from the same time series data stream. If these events also contain no time stamps from a known domain further earlier and or later events can be used for the interpolation. The time stamp extraction module automatically stores the time stamp of every hundredth event or some other configurable period from each time series data stream in order to facilitate time stamp interpolation . In another implementation time stamps are interpolated based on the time associated with the entire time series data stream including acquisition time creation time or other contextual meta time data.

Returning to in the indexing process indexes are created based on incoming event data . The indexing process organizes and optimizes the set of indices in an online fashion as they are extended with more events. An example TSSE indexing process is shown in and includes in one implementation several steps including bucketing segmenting archival allocation insertion committing to secondary storage merging buckets in secondary storage and expiring buckets in secondary storage .

Events indexed by the TSSE are often queried updated and expired using time based operators. By hashing the components of the index over a set of buckets organized by time the efficiency and performance of these operators can be significantly improved. The final efficiency of the bucketing will of course depend on the hardware configuration the order in which the events arrive and how they are queried so there is not a single perfect bucketing policy.

In one implementation buckets with a uniform extent are used. For example each time bucket can handle one hour s worth of data. Alternate policies might vary the bucket extents from one time period to another. For example a bucketing policy may specify that the buckets for events from earlier than today are three hour buckets but that the buckets for events occurring during the last 24 hours are hashed by the hour. In the information processing example a bucket might cover the period 01 15 2005 12 00 00 to 01 15 2005 14 59 59. In order to improve efficiency further buckets are instantiated using a lazy allocation policy i.e. as late as possible in primary memory i.e. RAM . In memory buckets have a maximum capacity and when they reach their limit they will be committed to disk and replaced by a new bucket. Bucket storage size is another element of the bucketing policy and varies along with the size of the temporal extent. Finally bucket policies typically enforce that buckets a do not overlap and b cover all possible incoming time stamps.

Step in indexing an event by time is to identify the appropriate bucket for the event based on the event s time stamp and the index s bucketing policy. Each incoming event is assigned to the time bucket where the time stamp from the event matches the bucket s temporal criteria. In one implementation we use half open intervals defined by a start time and an end time where the start time is an inclusive boundary and the end time is an exclusive boundary. We do this so that events occurring on bucket boundaries are uniquely assigned to a bucket. Following our example in the information processing environment a database server event with the time stamp of 01 15 2005 12 00 01 might be assigned to the above mentioned bucket.

Once an appropriate bucket has been identified for an event the raw event data is segmented . A segment also known as a token is a substring of the incoming event text and a segmentation is the collection of segments implied by the segmentation algorithm on the incoming event data. A segment sub string may overlap another substring but if it does it must be contained entirely within that substring. We allow this property to apply recursively to the containing substring so that the segment hierarchy forms a tree on the incoming text.

In one implementation segmentation is performed by choosing two mutually exclusive sets of characters called minor breakers and major breakers. Whenever a breaking character minor or major is encountered during segmentation of the raw data segments are emitted corresponding to any sequence of bytes that has at least one major breaker on one end of the sequence. For example if during segmentation a minor breaking character is found then a segment corresponding to the sequence of characters leading from the currently encountered minor breaker back to the last major breaker encountered is recorded. If a major breaker was encountered then the sequence of characters leading back to either the last major breaker or the last minor breaker whichever occurred most recently determines the next segment to be recorded.

Segmentation rules describe how to divide event data into segments also known as tokens . In one implementation a segmentation rule examines possible separators or punctuation within the event for example commas spaces or semicolons. An important aspect of segmentation is the ability to not only identify individual segments but also to identify overlapping segments. For example the text of an email address bob.smith corp.com can be broken into individual and overlapping segments and can be identified as individual segments and can also be identified as an overlapping segment. As described above in one implementation segmentation uses a two tier system of major and minor breaks. Major breaks are separators or punctuation that bound the outer most segment . Examples include spaces tabs and new lines. Minor breaks are separators or punctuation that break larger segments into sub segments for example periods commas and equal signs. In one implementation more complex separators and punctuation combinations are used to handle complex segmentation tasks for example handling Java exceptions in an application server log file.

An example of segmentation in our information processing example IP addresses could be broken down using white space as major breakers and periods as minor breakers. Thus the segments for the raw text 192.168.1.1 could be 

In another implementation certain segments may represent known entities that can be labeled and further understood algorithmically or by human added semantics. For example in the above representation 192.168.1.1 may be understood to be an IP address. Named entity extraction can be algorithmically performed in a number of ways. In one implementation the segment values or segment form from the same segment across multiple events is compared to an entity dictionary of known values or known forms.

In another implementation entity extraction techniques are used to identify semantic entities within the data. In one implementation search trees or regular expressions can be applied to extract and validate for example IP addresses or email addresses. The goal of extraction is to assist the segmentation process and provide semantic value to the data.

At this point in the process incoming events have time stamps segments and a time bucket associated with them. To create the persistent data structures that will be used later to perform lookups in the search process we store the raw data of the event with its segmentation create indices that map segments and time stamps to offsets in the event data store and compute and store metadata related to the indices.

Because the TSSE tolerates in near real time both the arrival of new events and new searches the system preferably is careful in managing access to disk. For the indexes this is accomplished by splitting index creation into two separate phases hot indexing and warm indexing. Hot indexes are managed entirely in RAM are optimized for the smallest possible insert time are not searchable and do not persist. Warm indexes are searchable and persistent but immutable. When hot indexes need to be made searchable or need to be persistent they are converted into warm indexes.

In the implementation shown in a hot index contains a packed array of segments a packed array of event addresses and their associated time stamps and a postings list that associates segments with their time stamped event addresses. For performance reasons the packed arrays can have hash tables associated with them to provide for quick removal of duplicates. When incoming events are being indexed each segment of the event is tested for duplication using the segment array and its associated hash. The event address is also tested for duplication against the event address array and its associated hash. If either of the attributes is a duplicate then the instance of that duplicate that has already been inserted into the packed array is used. Otherwise the new segment or event address is copied into the appropriate table and the associated hash table is updated. As events are inserted into the hot index the space associated with each of the packed arrays gets used. A hot slice is considered to be at capacity when one of its packed arrays fills up or when one of its hash tables exceeds a usage threshold e.g. if more than half of the hash table is in use . Once a hot index reaches capacity it cannot accept more segments for indexing. Instead it is converted to a warm index committed to disk and replaced with a new empty hot index.

Another feature of this particular system is speculative indexing. Based on earlier indexing processes new time buckets can be initialized using all or part of a representative completed bucket as an exemplar. In other words by keeping around copies of data that may reasonably be expected to occur in a time bucket we can improve indexing performance by speculatively initializing parts of the hot index. In one embodiment the speculative indexing is performed by copying the packed array of segments and its associated hash table from an earlier hot index. The hot slice is then populated as usual with the exception that the segment array is already populated and ready for duplicate testing. Because of the highly regular language and limited vocabulary of machines the hit rate associated with this speculation can be very good.

The searching process as described in the next section allows the user to search on segments segment prefixes and segment suffixes. To accommodate these search types in one implementation the segments array can be sorted and then stored as a blocked front coded lexicon hereafter called the forward lexicon . This data structure makes it possible to perform segment and segment prefix lookups efficiently while still achieving a reasonable amount of compression of the segment text. When a search is being performed on a particular segment the offset of the segment in the forward lexicon is used as an efficient way to look up metadata associated with the queried for segment in other associated tables.

To handle suffix lookups a blocked front coded lexicon can be created on the same collection of segments after they have been string reversed hereafter called the reverse lexicon . Also a map is populated that converts the offset of a reversed segment in the reverse lexicon to the equivalent non reversed segment s offset in the forward lexicon hereafter called the reverse forward map . When performing suffix lookups the offset in the reverse lexicon is used as an offset into the reverse forward map. The value stored at that position in the map is the appropriate offset to use for the other metadata arrays in the warm index.

The warm index provides a list of event offsets for each segment indexed preferably in an efficient manner. In one implementation this can be done by maintaining an array of compressed postings lists and an associated array of offsets to the beginning of each of those compressed postings lists. The postings lists are maintained in segment offset order so when a lookup is performed the segment ID can be used to find the appropriate entry of the postings lists offsets array. The values in the postings lists entries are the offsets that should be used to look up events in the packed array of event addresses.

Finally statistical metadata can be provided for each indexed segment e.g. the first and last time of occurrence of the segment the mean inter arrival time and the standard deviation of the inter arrival time .

During the course of the indexing process it is possible that a single time bucket will be filled and committed to disk several times. This will result in multiple independently searchable indices in secondary storage for a single time span. In an exemplary implementation there is a merging process that takes as input two or more warm indices and merges them into a single warm index for that time bucket. This is a performance optimization and is not strictly required for searching.

Furthermore over a long period of time it is possible that applying the indexing process to time series data will cause a large amount of persistent data to accumulate. The indexing process therefore preferably contains an expiration process that monitors the database for time buckets to be deleted based on user provided preferences. In one implementation these preferences might include a trailing time window events older than 3 months need not be returned in search results a time range events earlier than January 1 of this year need not be returned in search results a maximum number of events no more than 1 million events need be returned in search results or a maximum total size for the index return as many useful search results as possible while consuming no more than 100 GB of Disk . A process periodically wakes up and tests the collection of warm slices for any slices that meet the expiration criterion. Upon expiration a warm index file and its associated raw event data and segmentation is moved out of the active index. The index file need not necessarily be deleted. In one implementation the index file could be streamed to less expensive offline storage.

An example TSSE search process is shown in and includes several methods for parsing a search phrase issuing multiple sub searches in order to satisfy the incoming parent search using sub search results to prune searches and merging sub search results into a final set of search results for presentation to the user.

During search processing incoming search phrases are parsed according to a time series search language TSSL in order to generate annotated parse trees . An exemplary TSSL language syntax includes a series of modifiers or commands taking the format name value. Some modifiers may have default values and some can only be used once while some can appear several times in the same search with different values. Examples include the following 

Modifiers can be combined with keywords wildcard characters literal strings quoted phrases and Boolean operators such as AND OR NOT. Parentheses can be used to nest search and sub search phrases together. An example search phrase might be sourcetype mysql sock NOT started OR host foo OR host BAR maxresults IO eventtype baddb OR eventtype 8512 3 daysago 30 .

In one implementation a custom parser handles the Boolean operators NOT and OR and defaults to AND . This implementation also handles using parentheses to disambiguate the language when there are several operators. Otherwise it associates left to right. The implementation also supports special search operators that are indicated using a domain specifier followed by a demarcation element. For example searching for source 1234 might indicate that the searcher human or system wants to restrict results to events that were received from a particular source ID.

Incoming search phrases may also trigger ad hoc computation based on a map of special keywords. For example a special search string might be used to indicate that a search is to be stored and reissued on a periodic basis or to request a list of sources. In this case the search string would be stored in a table on disk along with a schedule specifying the schedule on which the search should be reissued. Depending on the results of the search when executed additional actions may be triggered. For example an email alert might be sent an RSS feed might be updated or a user supplied script might be executed. Another example of a search that triggers ad hoc computation is one that is indicated to be saved for later use but not to be reissued on a periodic basis.

Assuming that the search parser determined that an annotated syntax tree should be created for the search string the next component the search execution engine will use the annotated syntax tree to issue sub searches to the time bucketed indices . Each sub search is targeted at an individual time bucket . Time buckets are queried in the order that is most advantageous to pruning given the sort order for the results. For example if search results are sorted in reverse chronological order then the sub search for the most recent time bucket will be issued first. This allows the search execution engine to examine the results of the sub search before proceeding with additional expensive sub searches . For example if a particular sub search returns enough results then it is not necessary to proceed with additional sub searches .

Once enough results sets have been accumulated to satisfy the parent search another module will take the results and merge them into a single result set that satisfies the search. This merging process in one implementation performs a merge sort on the results from each of the buckets to keep them in the order required for the presentation process.

The final process in an exemplary implementation of our example TSSE is the preparation of search results for presentation as shown in . Unlike current large scale search engines that present non interactive results ordered by keyword relevance ranking this example TSSE can present results organized by time event relationships and keyword relevance ranking.

Unique to the challenge of indexing and searching time series data is the presentation of results using time as a primary dimension . Because existing large scale search engines do not organize information by time the presentation of time based results is not a consideration. However a primary benefit of a TSSE is the ability to index search and present time series data chronologically. Results can be presented by aggregating and summarizing search results based on discrete time ranges or based on statistical calculations.

For example the example TSSL can specify to see results for only a particular time frame and or to see results presented by seconds minutes hours days weeks or months. In this way the search window can be limited to a timeframe and the results can be constructed for optimal viewing based on the density of the expected result set returned from a search. The search 192.168.169.100 hoursago 24 page seconds will return time series events including the keyword 192.168.169.100 that occurred within the last 24 hours and will summarize the display results by seconds. In an exemplary implementation of a TSSE summarization can include both aggregated display lines summarizing the events for the summary window and or paging the results by the summary window. In the example above each page of the search results presentation may include one second in time. Examples include but are not limited to 

In addition to time based presentation an example TSSE preferably is able to present additional aggregation and summarization of results by metadata characteristics 720 such as data source data source type event type or originating host machine. In this way results can be not only organized by time but also refined by metadata aggregation and summarization. The search 192.168.169.100 page source will present all the results with 192.168.169.100 and put each data source containing results on a separate page. Examples include but are not limited to 

Because time and certain metadata parameters e.g. machine IP addresses can be continuous an example TSSE user interaction model can include the ability to move from small increments of time seconds or minutes or metadata parameters different classes of IP addresses using a zoom control . This zoom control can be combined with other metadata search parameters to enable the rapid movement through large amounts of data. Examples include but are not limited to 

Given the different types of users humans and systems and the varying types of time series data and events e.g. single line events a few bytes in size to multiple line events several megabytes in size it is useful to be able to specify the density of the results. In one implementation the presentation density can be controlled to return and or display only the raw data without any metadata in a simple ASCII text format. Alternatively the same results can be returned and or displayed with full metadata as rich XML.

The TSSE can be implemented in many different ways. In one approach each box shown in the various figures is implemented in software as a separate process. All of the processes can run on a single machine or they can be divided up to run on separate logical or physical machines. In alternate embodiments the invention is implemented in computer hardware firmware software and or combinations thereof. Apparatus of the invention can be implemented in a computer program product tangibly embodied in a machine readable storage device for execution by a programmable processor and method steps of the invention can be performed by a programmable processor executing a program of instructions to perform functions of the invention by operating on input data and generating output. The invention can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. Each computer program can be implemented in a high level procedural or object oriented programming language or in assembly or machine language if desired and in any case the language can be a compiled or interpreted language. Suitable processors include by way of example both general and special purpose microprocessors. Generally a processor will receive instructions and data from a read only memory and or a random access memory. Generally a computer will include one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM disks. Any of the foregoing can be supplemented by or incorporated in ASICs application specific integrated circuits and other forms of hardware.

Therefore although the detailed description contains many specifics these should not be construed as limiting the scope of the invention but merely as illustrating different examples and aspects of the invention. It should be appreciated that the scope of the invention includes other embodiments not discussed in detail above. Various modifications changes and variations which will be apparent to those skilled in the art may be made in the arrangement operation and details of the method and apparatus of the present invention disclosed herein without departing from the spirit and scope of the invention as defined in the appended claims. Therefore the scope of the invention should be determined by the appended claims and their legal equivalents.

